{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "550a0979",
   "metadata": {},
   "source": [
    "### **üß† TOPIC 1 ‚Äî What is Chat History in LLMs?**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65eb80ae",
   "metadata": {},
   "source": [
    "#### **üóÇÔ∏è Chat History in LLMs (Conversation Memory)**\n",
    "Chat history is the **previous conversation context** that we send back to the LLM\n",
    "along with every new user  message.\n",
    "\n",
    "LLMs do NOT remember anything by themselves.\n",
    "They are **stateless** by default.\n",
    "\n",
    "This Means:\n",
    "- Every request is independent\n",
    "- Memory must be manually provided\n",
    "\n",
    "----\n",
    "\n",
    "#### **Why Chat History Is Required**\n",
    "\n",
    "User: What is Python?\n",
    "Assistant: Explain Python\n",
    "\n",
    "User: Give an Example\n",
    "Assistant: ‚ùå Confused ‚Äî example of what?\n",
    "\n",
    "With chat history:\n",
    "The model understands:\n",
    "- What was discussed\n",
    "- What the user is referring to\n",
    "- How to respond correctly\n",
    "\n",
    "---\n",
    "\n",
    "#### **How Chat History Works Internally**\n",
    "\n",
    "Each Request contains a list of messages:\n",
    "\n",
    "- system ‚Üí rules / behavior\n",
    "- user ‚Üí questions\n",
    "- assistant ‚Üí previous answers\n",
    "\n",
    "The LLM reads this entire list **top to bottom** every time.\n",
    "\n",
    "There is no hidden memory.\n",
    "\n",
    "\n",
    "------\n",
    "\n",
    "#### **Important Interview Truth**\n",
    "\n",
    "LLMs do NOT:\n",
    "\n",
    "‚ùå Remember previous chats  \n",
    "‚ùå Store conversation state  \n",
    "‚ùå Learn across sessions \n",
    "\n",
    "Everything is:\n",
    "‚úÖ Explicit  \n",
    "‚úÖ Token-based  \n",
    "‚úÖ Re-sent every time \n",
    "\n",
    "-----\n",
    "\n",
    "#### **Why This Matters in Real Projects**\n",
    "\n",
    "Chat history is required for:\n",
    "- Chatbots\n",
    "- Assistants\n",
    "- Agents\n",
    "- Multi-step reasoning\n",
    "- Follow-up questions\n",
    "\n",
    "Without IT:\n",
    "\n",
    "- Conversation break\n",
    "- UX feels dumb\n",
    "- Users lose trust\n",
    "\n",
    "----\n",
    "\n",
    "#### **Senior Engineer Insight**\n",
    "\n",
    "Chat History is:\n",
    "\n",
    "- A **design decision**\n",
    "- A **cost decision** (more tokens)\n",
    "- A **performance decision**\n",
    "\n",
    "Good Engineers:\n",
    "\n",
    "- Control how much history to send\n",
    "- Avoid unnecessary repetition\n",
    "- Trim history intelligently\n",
    "\n",
    "----\n",
    "\n",
    "#### **Today's Goal**\n",
    "\n",
    "In this notebook, we will:\n",
    "1. Understand chat history conceptually\n",
    "2. Build a multi-turn conversation\n",
    "3. Observe token growth\n",
    "4. Learn memory pitfalls\n",
    "5. Design best practices"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba047645",
   "metadata": {},
   "source": [
    "‚úÖ Topic 1 Summary (Applied Rule)\n",
    "\n",
    "- LLMs are stateless\n",
    "- Chat history must be sent every time\n",
    "- Context = list of messages\n",
    "- Memory is token-based, not magical\n",
    "- Poor history design breaks conversations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16eca91f",
   "metadata": {},
   "source": [
    "#### **Client Configuration**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "50e4b6e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the 'nbimporter' package which allows you to import Jupyter notebooks as Python modules\n",
    "import nbimporter\n",
    "\n",
    "# Commented out line (doesn't execute): The '%run' magic command would run the '01_grokai_chat_intro.ipynb' notebook in Jupyter.\n",
    "## %run 01_grokai_chat_intro.ipynb\n",
    "\n",
    "# Importing 'sys' to interact with the Python runtime environment (for modifying the system path)\n",
    "import sys\n",
    "\n",
    "# Importing 'os' to work with the operating system, like handling file paths\n",
    "import os\n",
    "\n",
    "# Add the absolute path to the project directory ('genai_project') to the Python path\n",
    "# This allows Python to find and import modules from this directory\n",
    "sys.path.append(os.path.abspath(\"C:/Users/dhira/Desktop/genai_project\"))\n",
    "\n",
    "# Import the 'client' object from the 'grokai_client_setup.py' file located in the project directory\n",
    "# This client is likely responsible for setting up communication with the GrokAI system\n",
    "from grokai_client_setup import client\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9e95815",
   "metadata": {},
   "source": [
    "#### **üíª TOPIC 2 ‚Äî First Multi-Turn Chat (Using Chat History)**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f43577b4",
   "metadata": {},
   "source": [
    "**üìÑ DOCUMENTATION**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab493821",
   "metadata": {},
   "source": [
    "#### **üéØ Topic 2 Goal**\n",
    "- Build a 3-turn conversation by manually maintaining `messages` (chat history).\n",
    "- This proves that the model remembers ONLY what we send in the request."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ae14b5bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===== TURN 1 (Assistant) =====\n",
      "\n",
      "**What is a Python List?**\n",
      "\n",
      "In Python, a list is a collection of items that can be of any data type, including strings, integers, floats, and other lists. It's like a box where you can store multiple things.\n",
      "\n",
      "**Example:**\n",
      "\n",
      "```python\n",
      "my_list = [1, 2, 3, \"hello\", 4.5]\n",
      "```\n",
      "\n",
      "In this example, `my_list` is a list that contains five items: three integers, one string, and one float.\n",
      "\n",
      "**Accessing List Items:**\n",
      "\n",
      "You can access individual items in a list using their index (position). The index starts at 0, so the first item is at index 0, the second item is at index 1\n",
      "\n",
      "===== TURN 2 =================== (Assistant) =====\n",
      "\n",
      "**Simple List Example:**\n",
      "\n",
      "```python\n",
      "fruits = [\"apple\", \"banana\", \"cherry\"]\n",
      "print(fruits[0])  # Output: apple\n",
      "```\n",
      "\n",
      "In this example, `fruits` is a list of three strings. We're accessing the first item in the list (at index 0) using `fruits[0]`.\n",
      "\n",
      "===== TURN 3 ============================(Assistant) =====\n",
      "\n",
      "**Lists vs Tuples:**\n",
      "\n",
      "A list in Python is a collection that can be modified (items can be added, removed, or changed), whereas a tuple is a collection that cannot be modified once it's created.\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# üìò SECTION 1 ‚Äî Define Chat History Container (messages list)\n",
    "# ------------------------------------------------------------\n",
    "# Why?\n",
    "#   - LLMs are stateless: they don't remember past turns\n",
    "#   - We store chat history ourselves in a Python list\n",
    "#   - We send this full list in every request\n",
    "# ============================================================\n",
    "\n",
    "messages = []  # This list will store the conversation step-by-step\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# üìò SECTION 2 ‚Äî (Optional but recommended) Set a System Role\n",
    "# ------------------------------------------------------------\n",
    "# Why?\n",
    "#   - \"system\" sets rules / teaching style\n",
    "#   - Keeps responses consistent across turns\n",
    "#   - Best practice for production assistants\n",
    "# ============================================================\n",
    "\n",
    "messages.append({\n",
    "    \"role\": \"system\",\n",
    "    \"content\": \"You are a friendly Python tutor. Explain in very simple language with short examples.\"\n",
    "})\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# üìò SECTION 3 ‚Äî TURN 1 (User asks first question)\n",
    "# ------------------------------------------------------------\n",
    "# Why append?\n",
    "#   - We must record the user's message in history\n",
    "#   - If we don't, the model won't know what the user asked before\n",
    "# ============================================================\n",
    "\n",
    "messages.append({\n",
    "    \"role\": \"user\",\n",
    "    \"content\": \"What is a Python list?\"\n",
    "})\n",
    "\n",
    "# Send request with full history\n",
    "response_1 = client.chat.completions.create(\n",
    "    model=\"llama-3.1-8b-instant\",\n",
    "    messages=messages,\n",
    "    temperature=0.4,\n",
    "    top_p=0.9,\n",
    "    max_tokens=150\n",
    ")\n",
    "\n",
    "# Extract assistant reply\n",
    "assistant_reply_1 = response_1.choices[0].message.content\n",
    "\n",
    "# Save assistant reply in chat history\n",
    "messages.append({\n",
    "    \"role\": \"assistant\",\n",
    "    \"content\": assistant_reply_1\n",
    "})\n",
    "\n",
    "print(\"===== TURN 1 (Assistant) =====\\n\")\n",
    "print(assistant_reply_1)\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# üìò SECTION 4 ‚Äî TURN 2 (Follow-up question depends on history)\n",
    "# ------------------------------------------------------------\n",
    "# Why?\n",
    "#   - This question uses context: \"give me an example\"\n",
    "#   - The model can answer correctly only if Turn 1 is in messages\n",
    "# ============================================================\n",
    "\n",
    "messages.append({\n",
    "    \"role\": \"user\",\n",
    "    \"content\": \"Give me one simple example.\"\n",
    "})\n",
    "\n",
    "response_2 = client.chat.completions.create(\n",
    "    model=\"llama-3.1-8b-instant\",\n",
    "    messages=messages,\n",
    "    temperature=0.4,\n",
    "    top_p=0.9,\n",
    "    max_tokens=150\n",
    ")\n",
    "\n",
    "assistant_reply_2 = response_2.choices[0].message.content\n",
    "\n",
    "messages.append({\n",
    "    \"role\": \"assistant\",\n",
    "    \"content\": assistant_reply_2\n",
    "})\n",
    "\n",
    "print(\"\\n===== TURN 2 =================== (Assistant) =====\\n\")\n",
    "print(assistant_reply_2)\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# üìò SECTION 5 ‚Äî TURN 3 (Another follow-up)\n",
    "# ------------------------------------------------------------\n",
    "# Why?\n",
    "#   - Multi-turn chat means each new question depends on prior turns\n",
    "#   - We'll ask a follow-up that requires the assistant to stay consistent\n",
    "# ============================================================\n",
    "\n",
    "messages.append({\n",
    "    \"role\": \"user\",\n",
    "    \"content\": \"Now explain how list and tuple are different in one sentence.\"\n",
    "})\n",
    "\n",
    "response_3 = client.chat.completions.create(\n",
    "    model=\"llama-3.1-8b-instant\",\n",
    "    messages=messages,\n",
    "    temperature=0.4,\n",
    "    top_p=0.9,\n",
    "    max_tokens=120\n",
    ")\n",
    "\n",
    "assistant_reply_3 = response_3.choices[0].message.content\n",
    "\n",
    "messages.append({\n",
    "    \"role\": \"assistant\",\n",
    "    \"content\": assistant_reply_3\n",
    "})\n",
    "\n",
    "print(\"\\n===== TURN 3 ============================(Assistant) =====\\n\")\n",
    "print(assistant_reply_3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73a40869",
   "metadata": {},
   "source": [
    "#### **‚úÖ Observations (Topic 2)**\n",
    "\n",
    "- The assistant answered follow-up questions correctly because we included previous messages.\n",
    "- Chat history is simply a Python list of messages that we keep appending to.\n",
    "- Without storing assistant replies, later turns become inconsistent.\n",
    "\n",
    "Key takeaway:\n",
    "- LLM memory = chat history that we resend every time.\n",
    "\n",
    "\n",
    "**‚úÖ Topic 2 Final Summary (Rule Applied)**\n",
    "\n",
    "- Implemented a 3-turn conversation using a messages list\n",
    "- Used system/user/assistant roles correctly\n",
    "- Learned the model ‚Äúremembers‚Äù only what we provide in the request\n",
    "- Built the same foundation used in real chatbots\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "116d2450",
   "metadata": {},
   "source": [
    "#### **üß† TOPIC 3 ‚Äî What Breaks If We Don‚Äôt Store Chat History?**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f958b5d2",
   "metadata": {},
   "source": [
    "This section demonstrates **why chat history is mandatory** for any real conversation.\n",
    "\n",
    "We will intentionally NOT store previous messages\n",
    "and observe how the LLM behaves.\n",
    "\n",
    "This is a controlled failure ‚Äî very important for learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a73e804f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===== TURN 1 (Assistant) =====\n",
      "\n",
      "**Python List**\n",
      "===============\n",
      "\n",
      "In Python, a list is a collection of items that can be of any data type, including strings, integers, floats, and other lists. Lists are denoted by square brackets `[]` and are ordered, meaning that items have a specific position in the list.\n",
      "\n",
      "**Creating a List**\n",
      "-----------------\n",
      "\n",
      "You can create a list by enclosing a sequence of values in square brackets `[]`:\n",
      "\n",
      "```python\n",
      "# Create a list of strings\n",
      "fruits = ['apple', 'banana', 'cherry']\n",
      "\n",
      "# Create a list of integers\n",
      "numbers = [1,\n",
      "\n",
      "===== TURN 2 (Assistant) =====\n",
      "\n",
      "What would you like a simple example of?\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# üìò SECTION 1 ‚Äî Ask First Question (NO HISTORY)\n",
    "# ------------------------------------------------------------\n",
    "# Why?\n",
    "#   - We intentionally do NOT store messages\n",
    "#   - Each request is independent\n",
    "# ============================================================\n",
    "\n",
    "response_1 = client.chat.completions.create(\n",
    "    model=\"llama-3.1-8b-instant\",\n",
    "    messages=[{\"role\":\"user\",\n",
    "    \"content\":\"What is python list?\"}],\n",
    "    temperature=0.4,\n",
    "    max_tokens=120\n",
    ")\n",
    "\n",
    "print(\"===== TURN 1 (Assistant) =====\\n\")\n",
    "print(response_1.choices[0].message.content)\n",
    "\n",
    "# ============================================================\n",
    "# üìò SECTION 2 ‚Äî Ask Follow-up Question (STILL NO HISTORY)\n",
    "# ------------------------------------------------------------\n",
    "# Why this breaks:\n",
    "#   - The model has NO idea what was discussed earlier\n",
    "#   - The phrase 'give me an example' has no reference\n",
    "# ============================================================\n",
    "\n",
    "response_2 = client.chat.completions.create(\n",
    "    model=\"llama-3.1-8b-instant\",\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\":\"user\",\n",
    "            \"content\":\"Give me one simple example\",\n",
    "\n",
    "        }],\n",
    "        temperature=0.4,\n",
    "        max_tokens=120\n",
    ")\n",
    "\n",
    "print(\"\\n===== TURN 2 (Assistant) =====\\n\")\n",
    "print(response_2.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "852cb6cf",
   "metadata": {},
   "source": [
    "#### **üîç What Went Wrong?**\n",
    "\n",
    "- Each request was sent independently.\n",
    "- No previous messages were included.\n",
    "- The LLM had no context to understand:\n",
    "  \"example of what?\"\n",
    "\n",
    "This proves:\n",
    "LLMs do NOT remember past interactions unless we resend them.\n",
    "\n",
    "---\n",
    "\n",
    "#### **üß† Senior Engineer Insight**\n",
    "\n",
    "If chat history is not handled properly:\n",
    "- Conversations break\n",
    "- Users lose trust\n",
    "- Bots feel \"dumb\"\n",
    "\n",
    "This is the #1 mistake beginners make when building chatbots.\n",
    "\n",
    "---\n",
    "\n",
    "#### **‚úÖ Correct Mental Model**\n",
    "\n",
    "LLM ‚â† Chat application  \n",
    "LLM = Stateless text generator  \n",
    "\n",
    "Chat memory = YOUR responsibility\n",
    "\n",
    "\n",
    "-------------------------------------------\n",
    "\n",
    "‚úÖ Topic 3 Final Summary (Rule Applied)\n",
    "\n",
    "Demonstrated failure without chat history\n",
    "\n",
    "Proved LLMs are stateless\n",
    "\n",
    "Built intuition by breaking the system intentionally\n",
    "\n",
    "Understood why history management is critical"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "674925b8",
   "metadata": {},
   "source": [
    "#### **üß† TOPIC 4 ‚Äî Token Growth & Cost Impact (Chat History)**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "211b6bb8",
   "metadata": {},
   "source": [
    "#### **üìà Topic 4 ‚Äî Token Growth & Cost Impact**\n",
    "\n",
    "#### **What are Tokens?**\n",
    "Tokens are small pieces of text (words or parts of words) that LLMs read and generate.\n",
    "\n",
    "Examples:\n",
    "- \"Python\" ‚Üí 1 token\n",
    "- \"chat history\" ‚Üí 2 tokens\n",
    "- Long conversations ‚Üí many tokens\n",
    "\n",
    "LLMs charge and process requests based on **number of tokens**.\n",
    "\n",
    "---\n",
    "\n",
    "#### **Why Chat History Increases Tokens**\n",
    "\n",
    "Every time we send a request, we send:\n",
    "- system messages\n",
    "- user messages\n",
    "- assistant replies\n",
    "- follow-up questions\n",
    "\n",
    "This means:\n",
    "‚û°Ô∏è Each new request includes **all previous messages**\n",
    "‚û°Ô∏è Token count grows linearly with conversation length\n",
    "\n",
    "---\n",
    "\n",
    "#### **Important Rule (Interview-Level)**\n",
    "\n",
    "> LLMs do NOT remember conversations.  \n",
    "> We resend the entire chat history every time.\n",
    "\n",
    "So:\n",
    "- More messages = more tokens\n",
    "- More tokens = more cost\n",
    "- More tokens = slower responses\n",
    "\n",
    "---\n",
    "\n",
    "#### **Real-World Impact**\n",
    "\n",
    "If chat history is not controlled:\n",
    "- Costs increase quickly\n",
    "- Latency increases\n",
    "- Token limits may be hit\n",
    "- Apps may fail unexpectedly\n",
    "\n",
    "This is a **production risk**, not just theory.\n",
    "\n",
    "---\n",
    "\n",
    "#### **Senior Engineer Insight**\n",
    "\n",
    "Good systems:\n",
    "- Trim old messages\n",
    "- Summarize past context\n",
    "- Keep only what matters\n",
    "\n",
    "We will implement this later.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72b4b453",
   "metadata": {},
   "source": [
    "#### **üíª Code Cell ‚Äî Observe Token Growth (Simple & Safe)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "a891ee1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial message count: 1\n",
      "After turn 5, total messages sent: 11\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# üìò SECTION ‚Äî Observing Chat History Growth\n",
    "# ------------------------------------------------------------\n",
    "# Why?\n",
    "#   - To visually understand how chat history increases\n",
    "#   - To see how many messages we are sending each time\n",
    "# ============================================================\n",
    "\n",
    "# Create empty chat history list\n",
    "# We do this because LLMs are stateless and need full context\n",
    "\n",
    "messages = []\n",
    "\n",
    "# Add system message\n",
    "messages.append(\n",
    "    {\"role\":\"system\",\n",
    "    \"content\":\"You are a helpful Python tutor\"}),\n",
    "\n",
    "print(\"Initial message count:\", len(messages))\n",
    "\n",
    "\n",
    "# Simulate multiple conversation turns\n",
    "# Each append represents additional tokens sent to the model\n",
    "\n",
    "for i in range(1, 6):\n",
    "    messages.append({\n",
    "        \"role\":\"user\",\n",
    "        \"content\":f\"Question number {i}\"})\n",
    "    \n",
    "    messages.append({\n",
    "        \"role\":\"assistant\",\n",
    "        \"content\":f\"Answer number {i}\"\n",
    "    })\n",
    "\n",
    "print(f\"After turn {i}, total messages sent:\", len(messages))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9c4314c",
   "metadata": {},
   "source": [
    "#### **üîç Observations**\n",
    "\n",
    "- Each user + assistant turn adds TWO messages.\n",
    "- All messages are resent in every request.\n",
    "- Token usage grows with conversation length.\n",
    "- This directly impacts:\n",
    "  - Cost\n",
    "  - Performance\n",
    "  - Reliability\n",
    "\n",
    "Key idea:\n",
    "Chat history must be **managed**, not ignored.\n",
    "\n",
    "#### **üß† Production Reality (Very Important)**\n",
    "\n",
    "Chatbots with long sessions can become expensive\n",
    "\n",
    "Token limits can break conversations\n",
    "\n",
    "Engineers must actively manage memory\n",
    "\n",
    "This is why:\n",
    "\n",
    "Summarization\n",
    "\n",
    "Windowing\n",
    "\n",
    "Memory strategies\n",
    "exist in real systems.\n",
    "\n",
    "### **‚úÖ Topic 4 Final Summary (Rule Applied)**\n",
    "\n",
    "Tokens are the unit of cost and computation\n",
    "\n",
    "Chat history increases tokens every turn\n",
    "\n",
    "Uncontrolled history causes cost and latency issues\n",
    "\n",
    "Memory management is a core GenAI engineering skill"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "625940d0",
   "metadata": {},
   "source": [
    "### **üß† TOPIC 5 ‚Äî Memory Pitfalls & Best Practices (Chat History)**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d1345d4",
   "metadata": {},
   "source": [
    "#### **‚ö†Ô∏è Topic 5 ‚Äî Memory Pitfalls & Best Practices**\n",
    "\n",
    "Managing chat history is one of the **most important responsibilities**\n",
    "of a GenAI engineer.\n",
    "\n",
    "Poor memory handling causes:\n",
    "- Broken conversations\n",
    "- High costs\n",
    "- Slow responses\n",
    "- Token limit failures\n",
    "\n",
    "This section explains **what goes wrong** and **how professionals handle it**.\n",
    "\n",
    "---\n",
    "\n",
    "#### ‚ùå Common Memory Pitfalls (Very Important)\n",
    "\n",
    "#### **Pitfall 1 ‚Äî Storing Everything Forever**\n",
    "**What happens:**\n",
    "- Every message is kept\n",
    "- Token count keeps growing\n",
    "- Cost and latency increase\n",
    "\n",
    "**Why it‚Äôs bad:**\n",
    "- Most old messages are no longer relevant\n",
    "- LLMs waste tokens reading useless context\n",
    "\n",
    "---\n",
    "\n",
    "#### **Pitfall 2 ‚Äî Not Storing Assistant Replies**\n",
    "**What happens:**\n",
    "- Only user messages are saved\n",
    "- Assistant replies are missing\n",
    "\n",
    "**Why it breaks things:**\n",
    "- The model loses continuity\n",
    "- Follow-up questions become confusing\n",
    "\n",
    "---\n",
    "\n",
    "#### **Pitfall 3 ‚Äî Repeating System Prompts Every Time**\n",
    "**What happens:**\n",
    "- Same long system message repeated unnecessarily\n",
    "\n",
    "**Why it‚Äôs bad:**\n",
    "- Token waste\n",
    "- No additional benefit\n",
    "\n",
    "---\n",
    "\n",
    "#### **Pitfall 4 ‚Äî Mixing Memory with Business Logic**\n",
    "**What happens:**\n",
    "- Parsing JSON\n",
    "- Database writes\n",
    "- Tool calls\n",
    "inside chat memory loop\n",
    "\n",
    "**Why it‚Äôs dangerous:**\n",
    "- Hard to debug\n",
    "- Easy to corrupt memory\n",
    "- Leads to unpredictable behavior\n",
    "\n",
    "---\n",
    "\n",
    "#### **‚úÖ Best Practices (Senior-Level)**\n",
    "\n",
    "#### **Best Practice 1 ‚Äî Keep Only Relevant History**\n",
    "- Recent turns matter more than old ones\n",
    "- Drop greetings and confirmations\n",
    "- Preserve context, not noise\n",
    "\n",
    "---\n",
    "\n",
    "#### **Best Practice 2 ‚Äî Use Sliding Window Memory**\n",
    "- Keep last N turns only\n",
    "- Remove older messages\n",
    "- Simple and effective\n",
    "\n",
    "---\n",
    "\n",
    "#### **Best Practice 3 ‚Äî Summarize Old Context**\n",
    "- Convert old messages into a short summary\n",
    "- Replace many messages with one\n",
    "\n",
    "(We will implement this later.)\n",
    "\n",
    "---\n",
    "\n",
    "#### **Best Practice 4 ‚Äî Separate Roles Clearly**\n",
    "- system ‚Üí rules and behavior\n",
    "- user ‚Üí questions\n",
    "- assistant ‚Üí answers\n",
    "\n",
    "Never mix them.\n",
    "\n",
    "---\n",
    "\n",
    "#### **Best Practice 5 ‚Äî Treat Memory as a Design Component**\n",
    "Memory is:\n",
    "- A cost decision\n",
    "- A performance decision\n",
    "- A UX decision\n",
    "\n",
    "Not just a technical detail.\n",
    "\n",
    "---\n",
    "\n",
    "#### **üß† Interview-Level Insight**\n",
    "\n",
    "> ‚ÄúHow do you manage LLM memory in production?‚Äù\n",
    "\n",
    "Strong answer:\n",
    "- Sliding window\n",
    "- Summarization\n",
    "- Cost-awareness\n",
    "- Context relevance\n",
    "\n",
    "Weak answer:\n",
    "- ‚ÄúI store everything‚Äù\n",
    "\n",
    "---\n",
    "\n",
    "#### **‚úÖ Topic 5 Summary**\n",
    "\n",
    "- Memory grows with chat history\n",
    "- Poor memory handling breaks systems\n",
    "- Professionals actively manage memory\n",
    "- Memory strategy is a core GenAI skill\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1a478b5",
   "metadata": {},
   "source": [
    "#### **üß™ TOPIC 6 ‚Äî Mini Practice + Mock Test (Chat History)**\n",
    "\n",
    "\n",
    "**üß© PART A ‚Äî Mini Practice (Hands-on, Small & Focused)**\n",
    "\n",
    "üéØ Goal\n",
    "\n",
    "Reinforce how chat history is built\n",
    "\n",
    "Practice append-based memory\n",
    "\n",
    "Observe how removing memory breaks continuity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "83dc1494",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============TURN 1=============== RESPONSE:\n",
      " **What is a Dictionary in Python?**\n",
      "\n",
      "In Python, a dictionary is a data structure that stores a collection of key-value pairs. It's like a phonebook where you have names (keys) and phone numbers (values).\n",
      "\n",
      "**Key Features of a Dictionary:**\n",
      "\n",
      "1. **Key-Value Pairs**: Each item in a dictionary is a pair of a key and a value.\n",
      "2. **Unique Keys**: Each key in a dictionary must be unique, just like a phone number.\n",
      "3. **Flexible Data Type**: Keys and values can be of any data type, including strings, integers\n",
      "\n",
      "==========TURN 2============== RESPONSE:\n",
      " **Simple Dictionary Example**\n",
      "\n",
      "Here's a simple example of a dictionary in Python:\n",
      "```python\n",
      "# Create a dictionary\n",
      "person = {\n",
      "    \"name\": \"John Doe\",\n",
      "    \"age\": 30,\n",
      "    \"city\": \"New York\"\n",
      "}\n",
      "\n",
      "# Accessing values\n",
      "print(person[\"name\"])  # Output: John Doe\n",
      "print(person[\"age\"])   # Output: 30\n",
      "print(person[\"city\"])  # Output: New York\n",
      "\n",
      "# Adding a new key-value pair\n",
      "person[\"country\"] = \"USA\"\n",
      "print(person)  # Output: {'name': 'John\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# üß™ MINI PRACTICE ‚Äî Chat History Handling\n",
    "# ------------------------------------------------------------\n",
    "# Goal:\n",
    "#   - Manually maintain chat history\n",
    "#   - Observe what happens when history is preserved vs removed\n",
    "# ============================================================\n",
    "\n",
    "# STEP 1 ‚Äî Create an empty list to store chat history\n",
    "# Why?\n",
    "#   - LLMs are stateless\n",
    "#   - This list will act as the conversation memory\n",
    "messages = []\n",
    "\n",
    "# STEP 2 ‚Äî Add a system message\n",
    "# Why?\n",
    "#   - Sets consistent behavior for the assistant\n",
    "\n",
    "messages.append({\n",
    "    \"role\":\"system\",\n",
    "    \"content\":\"You are a Python tutor. Explain in simple language.\"})\n",
    "\n",
    "# STEP 2 ‚Äî Add a system message\n",
    "# Why?\n",
    "#   - Sets consistent behavior for the assistant\n",
    "\n",
    "messages.append({\n",
    "    \"role\":\"user\",\n",
    "    \"content\":\"What is the Python dictionary?\"})\n",
    "\n",
    "# STEP 4 ‚Äî Call the model with current chat history\n",
    "\n",
    "response_1 = client.chat.completions.create(\n",
    "    model=\"llama-3.1-8b-instant\",\n",
    "    messages=messages,\n",
    "    temperature=0.4,\n",
    "    max_tokens=120\n",
    ")\n",
    "\n",
    "# STEP 5 ‚Äî Extract assistant reply\n",
    "\n",
    "assistant_reply_1 = response_1.choices[0].message.content\n",
    "\n",
    "# STEP 6 ‚Äî Store assistant reply in history\n",
    "# Why?\n",
    "#   - Without storing assistant replies, follow-up questions break\n",
    "\n",
    "messages.append({\n",
    "    \"role\":\"assistant\",\n",
    "    \"content\": assistant_reply_1\n",
    "})\n",
    "\n",
    "print(\"============TURN 1=============== RESPONSE:\\n\", assistant_reply_1)\n",
    "\n",
    "# STEP 7 ‚Äî Follow-up question (depends on previous answer)\n",
    "messages.append({\n",
    "    \"role\":\"user\",\n",
    "    \"content\":\"Give me a simple example.\"})\n",
    "\n",
    "response_2 = client.chat.completions.create(\n",
    "    model=\"llama-3.1-8b-instant\",\n",
    "    messages=messages,\n",
    "    temperature=0.4,\n",
    "    max_tokens=120\n",
    ")\n",
    "\n",
    "assistant_reply_2 = response_2.choices[0].message.content\n",
    "\n",
    "print(\"\\n==========TURN 2============== RESPONSE:\\n\", assistant_reply_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3faefbe",
   "metadata": {},
   "source": [
    "#### **üß™ Mini Practice Reflection**\n",
    "\n",
    "- Chat history must include BOTH user and assistant messages.\n",
    "- Using append() preserves message order.\n",
    "- Removing assistant replies breaks continuity.\n",
    "- Memory handling is fully controlled by the developer.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2bf7d79",
   "metadata": {},
   "source": [
    "#### **üß† PART B ‚Äî Mock Test (Interview-Level)**\n",
    "\n",
    "#### **üìù Mock Test ‚Äî Chat History & Memory**\n",
    "\n",
    "1Ô∏è‚É£ Why do LLMs require chat history to be sent with every request?\n",
    "\n",
    "2Ô∏è‚É£ What happens if we store only user messages and not assistant replies?\n",
    "\n",
    "3Ô∏è‚É£ Why does chat history increase cost over time?\n",
    "\n",
    "4Ô∏è‚É£ True or False:\n",
    "`top_p` controls how much chat history the model remembers.\n",
    "\n",
    "5Ô∏è‚É£ What is one best practice to manage long conversations in production?\n",
    "\n",
    "---\n",
    "\n",
    "### ‚úÖ Self-Check Answers (Hide initially)\n",
    "1. Because LLMs are stateless and do not remember past interactions.\n",
    "2. Follow-up questions break due to missing context.\n",
    "3. More messages = more tokens = higher cost.\n",
    "4. False.\n",
    "5. Sliding window or summarization.\n",
    "\n",
    "\n",
    "#### **‚úÖ Topic 6 Final Summary**\n",
    "\n",
    "Practiced chat history storage using append()\n",
    "\n",
    "Verified assistant replies are required for continuity\n",
    "\n",
    "Built intuition by breaking and fixing memory\n",
    "\n",
    "Completed an interview-aligned mock test\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6059dff",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3365dedb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63c646f9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
