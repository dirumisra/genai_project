{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a3cce83e",
   "metadata": {},
   "source": [
    "### **What is Streaming in LLMs?**\n",
    "\n",
    "#### **üåä Streaming Responses (LLM)**\n",
    "\n",
    "#### **What is Streaming?**\n",
    "Streaming means the model sends the answer **piece-by-piece (token-by-token)** instead of waiting to send the full answer at the end.\n",
    "\n",
    "#### **Without Streaming (Normal Mode)**\n",
    "- You send a prompt\n",
    "- You wait (silence‚Ä¶)\n",
    "- You receive the full response at once\n",
    "\n",
    "#### **With Streaming**\n",
    "- You send a prompt\n",
    "- You immediately start receiving tokens (small chunks of text)\n",
    "- The response appears gradually (like ChatGPT typing)\n",
    "\n",
    "---\n",
    "\n",
    "#### **Why do we use Streaming?**\n",
    "#### **‚úÖ Better User Experience (UX)**\n",
    "Even if total response time is similar, users feel it is faster because:\n",
    "- **First token arrives quickly**\n",
    "- User sees progress immediately\n",
    "\n",
    "#### **‚úÖ Best for Long Answers**\n",
    "If the model‚Äôs answer is long, streaming avoids the ‚Äúblank screen‚Äù waiting problem.\n",
    "\n",
    "#### **‚úÖ Needed for Chat UI**\n",
    "ChatGPT-like apps (Streamlit, web apps) almost always use streaming.\n",
    "\n",
    "---\n",
    "\n",
    "#### **Important Truth (Interview Point)**\n",
    "**Streaming does NOT make the model compute faster.**\n",
    "\n",
    "It mostly improves:\n",
    "- Perceived speed\n",
    "- Interactivity\n",
    "- User trust\n",
    "\n",
    "---\n",
    "\n",
    "#### **Where Streaming is used in real systems?**\n",
    "- Chatbots / Assistants (Streamlit / Web)\n",
    "- Agents (token-by-token reasoning output)\n",
    "- Live summarization\n",
    "- Long document Q&A (RAG)\n",
    "- Coding assistants\n",
    "\n",
    "---\n",
    "\n",
    "#### **Today‚Äôs Goal**\n",
    "In this notebook, we will:\n",
    "1) Do one streaming request  \n",
    "2) Print tokens as they arrive  \n",
    "3) Compare with non-streaming  \n",
    "4) Build intuition via small practice\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "**Final Summary**\n",
    "- Streaming = output comes token-by-token\n",
    "- Improves UX (perceived speed), not computation speed\n",
    "- Used in chat UIs and long responses\n",
    "- Essential for production-grade assistants\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9285ed7d",
   "metadata": {},
   "source": [
    "#### **Client Configuration**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0433a9b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nbimporter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9c591a48",
   "metadata": {},
   "outputs": [],
   "source": [
    "## %run 01_grokai_chat_intro.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acab2049",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "# Add project root to Python path\n",
    "sys.path.append(os.path.abspath(\"C:/Users/dhira/Desktop/genai_project\"))\n",
    "\n",
    "# Now import client\n",
    "from grokai_client_setup import client"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a91ac532",
   "metadata": {},
   "source": [
    "##### **üíª First Streaming Response (Hands-on)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0a100346",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ü§ñ Streaming output:\n",
      "\n",
      "Here are 3 simple bullet points explaining Python lists:\n",
      "\n",
      "* **Definition**: A Python list is a collection of items that can be of any data type, including strings, integers, floats, and other lists. It is denoted by square brackets `[]`.\n",
      "* **Indexing**: Lists are indexed, meaning you can access and modify individual elements using their index (position) in the list. Indexing starts at 0, so the first element is at index 0, the second element is at index 1, and so on.\n",
      "* **Common operations**: You can perform various operations on lists, such as appending elements using the `append()` method, inserting elements at a specific position using the `insert()` method, and removing elements using\n",
      "\n",
      "‚úÖ Streaming completed.\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# üìò SECTION 1 ‚Äî Import Libraries\n",
    "# ------------------------------------------------------------\n",
    "# Why?\n",
    "#   - We use our already-configured OpenAI client (Groq-compatible)\n",
    "#   - No new libraries needed if it is already done\n",
    "# ============================================================\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# üìò SECTION 2 ‚Äî Define a Prompt (What we want to ask the model)\n",
    "# ------------------------------------------------------------\n",
    "# Why?\n",
    "#   - Keep the prompt simple for first streaming test\n",
    "#   - Streaming is about receiving output gradually\n",
    "# ============================================================\n",
    "\n",
    "prompt = \"Explain Python lists in 3 simple bullet points.\"\n",
    "\n",
    "# ============================================================\n",
    "# üìò SECTION 3 ‚Äî Send Streaming Request (stream=True)\n",
    "# ------------------------------------------------------------\n",
    "# Why stream=True?\n",
    "#   - Instead of waiting for the full answer,\n",
    "#     we receive the response in small chunks (tokens)\n",
    "# ============================================================\n",
    "\n",
    "stream = client.chat.completions.create(\n",
    "    model=\"llama-3.1-8b-instant\",\n",
    "    messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "    temperature=0.3,     # low creativity (more factual)\n",
    "    top_p=0.9,           # stable token selection\n",
    "    max_tokens=150,      # limit output length\n",
    "    stream=True          # ‚úÖ THIS enables streaming\n",
    ")\n",
    "\n",
    "# ============================================================\n",
    "# üìò SECTION 4 ‚Äî Print Tokens as They Arrive\n",
    "# ------------------------------------------------------------\n",
    "# Why this loop?\n",
    "#   - 'stream' returns an iterator of events (chunks)\n",
    "#   - Each chunk may contain partial text in:\n",
    "#       chunk.choices[0].delta.content\n",
    "#   - Some chunks may have no text (None), so we check before printing\n",
    "#\n",
    "# Why end=\"\" and flush=True?\n",
    "#   - end=\"\" avoids adding a new line after every chunk\n",
    "#   - flush=True forces Python to show output immediately\n",
    "# ============================================================\n",
    "\n",
    "print(\"ü§ñ Streaming output:\\n\")\n",
    "\n",
    "for chunk in stream:\n",
    "    # Extract partial text from the stream chunk\n",
    "    delta_text = chunk.choices[0].delta.content\n",
    "\n",
    "    # Some chunks don't contain content (they contain metadata), so skip them\n",
    "    if delta_text:\n",
    "        print(delta_text, end=\"\",flush=True)\n",
    "\n",
    "print(\"\\n\\n‚úÖ Streaming completed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "154e3079",
   "metadata": {},
   "source": [
    "**üìÑ DOCUMENTATION ‚Äî What to note**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85eb811e",
   "metadata": {},
   "source": [
    "#### ‚úÖ Observations (Topic 2)\n",
    "\n",
    "- Output appeared gradually (token-by-token).\n",
    "- The first token appeared quickly.\n",
    "- Streaming feels faster even if total time is similar.\n",
    "\n",
    "Key takeaway:\n",
    "Streaming improves **user experience** and is essential for chat UIs.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7541b58e",
   "metadata": {},
   "source": [
    "‚úÖ Topic 2 Final Summary (as per rule)\n",
    "   - Used stream=True to enable streaming\n",
    "   - Iterated over chunks and printed incremental text\n",
    "   - Learned chunk.choices[0].delta.content is where streamed text appears\n",
    "   - Built the basic streaming loop used in real chat apps"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47fb108b",
   "metadata": {},
   "source": [
    "#### **üíª TOPIC 3 ‚Äî Compare Streaming vs Non-Streaming (Hands-on)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c4b14c45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===== ‚ùå NON-STREAMING RESPONSE =====\n",
      "\n",
      "**Python Dictionaries**\n",
      "=======================\n",
      "\n",
      "A dictionary in Python is a mutable data type that stores mappings of unique keys to values. It is an unordered collection of key-value pairs, where each key is unique and maps to a specific value.\n",
      "\n",
      "**Creating a Dictionary**\n",
      "------------------------\n",
      "\n",
      "A dictionary can be created using the `{}` syntax or the `dict()` function.\n",
      "\n",
      "```python\n",
      "# Using the {} syntax\n",
      "person = {\"name\": \"John\", \"age\": 30, \"city\": \"New York\"}\n",
      "\n",
      "# Using the dict() function\n",
      "person = dict(name=\"John\", age=30, city=\"New York\")\n",
      "```\n",
      "\n",
      "**Accessing Dictionary Values**\n",
      "------------------------------\n",
      "\n",
      "You can access the values in a dictionary using the key.\n",
      "\n",
      "```python\n",
      "\n",
      "\n",
      "\n",
      "===== ‚úÖ STREAMING RESPONSE =====\n",
      "\n",
      "**Python Dictionaries**\n",
      "=======================\n",
      "\n",
      "A dictionary in Python is a mutable data type that stores mappings of unique keys to values. It is an unordered collection of key-value pairs, where each key is unique and maps to a specific value.\n",
      "\n",
      "**Creating a Dictionary**\n",
      "------------------------\n",
      "\n",
      "A dictionary can be created using the `dict()` function or by using curly brackets `{}`.\n",
      "\n",
      "```python\n",
      "# Using the dict() function\n",
      "person = dict(name='John', age=30, city='New York')\n",
      "\n",
      "# Using curly brackets\n",
      "person = {'name': 'John', 'age': 30, 'city': 'New York'}\n",
      "```\n",
      "\n",
      "**Accessing Dictionary Values**\n",
      "------------------------------\n",
      "\n",
      "You can access the values in a dictionary using the key.\n",
      "\n",
      "```python\n",
      "\n",
      "‚úÖ Streaming completed.\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# üìò SECTION 1 ‚Äî Define a Common Prompt\n",
    "# ------------------------------------------------------------\n",
    "# Why?\n",
    "#   - We must use the SAME prompt for fair comparison\n",
    "#   - This helps us isolate the effect of streaming only\n",
    "# ============================================================\n",
    "\n",
    "prompt = \"Explain Python dictionaries with a small example.\"\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# üìò SECTION 2 ‚Äî Non-Streaming Request (Normal Mode)\n",
    "# ------------------------------------------------------------\n",
    "# Why?\n",
    "#   - This is the traditional way of calling an LLM\n",
    "#   - The response is returned ONLY after the model finishes\n",
    "# ============================================================\n",
    "\n",
    "print(\"===== ‚ùå NON-STREAMING RESPONSE =====\\n\")\n",
    "\n",
    "response_normal = client.chat.completions.create(\n",
    "    model=\"llama-3.1-8b-instant\",\n",
    "    messages=[{\"role\":\"user\",\"content\":prompt}],\n",
    "    temperature=0.3,\n",
    "    top_p=0.9,\n",
    "    max_tokens=150\n",
    ")\n",
    "\n",
    "# Extract full response text\n",
    "normal_output = response_normal.choices[0].message.content\n",
    "print(normal_output)\n",
    "\n",
    "# ============================================================\n",
    "# üìò SECTION 3 ‚Äî Streaming Request (Real-Time Mode)\n",
    "# ------------------------------------------------------------\n",
    "# Why?\n",
    "#   - With streaming, output arrives token-by-token\n",
    "#   - This improves perceived speed and UX\n",
    "# ============================================================\n",
    "\n",
    "print(\"\\n\\n===== ‚úÖ STREAMING RESPONSE =====\\n\")\n",
    "\n",
    "stream = client.chat.completions.create(\n",
    "    model=\"llama-3.1-8b-instant\",\n",
    "    messages=[{\"role\":\"user\",\"content\":prompt}],\n",
    "    temperature=0.3,\n",
    "    top_p=0.9,\n",
    "    max_tokens=150,\n",
    "    stream=True\n",
    ")\n",
    "\n",
    "for chunk in stream:\n",
    "    delta_text = chunk.choices[0].delta.content\n",
    "    if delta_text:\n",
    "        print(delta_text, end=\"\",flush=True)\n",
    "\n",
    "print(\"\\n\\n‚úÖ Streaming completed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccb084b6",
   "metadata": {},
   "source": [
    "#### **‚úÖ Observations (Topic 3 ‚Äî Streaming vs Non-Streaming)**\n",
    "\n",
    "- Non-streaming:\n",
    "  - Output appears only after full computation\n",
    "  - User waits without feedback\n",
    "\n",
    "- Streaming:\n",
    "  - Output appears token-by-token\n",
    "  - First token arrives quickly\n",
    "  - Feels faster and more interactive\n",
    "\n",
    "Important:\n",
    "Streaming improves **perceived performance**, not model speed.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "435150a6",
   "metadata": {},
   "source": [
    "üß† Interview & Real-World Insight (Very Important)\n",
    "- Streaming does NOT reduce total latency\n",
    "- It reduces user anxiety\n",
    "- It increases trust\n",
    "- It is mandatory for:\n",
    "- ChatGPT-like UIs\n",
    "- Streamlit apps\n",
    "- Long answers\n",
    "- Agents that ‚Äúthink aloud‚Äù\n",
    "\n",
    "**‚úÖ Topic 3 Final Summary**\n",
    "- Compared streaming vs non-streaming using the same prompt\n",
    "- Learned streaming improves UX, not computation speed\n",
    "- Understood why streaming is industry standard for chat apps"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e785c16",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "42dc57e8",
   "metadata": {},
   "source": [
    "#### **üìÑ TOPIC 4 ‚Äî Stream Lifecycle & UX Pitfalls (Concept + Senior Insight)**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13ed6b70",
   "metadata": {},
   "source": [
    "Streaming is not just ‚Äúprinting tokens‚Äù.\n",
    "In real systems, streaming has a **lifecycle** and **UX implications** that engineers must understand.\n",
    "\n",
    "---\n",
    "\n",
    "#### **üîÅ Streaming Lifecycle (High-Level)**\n",
    "\n",
    "A streaming request follows this flow:\n",
    "\n",
    "1. **Request Sent**\n",
    "   - Client sends prompt to the LLM API\n",
    "   - No output yet\n",
    "\n",
    "2. **Stream Starts**\n",
    "   - The first token arrives\n",
    "   - UI should immediately show activity\n",
    "\n",
    "3. **Token Flow**\n",
    "   - Tokens arrive chunk-by-chunk\n",
    "   - Output is gradually built\n",
    "\n",
    "4. **Stream Ends**\n",
    "   - Model finishes generating\n",
    "   - Stream closes cleanly\n",
    "\n",
    "5. **Post-Processing (Optional)**\n",
    "   - Save conversation\n",
    "   - Update chat history\n",
    "   - Log usage or analytics\n",
    "\n",
    "---\n",
    "\n",
    "#### **üß† Important UX Truth (Interview Point)**\n",
    "\n",
    "Streaming does **NOT** make the model faster.\n",
    "\n",
    "It improves:\n",
    "- Perceived speed\n",
    "- User engagement\n",
    "- Trust\n",
    "- Interactivity\n",
    "\n",
    "Users prefer:\n",
    "> ‚ÄúSomething is happening‚Äù  \n",
    "over  \n",
    "> ‚ÄúBlank screen for 5 seconds‚Äù\n",
    "\n",
    "---\n",
    "\n",
    "#### **‚ö†Ô∏è Common Streaming Pitfalls (Very Important)**\n",
    "\n",
    "#### **‚ùå Pitfall 1 ‚Äî Thinking Streaming Is Faster Computation**\n",
    "- Total response time is usually the same\n",
    "- Only the **first token arrives earlier**\n",
    "\n",
    "#### **‚ùå Pitfall 2 ‚Äî Forgetting to Flush Output**\n",
    "- Without `flush=True`, output may appear delayed\n",
    "- Especially problematic in terminals and logs\n",
    "\n",
    "#### **‚ùå Pitfall 3 ‚Äî Mixing Business Logic with Streaming**\n",
    "Bad practice:\n",
    "- Parsing JSON\n",
    "- Updating DB\n",
    "- Calling tools\n",
    "inside the streaming loop\n",
    "\n",
    "**Streaming loop should be **output-only**.**\n",
    "\n",
    "---\n",
    "\n",
    "#### **‚ùå Pitfall 4 ‚Äî Not Handling Interruptions**\n",
    "In real apps:\n",
    "- User may close the page\n",
    "- Network may break\n",
    "- Request may be cancelled\n",
    "\n",
    "**Good systems:**\n",
    "- Gracefully stop streaming\n",
    "- Clean up resource\n",
    "\n",
    "**Streaming logic should be:**\n",
    "- Simple\n",
    "- Isolated\n",
    "- UI-focused\n",
    "\n",
    "**Business logic should be:**\n",
    "- Outside the streaming loop\n",
    "- Executed after stream completes\n",
    "\n",
    "**This separation prevents bugs and race conditions.**\n",
    "\n",
    "---\n",
    "\n",
    "#### **üèóÔ∏è Where This Matters Most**\n",
    "- Chatbots\n",
    "- Streamlit apps\n",
    "- Agents showing ‚Äúthinking‚Äù\n",
    "- Long-form answers\n",
    "- Live dashboards\n",
    "\n",
    "---\n",
    "\n",
    "#### **‚úÖ Topic 4 Summary**\n",
    "\n",
    "- Streaming has a clear lifecycle\n",
    "- Streaming improves UX, not computation speed\n",
    "- Many bugs come from misunderstanding streaming\n",
    "- Clean separation of concerns is critical"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1455c8bf",
   "metadata": {},
   "source": [
    "#### **üìÑ TOPIC 5 ‚Äî Where Streaming Is Used (Architecture Perspective)**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dc26109",
   "metadata": {},
   "source": [
    "#### üèóÔ∏è Topic 5 ‚Äî Where Streaming Is Used in Real Architectures\n",
    "\n",
    "Streaming is not a ‚Äúnice-to-have‚Äù.\n",
    "In modern GenAI systems, it is a **core architectural decision**.\n",
    "\n",
    "This section explains **where** and **why** streaming is used.\n",
    "\n",
    "---\n",
    "\n",
    "#### **ü§ñ 1. Chatbots & Assistants**\n",
    "\n",
    "#### **Architecture Flow:**\n",
    "User ‚Üí LLM API ‚Üí Streaming Tokens ‚Üí UI\n",
    "\n",
    "#### **Why Streaming?**\n",
    "- Users expect ChatGPT-like behavior\n",
    "- Long answers feel interactive\n",
    "- First token arrives fast\n",
    "\n",
    "#### **Without Streaming:**\n",
    "- Blank screen\n",
    "- Poor UX\n",
    "- Users think the app is slow or broken\n",
    "\n",
    "Streaming is **mandatory** for chatbots.\n",
    "\n",
    "---\n",
    "\n",
    "#### **üñ•Ô∏è 2. Streamlit & Web UIs**\n",
    "\n",
    "#### **Architecture Flow:**\n",
    "Frontend (Streamlit) ‚Üí Backend ‚Üí LLM ‚Üí Stream ‚Üí UI\n",
    "\n",
    "#### **Why Streaming?**\n",
    "- Live typing effect\n",
    "- Better engagement\n",
    "- Prevents UI freezing\n",
    "\n",
    "In Streamlit:\n",
    "- Streaming improves perceived performance\n",
    "- Session state updates feel natural\n",
    "\n",
    "---\n",
    "\n",
    "#### **üß† 3. Agents (Reasoning Systems)**\n",
    "\n",
    "#### **Architecture Flow:**\n",
    "Agent ‚Üí Think ‚Üí Tool ‚Üí Observe ‚Üí Stream reasoning ‚Üí User\n",
    "\n",
    "#### **Why Streaming?**\n",
    "- Users can see the agent ‚Äúthinking‚Äù\n",
    "- Debugging becomes easier\n",
    "- Trust increases\n",
    "\n",
    "Streaming helps explain **multi-step reasoning**.\n",
    "\n",
    "---\n",
    "\n",
    "#### **üìö 4. RAG (Long Answers from Documents)**\n",
    "\n",
    "#### **Architecture Flow:**\n",
    "Query ‚Üí Retrieve chunks ‚Üí Generate answer ‚Üí Stream output\n",
    "\n",
    "#### **Why Streaming?**\n",
    "- RAG answers are often long\n",
    "- Streaming avoids waiting for full synthesis\n",
    "- Improves UX for document-heavy systems\n",
    "\n",
    "---\n",
    "\n",
    "#### üìä 5. Dashboards & Live Insights\n",
    "\n",
    "#### **Architecture Flow:**\n",
    "Data ‚Üí LLM ‚Üí Stream insights ‚Üí Dashboard\n",
    "\n",
    "#### **Why Streaming?**\n",
    "- Progressive insights\n",
    "- Better real-time feel\n",
    "- Useful in analytics and monitoring tools\n",
    "\n",
    "---\n",
    "\n",
    "#### **‚ö†Ô∏è Where Streaming Is NOT Needed**\n",
    "\n",
    "Streaming is usually unnecessary for:\n",
    "- Short JSON extraction\n",
    "- SQL generation\n",
    "- Classification tasks\n",
    "- Background batch jobs\n",
    "\n",
    "In these cases:\n",
    "- Non-streaming is simpler\n",
    "- Easier to parse output\n",
    "- Lower complexity\n",
    "\n",
    "---\n",
    "\n",
    "#### **üß† Senior-Level Decision Rule**\n",
    "\n",
    "Use streaming when:\n",
    "- Output is long\n",
    "- UX matters\n",
    "- User is waiting\n",
    "\n",
    "Avoid streaming when:\n",
    "- Output must be parsed\n",
    "- Strict structure is required\n",
    "- Task runs in background\n",
    "\n",
    "---\n",
    "\n",
    "#### **‚úÖ Topic 5 Summary**\n",
    "\n",
    "- Streaming is a UX-driven architectural choice\n",
    "- Essential for chatbots, UIs, agents, and RAG\n",
    "- Not suitable for strict structured outputs\n",
    "- Senior engineers choose streaming intentionally\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5abf5372",
   "metadata": {},
   "source": [
    "#### **üíª TOPIC 6 ‚Äî Micro Practice: Change Parameters & Observe Streaming**\n",
    "üéØ Goal (Very Important)\n",
    "\n",
    "- Build intuition, not memory\n",
    "- See how small parameter changes affect streaming output\n",
    "- Understand what to use in real systems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "820fe5c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== Low temperature (factual) =====\n",
      "\n",
      "**Python Sets**\n",
      "===============\n",
      "\n",
      "Python sets are unordered collections of unique elements. They are similar to lists, but they do not allow duplicate values. Sets are useful when you need to store a collection of items and you don't care about the order or duplicates.\n",
      "\n",
      "**Creating a Set**\n",
      "-----------------\n",
      "\n",
      "You can create a set in Python using the `set()` function or by placing elements inside curly brackets `{}`.\n",
      "\n",
      "```python\n",
      "# Creating a set using the set() function\n",
      "my_set = set([1, 2, 3, 4, 5])\n",
      "print(my_set)  #\n",
      "\n",
      "-- Streaming ended --\n",
      "\n",
      "\n",
      "===== Medium temperature (balanced) =====\n",
      "\n",
      "**Python Sets**\n",
      "================\n",
      "\n",
      "Python sets are unordered collections of unique elements. They are similar to lists, but they do not allow duplicate values. Sets are useful when you need to perform mathematical set operations such as union, intersection, and difference.\n",
      "\n",
      "**Creating a Set**\n",
      "-----------------\n",
      "\n",
      "You can create a set in Python using the `set()` function or by using the `{}` syntax.\n",
      "\n",
      "```python\n",
      "# Using the set() function\n",
      "my_set = set([1, 2, 3, 2, 4])\n",
      "print(my_set)  # Output: {1, 2,\n",
      "\n",
      "-- Streaming ended --\n",
      "\n",
      "\n",
      "===== High temperature (creative) =====\n",
      "\n",
      "**Python Sets**\n",
      "\n",
      "Python sets are an unordered collection of unique elements. They are useful for performing mathematical operations like union, intersection, and difference.\n",
      "\n",
      "### Creating a Set\n",
      "\n",
      "Here's a simple example of creating a set in Python:\n",
      "\n",
      "```python\n",
      "fruits = {\"apple\", \"banana\", \"cherry\"}\n",
      "\n",
      "print(fruits)  # Output: {'banana', 'apple', 'cherry'}\n",
      "```\n",
      "\n",
      "### Adding an Element to a Set\n",
      "\n",
      "You can add an element to a set using the `add()` method:\n",
      "\n",
      "```python\n",
      "fruits = {\"apple\", \"banana\", \"cherry\n",
      "\n",
      "-- Streaming ended --\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# üß™ TOPIC 6 ‚Äî Micro Practice: Streaming + Parameter Changes\n",
    "# ------------------------------------------------------------\n",
    "# Goal:\n",
    "#   - Use streaming\n",
    "#   - Change ONE parameter at a time\n",
    "#   - Observe how output behavior changes\n",
    "#\n",
    "# Rule:\n",
    "#   - Do NOT memorize\n",
    "#   - Just observe and understand\n",
    "# ============================================================\n",
    "\n",
    "prompt = \"Explain Python sets with a simple example.\"\n",
    "\n",
    "configs=[\n",
    "    {\n",
    "        \"label\":\"Low temperature (factual)\",\n",
    "        \"temperature\":0.1,\n",
    "        \"top_p\":0.9\n",
    "    },\n",
    "\n",
    "    {\n",
    "        \"label\":\"Medium temperature (balanced)\",\n",
    "        \"temperature\":0.6,\n",
    "        \"top_p\":0.9\n",
    "    },\n",
    "\n",
    "    {\n",
    "        \"label\":\"High temperature (creative)\",\n",
    "        \"temperature\":1.1,\n",
    "        \"top_p\":1.0\n",
    "\n",
    "    }\n",
    "]\n",
    "\n",
    "\n",
    "for cfg in configs:\n",
    "    print(f\"\\n===== {cfg['label']} =====\\n\")\n",
    "\n",
    "    stream = client.chat.completions.create(\n",
    "        model=\"llama-3.1-8b-instant\",\n",
    "        messages=[{\"role\":\"user\",\"content\":prompt}],\n",
    "        temperature=cfg[\"temperature\"],\n",
    "        top_p=cfg[\"top_p\"],\n",
    "        max_tokens=120,\n",
    "        stream=True\n",
    "    )\n",
    "\n",
    "    for chunk in stream:\n",
    "        delta_text = chunk.choices[0].delta.content\n",
    "        if delta_text:\n",
    "            print(delta_text, end=\"\",flush=True)\n",
    "\n",
    "    print(\"\\n\\n-- Streaming ended --\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b769c4bd",
   "metadata": {},
   "source": [
    "#### **‚úÖ Topic 6 ‚Äî Micro Practice Reflection**\n",
    "\n",
    "- Streaming output changes significantly with temperature and top_p.\n",
    "- Low temperature is best for factual answers.\n",
    "- Medium temperature feels best for teaching.\n",
    "- High temperature increases creativity.\n",
    "- Streaming + tuning builds intuition, not memorization.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fccdf885",
   "metadata": {},
   "source": [
    "#### **üßæ TOPIC 7 ‚Äî Final Daily Summary & Closure**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23620835",
   "metadata": {},
   "source": [
    "#### **What I learned today**\n",
    "- Streaming means receiving LLM output token-by-token instead of waiting for the full response.\n",
    "- Streaming improves **user experience**, not model computation speed.\n",
    "- Streaming is essential for chatbots, Streamlit apps, agents, and long answers.\n",
    "- The streaming lifecycle has clear stages: request ‚Üí tokens ‚Üí completion.\n",
    "- Poor streaming design leads to UX and architectural issues.\n",
    "\n",
    "#### **Practical understanding gained**\n",
    "- Implemented streaming using `stream=True`.\n",
    "- Observed how tokens arrive incrementally.\n",
    "- Compared streaming vs non-streaming responses.\n",
    "- Experimented with temperature and top_p to build intuition.\n",
    "- Learned when NOT to use streaming (JSON, strict parsing).\n",
    "\n",
    "#### **Key engineering insight**\n",
    "Streaming logic should be:\n",
    "- Simple\n",
    "- UI-focused\n",
    "- Isolated from business logic\n",
    "\n",
    "#### **Where I will use this**\n",
    "- Chatbots\n",
    "- Streamlit UIs\n",
    "- RAG systems\n",
    "- Agent reasoning display"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
