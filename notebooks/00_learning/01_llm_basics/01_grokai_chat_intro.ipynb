{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "47c896c7",
   "metadata": {},
   "source": [
    "#### **üß© üìò Code Cell 1 ‚Äî Configure Grok Client (with Headings + Comments)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "4feeffe9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Groq client configured successfully and ready to use.\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# üìò SECTION 1 ‚Äî Import Required Libraries\n",
    "# ------------------------------------------------------------\n",
    "# Why?\n",
    "#   - os: Interact with operating system (read env variables)\n",
    "#   - dotenv: Load API keys from .env files securely\n",
    "#   - OpenAI: Used because Groq follows the OpenAI-compatible API format\n",
    "# ============================================================\n",
    "\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI\n",
    "\n",
    "# ============================================================\n",
    "# üìò SECTION 2 ‚Äî Load Environment Variables (.env.dev)\n",
    "# ------------------------------------------------------------\n",
    "# Why?\n",
    "#   - Keeps API keys OUT of source code\n",
    "#   - Allows switching between environments:\n",
    "#       dev / uat / prod\n",
    "#   - Uses python-dotenv to read envs/.env.dev\n",
    "# ============================================================\n",
    "\n",
    "load_dotenv(\"../../../envs/.env.dev\")\n",
    "\n",
    "# ============================================================\n",
    "# üìò SECTION 3 ‚Äî Read the Groq API Key from Environment\n",
    "# ------------------------------------------------------------\n",
    "# Why?\n",
    "#   - GROQ_API_KEY is stored safely in .env.dev\n",
    "#   - Using os.getenv ensures security + flexibility\n",
    "#   - If the key is missing, we raise an error immediately\n",
    "# ============================================================\n",
    "\n",
    "groq_api_key = os.getenv(\"GROQ_API_KEY\")\n",
    "\n",
    "if not groq_api_key:\n",
    "    raise RuntimeError(\n",
    "        \"‚ùå GROQ_API_KEY is missing. Please verify it exists inside envs/.env.dev\"\n",
    "    )\n",
    "\n",
    "# ============================================================\n",
    "# üìò SECTION 4 ‚Äî Create the Groq Client (OpenAI-compatible)\n",
    "# ------------------------------------------------------------\n",
    "# Why?\n",
    "#   - Groq uses OpenAI-style API endpoints\n",
    "#   - base_url MUST be set to https://api.groq.com/openai/v1\n",
    "#   - After this, we can use:\n",
    "#         client.chat.completions.create(...)\n",
    "#   - This client object will be reused in all other notebook cells\n",
    "# ============================================================\n",
    "\n",
    "client = OpenAI(\n",
    "    api_key=groq_api_key,\n",
    "    base_url=\"https://api.groq.com/openai/v1\"\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Groq client configured successfully and ready to use.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b19d308",
   "metadata": {},
   "source": [
    "#### **First Grok Chat Request (Hello World LLM request)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "ee690d6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ü§ñ Groq says:\n",
      "\n",
      "Nice to meet you! I'm Groq, your friendly Python tutor, here to help you learn and grow with the amazing world of Python programming. I'll break down complex concepts into simple and easy-to-understand language. Let's get coding!\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# üìò SECTION 5 ‚Äî First Chat Request to Groq (Hello World)\n",
    "# ------------------------------------------------------------\n",
    "# Goal:\n",
    "#   - Verify that the Groq client works end-to-end\n",
    "#   - Send a simple question to the model\n",
    "#   - Receive and print the assistant's reply\n",
    "# ============================================================\n",
    "\n",
    "# 1Ô∏è‚É£ Build the messages list (conversation context)\n",
    "messages = [\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": \"You are a friendly Python tutor. Explain things in simple language.\"\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"Hello Groq! This is my first request. Please introduce yourself in 2‚Äì3 lines.\"\n",
    "    }\n",
    "]\n",
    "\n",
    "# 2Ô∏è‚É£ Send the chat completion request to Groq\n",
    "#    Using a supported model: llama-3.1-8b-instant\n",
    "response = client.chat.completions.create(\n",
    "    model=\"llama-3.1-8b-instant\",\n",
    "    messages=messages\n",
    ")\n",
    "\n",
    "# 3Ô∏è‚É£ Extract the assistant's reply\n",
    "#    IMPORTANT:\n",
    "#    - response.choices[0].message is an object (ChatCompletionMessage)\n",
    "#    - So we must use `.content` (attribute), NOT [\"content\"] (dict style)\n",
    "assistant_reply = response.choices[0].message.content\n",
    "\n",
    "# 4Ô∏è‚É£ Print reply\n",
    "print(\"ü§ñ Groq says:\\n\")\n",
    "print(assistant_reply)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd0afe25",
   "metadata": {},
   "source": [
    "#### **Inspect the Raw Response Object**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "dd900199",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======= RAW RESPONSE OBJECT =======\n",
      "ChatCompletion(id='chatcmpl-7e162915-bc11-4a48-be41-a0779561acb1', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content=\"In the context of Large Language Model (LLM) APIs, a Response Object typically contains the output of the model's processing, such as:\\n\\n1. The model's response text or generated text.\\n2. Metadata about the request, such as input parameters, model selection, and task details.\\n3. Additional information, like confidence scores, sentiment analysis, or entity recognition results.\\n\\nThink of a response object as a 'package' that delivers the model's output and related data to your application.\", refusal=None, role='assistant', annotations=None, audio=None, function_call=None, tool_calls=None))], created=1765956595, model='llama-3.1-8b-instant', object='chat.completion', service_tier='on_demand', system_fingerprint='fp_4387d3edbb', usage=CompletionUsage(completion_tokens=100, prompt_tokens=61, total_tokens=161, completion_tokens_details=None, prompt_tokens_details=None, queue_time=0.055063995, prompt_time=0.003313195, completion_time=0.182903905, total_time=0.1862171), usage_breakdown=None, x_groq={'id': 'req_01kcnkebh8f20sf6mqagbdb0jb', 'seed': 1252408281})\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# üìò SECTION 6 ‚Äî Inspecting the Raw Response Object\n",
    "# ------------------------------------------------------------\n",
    "# Why?\n",
    "#   - To see the full structure returned by the LLM.\n",
    "#   - This helps us understand:\n",
    "#       * where the model's reply lives\n",
    "#       * how choices[] is structured\n",
    "#       * how we might access metadata later (tokens, model, etc.)\n",
    "# ============================================================\n",
    "\n",
    "# üß© 1) Build a simple messages list for testing\n",
    "#    Why?\n",
    "#      - We send a short, clear question so the response object\n",
    "#        is easy to read and understand.\n",
    "\n",
    "messages = [\n",
    "    {\n",
    "        \"role\":\"system\",\n",
    "        \"content\":\"You are a helpful assistant who explains things simply:\" \n",
    "    },\n",
    "    {\n",
    "        \"role\":\"user\",\n",
    "        \"content\":\"What is a response object in the context of LLM APIs? Explain briefly.\"\n",
    "    }\n",
    "]\n",
    "\n",
    "# üß© 2) Send a chat completion request to Groq\n",
    "#    Why?\n",
    "#      - Same pattern as before:\n",
    "#          client.chat.completions.create(...)\n",
    "#      - We use the same model as in Section 5.\n",
    "response = client.chat.completions.create(\n",
    "    model=\"llama-3.1-8b-instant\",\n",
    "    messages=messages\n",
    ")\n",
    "\n",
    "# üß© 3) Print the entire response object\n",
    "#    Why?\n",
    "#      - For learning, we want to see everything Groq returns.\n",
    "#      - In real applications, we wouldn't print this every time.\n",
    "\n",
    "print(\"======= RAW RESPONSE OBJECT =======\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "260689b7",
   "metadata": {},
   "source": [
    "#### **Extract Key Fields from the Response Object**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "54a4ee68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===== üîé RESPONSE SUMMARY =====\n",
      "Model Used    :llama-3.1-8b-instant\n",
      "Assistant role :assistant\n",
      "Finish reason  :stop\n",
      "\n",
      "{'----- üß† Assistant Reply -----'}\n",
      "In the context of Large Language Model (LLM) APIs, a Response Object typically contains the output of the model's processing, such as:\n",
      "\n",
      "1. The model's response text or generated text.\n",
      "2. Metadata about the request, such as input parameters, model selection, and task details.\n",
      "3. Additional information, like confidence scores, sentiment analysis, or entity recognition results.\n",
      "\n",
      "Think of a response object as a 'package' that delivers the model's output and related data to your application.\n",
      "\n",
      "----- üìä Token Usage -----\n",
      "Prompt tokens    :61\n",
      "completion_tokens   :100\n",
      "Total Tokens   :161\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# üìò SECTION 7 ‚Äî Extracting Important Fields from Response\n",
    "# ------------------------------------------------------------\n",
    "# Goal:\n",
    "#   - Understand how to read specific parts of the response:\n",
    "#       * Model name\n",
    "#       * Assistant role\n",
    "#       * Assistant message (reply)\n",
    "#       * Finish reason\n",
    "#       * Token usage (if available)\n",
    "#\n",
    "# Note:\n",
    "#   - This assumes 'response' already exists from SECTION 6.\n",
    "#   - If not, re-run SECTION 6 before running this cell.\n",
    "# ============================================================\n",
    "# üß© 1) Extract the model name\n",
    "#    Why?\n",
    "#      - Useful for logging, debugging, and knowing which LLM handled the request.\n",
    "\n",
    "model_name = response.model\n",
    "\n",
    "# üß© 2) Extract the first choice (index 0)\n",
    "#    Why?\n",
    "#      - Most calls only care about the first suggested answer.\n",
    "\n",
    "first_choice = response.choices[0]\n",
    "\n",
    "# üß© 3) Extract the assistant's message role and content\n",
    "#    Why?\n",
    "#      - role  ‚Üí usually \"assistant\"\n",
    "#      - content ‚Üí actual reply text from the model\n",
    "\n",
    "assistant_role = first_choice.message.role\n",
    "assistant_content = first_choice.message.content\n",
    "\n",
    "# üß© 4) Extract the finish reason\n",
    "#    Why?\n",
    "#      - Tells us WHY the model stopped generating:\n",
    "#          * \"stop\"        ‚Üí completed naturally\n",
    "#          * \"length\"      ‚Üí hit max_tokens limit\n",
    "#          * \"content_filter\" ‚Üí blocked by safety filter (in some providers)\n",
    "\n",
    "finish_reason = first_choice.finish_reason\n",
    "\n",
    "# üß© 5) Extract token usage (if available)\n",
    "#    Why?\n",
    "#      - Helps us understand cost and length of prompts/responses.\n",
    "#      - Some providers may not always return usage; we handle that safely.\n",
    "\n",
    "usage_info = getattr(response,\"usage\",None)\n",
    "\n",
    "if usage_info:\n",
    "    prompt_tokens = usage_info.prompt_tokens\n",
    "    completion_tokens = usage_info.completion_tokens\n",
    "    total_tokens = usage_info.total_tokens\n",
    "else:\n",
    "    prompt_tokens = completion_tokens = total_tokens = None\n",
    "\n",
    "# üß© 6) Print everything in a clean, readable way\n",
    "\n",
    "print(\"===== üîé RESPONSE SUMMARY =====\")\n",
    "print(f\"Model Used    :{model_name}\")\n",
    "print(f\"Assistant role :{assistant_role}\")\n",
    "print(f\"Finish reason  :{finish_reason}\")\n",
    "print()\n",
    "print({\"----- üß† Assistant Reply -----\"})\n",
    "print(assistant_content)\n",
    "print()\n",
    "\n",
    "if usage_info:\n",
    "    print(\"----- üìä Token Usage -----\")\n",
    "    print(f\"Prompt tokens    :{prompt_tokens}\")\n",
    "    print(f\"completion_tokens   :{completion_tokens}\")\n",
    "    print(f\"Total Tokens   :{total_tokens}\")\n",
    "else:\n",
    "    print(\"Token usage information not provided by this response.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "391a943f",
   "metadata": {},
   "source": [
    "#### **Why These Fields Matter in REAL GenAI Projects**\n",
    "\n",
    "(One step only. No code yet ‚Äî pure understanding.)\n",
    "\n",
    "Before building:\n",
    "- Chatbots\n",
    "- RAG systems\n",
    "- Agents\n",
    "- Evaluators\n",
    "- Streamlit apps\n",
    "- FastAPI endpoints\n",
    "- Workflow automation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc7dacb5",
   "metadata": {},
   "source": [
    "#### **Understanding Field Importance (Simple & Practical)**\n",
    "\n",
    "**Why Response Fields Matter in Real GenAI Projects**\n",
    "\n",
    "**1Ô∏è‚É£ model ‚Äî Which brain answered your question**\n",
    "\n",
    "- Helps track which model produced what output\n",
    "- Useful in logs & debugging\n",
    "- Important when switching models for performance or cost\n",
    "- In production, you often A/B test multiple models\n",
    "\n",
    "\n",
    "Example:\n",
    "\n",
    "- llama-3.1-8b-instant ‚Üí fast, cheap, good for simple tasks\n",
    "- llama-3.1-70b-versatile ‚Üí slower, expensive, high quality\n",
    "\n",
    "\n",
    "**2Ô∏è‚É£ choices[0].message.role ‚Äî Usually assistant**\n",
    "\n",
    "Why it matters:\n",
    "\n",
    "- Ensures you‚Äôre reading the right message\n",
    "- Maintains consistent chat structure\n",
    "- Needed for chat history formatting\n",
    "\n",
    "**3Ô∏è‚É£ choices[0].message.content ‚Äî The actual answer**\n",
    "\n",
    "This is the core output used in:\n",
    "- Chatbots\n",
    "- RAG responses\n",
    "- SQL generator bots\n",
    "- Code generators\n",
    "- Multimodal apps\n",
    "- Streamlit apps\n",
    "- FastAPI endpoints\n",
    "\n",
    "**4Ô∏è‚É£ finish_reason ‚Äî Why the model stopped writing**\n",
    "\n",
    "| finish_reason      | Meaning              |\n",
    "| ------------------ | -------------------- |\n",
    "| `\"stop\"`           | Completed normally   |\n",
    "| `\"length\"`         | Hit max_tokens limit |\n",
    "| `\"content_filter\"` | Safety block         |\n",
    "| `\"error\"`          | Model failure        |\n",
    "\n",
    "Why it matters:\n",
    "\n",
    "- If \"length\", you must increase max_tokens\n",
    "- If \"content_filter\", your input might be restricted\n",
    "- Used in production monitoring\n",
    "\n",
    "**5Ô∏è‚É£ usage ‚Äî Token cost + performance indicator**\n",
    "\n",
    "If available, contains:\n",
    "\n",
    "- prompt_tokens\n",
    "- completion_tokens\n",
    "- total_tokens\n",
    "\n",
    "Why it matters:\n",
    "- Cost = based on tokens\n",
    "- Performance tuning\n",
    "- Budget control in production\n",
    "- Monitoring usage per request, per user, per endpoint\n",
    "\n",
    "Even if Groq doesn‚Äôt always return usage, understanding it is essential for:\n",
    "- OpenAI\n",
    "- Anthropic\n",
    "- Gemini\n",
    "- Azure OpenAI\n",
    "\n",
    "\n",
    "6Ô∏è‚É£ Why this entire structure matters\n",
    "\n",
    "You‚Äôll use these fields in:\n",
    "\n",
    "‚úî RAG\n",
    "\n",
    "Monitor reason, track chunks, improve retrieval.\n",
    "\n",
    "‚úî Agents\n",
    "\n",
    "Determine when to stop, retry, or dispatch.\n",
    "\n",
    "‚úî Evaluations\n",
    "\n",
    "Compare output quality across models.\n",
    "\n",
    "‚úî Monitoring dashboards\n",
    "\n",
    "Track per-request cost, latency, and tokens.\n",
    "\n",
    "‚úî Debugging\n",
    "\n",
    "See why a model behaved unexpectedly.\n",
    "\n",
    "‚úî Production logs\n",
    "\n",
    "Every LLM call is logged with:\n",
    "- model\n",
    "- tokens\n",
    "- user prompt\n",
    "- output\n",
    "- finish_reason\n",
    "\n",
    "\n",
    "**üéØ Summary (Remember This!)**\n",
    "\n",
    "This response object is the foundation of everything in GenAI.\n",
    "\n",
    "If you understand this structure deeply:\n",
    "\n",
    "You can build ANY system:\n",
    "\n",
    "- chatbots\n",
    "- RAG\n",
    "- agents\n",
    "- multimodal apps\n",
    "- LLM APIs\n",
    "- batch processing\n",
    "- evaluation frameworks\n",
    "- enterprise AI systems"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ab7688f",
   "metadata": {},
   "source": [
    "#### **LLM API Concepts Explained (Human-Friendly, Deep, Practical)**\n",
    "\n",
    "**‚≠ê 1. client.chat.completions.create(...)**\n",
    "\n",
    "This is the heart of every LLM request.\n",
    "\n",
    "**‚úî What does it do?**\n",
    "\n",
    "- Sends your messages (conversation) to the LLM\n",
    "- Tells the model which brain (model) to use\n",
    "- Returns the model‚Äôs answer\n",
    "\n",
    "**‚úî When do we use it?**\n",
    "\n",
    "Always.\n",
    "Every chatbot, RAG system, agent, app, or API uses this function.\n",
    "\n",
    "**‚úî Why ‚Äúchat‚Äù?**\n",
    "\n",
    "Even if you send one message, the model still works in a chat format with roles."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60910eac",
   "metadata": {},
   "source": [
    "**‚≠ê 2. messages=[...]**\n",
    "\n",
    "**‚úî What does this list represent?**\n",
    "\n",
    "This is the conversation history.\n",
    "\n",
    "**Each item has:**\n",
    "\n",
    "{\"role\": \"system\" / \"user\" / \"assistant\", \"content\": \"...\"}\n",
    "\n",
    "**‚úî Why do we need roles?**\n",
    "\n",
    "- system ‚Üí controls personality & rules\n",
    "- user ‚Üí what you are asking\n",
    "- assistant ‚Üí previous model replies (for multi-turn chat)\n",
    "\n",
    "**‚úî Real scenarios:**\n",
    "\n",
    "- Chatbot\n",
    "- SQL Bot\n",
    "- Business assistant\n",
    "- RAG system with memory\n",
    "- Multi-agent workflows\n",
    "\n",
    "Messages = context.\n",
    "\n",
    "**‚≠ê 3. choices[0]**\n",
    "\n",
    "**‚úî Why ‚Äúchoices‚Äù?**\n",
    "\n",
    "LLMs can generate multiple outputs, like:\n",
    "\n",
    "- Choice 1\n",
    "- Choice 2\n",
    "- Choice 3\n",
    "\n",
    "But we usually want the first one. \n",
    "choices[0]\n",
    "‚ÄúGive me the first answer from the model.\n",
    "\n",
    "**‚úî Real scenario:**\n",
    "\n",
    "99% of industry apps use only choices[0]."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ac0a55b",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b01ecab0",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d3b5e9ea",
   "metadata": {},
   "source": [
    "#### **User Prompt: Clarity, Length & Style Control**\n",
    "\n",
    "\n",
    "How you write the user prompt changes the output quality by 70‚Äì80%.\n",
    "\n",
    "- Why vague prompts fail\n",
    "- Why specific prompts win\n",
    "- How length and detail affect reasoning\n",
    "- How structure affects reliability\n",
    "- How to write prompts like a Google/Microsoft engineer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "2cf20038",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===== ‚ùå Version A ‚Äî Vague Prompt =====\n",
      "**Python Variable**\n",
      "\n",
      "A Python variable is a symbolic name that can be used to store, reference, and manipulate a value. Variables are crucial in programming as they allow us to store and use data easily.\n",
      "\n",
      "**Simple Example**\n",
      "\n",
      "```python\n",
      "x = 5\n",
      "print(x)\n",
      "```\n",
      "\n",
      "In this example, we declare a variable `x` and assign it the value `5`. We then print the value of `x` to the console, which outputs `5`.\n",
      "\n",
      "**Real-World Analogy**\n",
      "\n",
      "Think of a variable like a labeled box in a storage room. You can put any item of value (e.g., a book, a toy, or a file) inside the box, and then later use the box's label to retrieve the item. Similarly, in programming, you can store a value in a variable and later use the variable's name to access that value.\n",
      "\n",
      "**Common Mistake Beginners Make**\n",
      "\n",
      "One common mistake beginners make with Python variables is not understanding the concept of **scope**. Variable scope refers to the region of the code where a variable can be accessed. If a variable is defined within a block of code (e.g., inside a `for` loop or a function), it is only accessible within that block. Try to access the variable outside that block, and you will encounter a `NameError`.\n",
      "\n",
      "```python\n",
      "x = 10  # global scope, accessible everywhere\n",
      "\n",
      "def my_function():\n",
      "    x = 20  # local scope, accessible only within this function\n",
      "\n",
      "print(x)  # prints: 10\n",
      "my_function()\n",
      "print(x)  # prints: 10 again, because it's still global scope\n",
      "print(x)  # prints: 20, but only after my_function() ends\n",
      "```\n",
      "\n",
      "However, when you try to access the variable from within a child scope:\n",
      "```python\n",
      "x = 10\n",
      "\n",
      "def my_function():\n",
      "    print(x) #NameError: name 'x' is not defined.\n",
      "\n",
      "my_function()\n",
      "```\n",
      "It is always best to explicitly declare your variables' scope to avoid confusion.\n",
      "\n",
      "**Interview-Style Points**\n",
      "\n",
      "1. **Variable Assignment**: How do you assign a new value to a variable in Python? (Answer: Using the `=` operator, e.g., `x = 5`)\n",
      "\n",
      "```python\n",
      "x = 5\n",
      "x = 10  # assigns a new value to x\n",
      "```\n",
      "\n",
      "2. **Variable Type**: How can you print the type of a variable in Python? (Answer: Using the `type()` function, e.g., `print(type(x))`)\n",
      "\n",
      "```python\n",
      "x = 10\n",
      "print(type(x))  # prints: <class 'int'>\n",
      "```\n",
      "\n",
      "3. **Variable Shadowing**: Can you explain what happens when a variable is re-declared within a child scope? How does it affect the value of the variable in the parent scope? (Answer: This is a classic case of variable shadowing. The child variable is said to \"shadow\" the parent variable, effectively making it inaccessible from within the child scope.)\n",
      "\n",
      "```python\n",
      "x = 10\n",
      "\n",
      "def child_function():\n",
      "    x = 20\n",
      "    print(\"Inside child function: \", x)  # prints: 20\n",
      "    return x\n",
      "\n",
      "print(\"Outside child function: \", x)  # still prints: 10\n",
      "x = child_function()\n",
      "print(\"After assignment from child function: \", x) # prints: 20\n",
      "```\n",
      "\n",
      "\n",
      "==================================================================== ‚ö†Ô∏è Version B ‚Äî Better Prompt =====\n",
      "**Python Variables**\n",
      "======================\n",
      "\n",
      "In Python, variables are used to store and manipulate data. Variables can hold different types of data such as numbers, strings, lists, etc.\n",
      "\n",
      "**Declaring a Variable**\n",
      "-----------------------\n",
      "\n",
      "To declare a variable in Python, you simply assign a value to a name using the assignment operator (=). Here's an example:\n",
      "\n",
      "```python\n",
      "# Declare a variable named 'name' and assign it the value 'John'\n",
      "name = 'John'\n",
      "```\n",
      "\n",
      "In this example, we're creating a variable named `name` and assigning it the value `'John'`, which is a string.\n",
      "\n",
      "**Example Use Case**\n",
      "---------------------\n",
      "\n",
      "Let's say you want to store the name of a person in a variable and then print it out. Here's how you can do it:\n",
      "\n",
      "```python\n",
      "# Declare a variable named 'name' and assign it the value 'John'\n",
      "name = 'John'\n",
      "\n",
      "# Print out the value of the 'name' variable\n",
      "print(name)  # Output: John\n",
      "```\n",
      "\n",
      "In this example, we first declare a variable `name` and assign it the value `'John'`. Then, we use the `print()` function to output the value of the `name` variable to the console.\n",
      "\n",
      "\n",
      "===================================================================== ‚úÖ Version C ‚Äî Best Prompt =====\n",
      "**Python Variables: A Comprehensive Guide**\n",
      "\n",
      "**Simple Example**\n",
      "\n",
      "In Python, a variable is a name given to a value. Variables are used to store and manipulate data within a program.\n",
      "\n",
      "```python\n",
      "# assigning a value to a variable\n",
      "x = 5\n",
      "\n",
      "# printing the value of the variable\n",
      "print(x)  # output: 5\n",
      "```\n",
      "\n",
      "In this example, we assign the value `5` to the variable `x` using the assignment operator (`=`).\n",
      "\n",
      "**Real World Analogy**\n",
      "\n",
      "Think of variables like labeled boxes in a storage room. When you assign a value to a variable, it's like placing an object inside a labeled box. Later, when you want to use the value, you can simply retrieve it from the labeled box.\n",
      "\n",
      "For instance, imagine we have a variable `username` like a labeled box containing a person's username. When we create the box, we can store a value in it, like \"john.doe\". We can then retrieve the value from the box using the label `username`.\n",
      "\n",
      "**Common Mistake Beginners Make**\n",
      "\n",
      "One common mistake beginners make when using variables is using the same variable name multiple times, often with different values, in the same scope. This is known as reusing a variable or variable shadowing. It can lead to bugs that are difficult to track.\n",
      "\n",
      "Here's an example of how this mistake can occur:\n",
      "\n",
      "```python\n",
      "x = 5\n",
      "print(x)  # output: 5\n",
      "x = \"Hello, World!\"\n",
      "print(x)  # output: Hello, World!\n",
      "```\n",
      "\n",
      "In this example, we start with the value `5` assigned to the variable `x`. Later, we assign a new value `\"Hello, World!\"` to the same variable `x`. This is perfectly valid in Python, and the variable `x` is now holding a string value.\n",
      "\n",
      "**Interview-Style Points**\n",
      "\n",
      "Here are three interview-style points that showcase various aspects of Python variables:\n",
      "\n",
      "1. **What is the difference between `x = 5` and `x = \"5\"` in Python?**\n",
      "\n",
      "When you use `x = 5`, you are assigning a numeric value to the variable `x`. On the other hand, when you use `x = \"5\"`, you are assigning a string value to the variable `x`. In Python, `5` is an integer, while `\"5\"` is a string.\n",
      "\n",
      "2. **How do you assign and print the value of a variable in Python?**\n",
      "\n",
      "In Python, you can assign a value to a variable using the assignment operator (`=`), and print the value of a variable using the `print()` function.\n",
      "\n",
      "```python\n",
      "x = 5\n",
      "print(x)\n",
      "```\n",
      "\n",
      "This code assigns the value `5` to the variable `x` and prints the value of `x`.\n",
      "\n",
      "3. **Why is it a bad practice to reuse a variable name in the same scope in Python?**\n",
      "\n",
      "Reusing a variable name in the same scope can lead to bugs that are difficult to track. For example, if you assign a value to a variable `x`, and later assign a new value to the same variable `x`, the previous value of `x` will be lost, and the new value will be associated with the same variable name. To avoid this, it's best to avoid reusing variable names in the same scope.\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# üìò SECTION 9.2 ‚Äî User Prompt Quality: Clarity, Length & Style\n",
    "# ------------------------------------------------------------\n",
    "# Goal:\n",
    "#   - Understand how different user prompt styles affect output.\n",
    "#   - Learn how clarity, detail, and structure change the response.\n",
    "#\n",
    "# Real-world relevance:\n",
    "#   - Client queries\n",
    "#   - Business requirements\n",
    "#   - Analytics agents\n",
    "#   - Chatbots & assistants\n",
    "#   - SQL generators\n",
    "#   - Coding copilots\n",
    "#   - RAG systems\n",
    "# ============================================================\n",
    "\n",
    "\n",
    "# 1Ô∏è‚É£ Three types of user prompts to compare\n",
    "\n",
    "# ‚ùå Version A ‚Äî Vague, unclear\n",
    "user_prompt_vague = \"Explain Python Variable.\"\n",
    "\n",
    "# ‚ö†Ô∏è Version B ‚Äî Better, more clear\n",
    "user_prompt_medium = 'Explain Python variable with one simple example.'\n",
    "\n",
    "# ‚úÖ Version C ‚Äî Best (Google-level), structured, clear\n",
    "user_prompt_best = \"\"\"Explain Python Variable with:\n",
    "- a simple example\n",
    "- a real world analogy\n",
    "- common mistake beginners make\n",
    "- 3 interview-style point\n",
    "\"\"\"\n",
    "\n",
    "# 2Ô∏è‚É£ Build three message sets (system prompt stays the same)\n",
    "messages_vague = [\n",
    "    {\"role\":\"system\",\n",
    "    \"content\":\"You are an expert Python tutor.\"},\n",
    "    {\"role\":\"user\",\"content\": user_prompt_best}\n",
    "]\n",
    "\n",
    "messages_medium = [\n",
    "    {\"role\":\"system\",\"content\":\"You are an expert Python tutor.\"},\n",
    "    {\"role\":\"user\",\"content\":user_prompt_medium}\n",
    "]\n",
    "\n",
    "messages_best = [\n",
    "    {\"role\":\"system\",\"content\":\"You are an expert Python tutor.\"},\n",
    "    {\"role\":\"user\",\"content\":user_prompt_best}\n",
    "\n",
    "]\n",
    "\n",
    "# 3Ô∏è‚É£ Call Groq for each prompt version\n",
    "\n",
    "resp_vague = client.chat.completions.create(\n",
    "    model=\"llama-3.1-8b-instant\",\n",
    "    messages=messages_vague\n",
    ")\n",
    "out_vague = resp_vague.choices[0].message.content\n",
    "\n",
    "resp_medium = client.chat.completions.create(\n",
    "    model=\"llama-3.1-8b-instant\",\n",
    "    messages=messages_medium\n",
    ")\n",
    "out_medium = resp_medium.choices[0].message.content\n",
    "\n",
    "resp_best = client.chat.completions.create(\n",
    "    model=\"llama-3.1-8b-instant\",\n",
    "    messages=messages_best\n",
    ")\n",
    "\n",
    "out_best = resp_best.choices[0].message.content\n",
    "\n",
    "# 4Ô∏è‚É£ Print results for comparison\n",
    "print(\"===== ‚ùå Version A ‚Äî Vague Prompt =====\")\n",
    "print(out_vague)\n",
    "print(\"\\n\")\n",
    "\n",
    "print(\"==================================================================== ‚ö†Ô∏è Version B ‚Äî Better Prompt =====\")\n",
    "print(out_medium)\n",
    "print(\"\\n\")\n",
    "\n",
    "print(\"===================================================================== ‚úÖ Version C ‚Äî Best Prompt =====\")\n",
    "print(out_best)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35940bfa",
   "metadata": {},
   "source": [
    "#### **Temperature, Top_p, Max Tokens (LLM Behavior Controls)**\n",
    "\n",
    "Every LLM engineer at Google, Microsoft, OpenAI must master these 3 parameters because they control:\n",
    "\n",
    "- Creativity\n",
    "- Determinism\n",
    "- Output length\n",
    "- Safety\n",
    "- Reliability\n",
    "- Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "660caa0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================üßä Temperature 0.0 ‚Äî Deterministic =====\n",
      "As the stars whizzed by like diamonds on velvet, Zeta-5, a fearless robot with a heart of circuitry, boldly ventured into the unknown expanse of the Andromeda galaxy.\n",
      "\n",
      "\n",
      "==================================üî• Temperature 1.5 ‚Äî Creative & Random =====\n",
      "As the stars streaked by like diamonds in the vast expanse of deep space, robotic astronaut Aurora Ventoris boldly forged ahead with an insatiable thirst to unravel the secrets of the cosmos.\n",
      "\n",
      "\n",
      "===========================================üéØ top_p = 0.3 ‚Äî Restrictive Creativity =====\n",
      "As the stars whizzed by like diamonds on velvet, robot explorer Zeta-5 pierced the unknown expanse of the cosmos, its gleaming metal heart beating with an insatiable thirst for discovery.\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# üìò SECTION 9.3 ‚Äî LLM Behavior Controls:\n",
    "#     temperature, top_p, max_tokens\n",
    "# ------------------------------------------------------------\n",
    "# Why this matters?\n",
    "#   These 3 parameters allow us to control HOW the model behaves.\n",
    "#\n",
    "#   temperature ‚Üí creativity vs stability\n",
    "#   top_p       ‚Üí nucleus sampling (controls randomness range)\n",
    "#   max_tokens  ‚Üí how much the model is allowed to speak\n",
    "#\n",
    "# Real-world impact:\n",
    "#   - chatbots (stable responses)\n",
    "#   - code generation (deterministic answers)\n",
    "#   - story writing (high creativity)\n",
    "#   - RAG systems (must stay factual)\n",
    "#   - SQL bots (must be deterministic, low temperature)\n",
    "# ============================================================\n",
    "\n",
    "# 1Ô∏è‚É£ Let's prepare one simple prompt\n",
    "\n",
    "messages = [\n",
    "    {\"role\":\"system\",\"content\":\"You are a creative storyteller.\"},\n",
    "    {\"role\":\"user\",\"content\":\"Write one line about a brave robot exploring space.\"}\n",
    "]\n",
    "\n",
    "# 2Ô∏è‚É£ Low Temperature (0.0) ‚Üí deterministic / predictable\n",
    "response_low_temp = client.chat.completions.create(\n",
    "    model=\"llama-3.1-8b-instant\",\n",
    "    temperature=0.0,\n",
    "    top_p=1.0,\n",
    "    max_tokens=50,\n",
    "    messages=messages\n",
    ")\n",
    "out_low_temp = response_low_temp.choices[0].message.content\n",
    "\n",
    "# 3Ô∏è‚É£ High Temperature (1.5) ‚Üí creative / random / surprising\n",
    "response_high_temp = client.chat.completions.create(\n",
    "    model=\"llama-3.1-8b-instant\",\n",
    "    temperature=1.5,\n",
    "    top_p=1.0,\n",
    "    max_tokens=50,\n",
    "    messages=messages\n",
    ")\n",
    "out_high_temp = response_high_temp.choices[0].message.content\n",
    "\n",
    "# 4Ô∏è‚É£ Top_p control (restrict randomness window)\n",
    "response_top_p_low = client.chat.completions.create(\n",
    "    model=\"llama-3.1-8b-instant\",\n",
    "    temperature=0.8,\n",
    "    top_p=0.3,\n",
    "    max_tokens=50,\n",
    "   messages=messages\n",
    ")\n",
    "out_top_p_low = response_top_p_low.choices[0].message.content\n",
    "\n",
    "# 5Ô∏è‚É£ Print results side by side\n",
    "print(\"====================üßä Temperature 0.0 ‚Äî Deterministic =====\")\n",
    "print(out_low_temp)\n",
    "print('\\n')\n",
    "\n",
    "print(\"==================================üî• Temperature 1.5 ‚Äî Creative & Random =====\")\n",
    "print(out_high_temp)\n",
    "print(\"\\n\")\n",
    "\n",
    "print(\"===========================================üéØ top_p = 0.3 ‚Äî Restrictive Creativity =====\")\n",
    "print(out_top_p_low)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d702ce99",
   "metadata": {},
   "source": [
    "#### **Stop Sequences (Prevent Unwanted Output)**\n",
    "\n",
    "Stop sequences are EXTREMELY important in:\n",
    "\n",
    "- Chatbots : \n",
    "Stop sequences are critical in chatbot systems because they define where the chatbot's output should stop. Without a stop sequence, the chatbot could continue generating text indefinitely or produce responses that aren't clean or useful. For example, the bot might keep generating irrelevant or redundant responses.\n",
    "\n",
    "- Agents : \n",
    "Similar to chatbots, agents (which might be virtual assistants or automated systems) need stop sequences to prevent runaway or endless output. It ensures that the agent stops once the relevant task or response is completed.\n",
    "\n",
    "- Function calling : \n",
    "In function-based programming or API calls, stop sequences can define where the output or result of a function should be terminated. This helps ensure that the function doesn‚Äôt accidentally return too much or too little data.\n",
    "\n",
    "- Tools : \n",
    "If an AI system interacts with tools (like executing code, querying databases, etc.), stop sequences can be used to limit the response or actions to only what's needed. This can also apply to systems like text editors, where you want to limit the response size or structure.\n",
    "\n",
    "- RAG systems : \n",
    "RAG systems pull in external information to generate responses (often combining a search engine and a generative model). In these systems, a stop sequence is used to cut off the generated text at a logical point, ensuring that the AI doesn't just ramble or include irrelevant information from its knowledge base.\n",
    "\n",
    "- Structured JSON output : \n",
    "When dealing with structured data formats like JSON, stop sequences help ensure the output is clean and properly formatted. Without a stop sequence, the generated JSON could become malformed or continue indefinitely.\n",
    "\n",
    "- Limiting hallucinations : \n",
    "Hallucinations in AI refer to when the model generates incorrect or nonsensical information. Stop sequences can be used as a tool to limit this behavior by halting the output once a coherent answer is generated, preventing the AI from continuing and possibly inventing information\n",
    "\n",
    "- Preventing ‚Äúextra text‚Äù : \n",
    "Sometimes models generate extra text, filler, or tangents that don‚Äôt serve the purpose. Stop sequences are used to halt the model once the response is complete, cutting off unnecessary or irrelevant additions\n",
    "\n",
    "- Controlling formatting : \n",
    "In cases where a specific format is required (e.g., code snippets, structured responses), stop sequences can help ensure the model stops at the correct point to match the desired formatting and avoid messy output\n",
    "\n",
    "- API integration : \n",
    "In API-based systems, stop sequences can be used to control how much data is returned, how the data is formatted, and how the system behaves when interacting with APIs. For instance, you can use stop sequences to ensure the API responses are concise and properly structured, improving performance and readability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "d3e3ef2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===== ‚ùå Without Stop Sequence =====\n",
      "{\"name\": \"Dhiru\", \"age\": 36}\n",
      "\n",
      "\n",
      "===== ‚úÖ With Stop Sequence =====\n",
      "{\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# üìò SECTION 9.4 ‚Äî Stop Sequences (Prevent Unwanted Output)\n",
    "# ------------------------------------------------------------\n",
    "# Why this matters?\n",
    "#   - LLMs sometimes continue speaking beyond what we want.\n",
    "#   - Stop sequences tell the model:\n",
    "#         \"STOP generating when you see this pattern.\"\n",
    "#\n",
    "# Real-world uses:\n",
    "#   - Prevent extra sentences after JSON output\n",
    "#   - Stop the model before adding explanations\n",
    "#   - Control agent/tool responses\n",
    "#   - Enforce strict formatting\n",
    "#   - Avoid hallucinated closing remarks\n",
    "# ============================================================\n",
    "\n",
    "# 1Ô∏è‚É£ Build a prompt where model tends to continue speaking\n",
    "messages = [\n",
    "    {\"role\": \"system\",\n",
    "    \"content\": \"Your Output ONLY the JSON asked for. NOTHING else.\"},\n",
    "    {\"role\": \"user\",\n",
    "    \"content\": \"Give me a JSON with name= 'Dhiru' and age=36\"}\n",
    "]\n",
    "\n",
    "# ‚ùå Without stop sequences ‚Üí model may add:\n",
    "#    - \"Here is the JSON:\"\n",
    "#    - backticks\n",
    "#    - explanations\n",
    "#    - extra comments\n",
    "\n",
    "response_no_stop = client.chat.completions.create(\n",
    "    model=\"llama-3.1-8b-instant\",\n",
    "    messages=messages,\n",
    "    max_tokens=50\n",
    ")\n",
    "\n",
    "out_no_stop = response_no_stop.choices[0].message.content\n",
    "\n",
    "# 2Ô∏è‚É£ Now apply STOP SEQUENCES\n",
    "#    Tell model:\n",
    "#       - stop when you see a newline\n",
    "#       - stop when you see trailing text like \"</end>\"\n",
    "#\n",
    "# Common patterns used in industry:\n",
    "#       stop=[\"```\", \"\\n\\n\", \"</end>\"]\n",
    "\n",
    "response_with_stop = client.chat.completions.create(\n",
    "    model=\"llama-3.1-8b-instant\",\n",
    "    messages=messages,\n",
    "    max_tokens = 50,\n",
    "    stop=[\"\\n\"]  # stop generation at first newline\n",
    ")\n",
    "\n",
    "out_with_stop = response_with_stop.choices[0].message.content\n",
    "\n",
    "# 3Ô∏è‚É£ Print results\n",
    "print(\"===== ‚ùå Without Stop Sequence =====\")\n",
    "print(out_no_stop)\n",
    "print(\"\\n\")\n",
    "\n",
    "print(\"===== ‚úÖ With Stop Sequence =====\")\n",
    "print(out_with_stop)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41b93834",
   "metadata": {},
   "source": [
    "#### **Structured Output & JSON Mode (Production-Grade Output Control)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c90edc6a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "1bf87b49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===== üü° Without Strict JSON Mode =====\n",
      "```json\n",
      "{\n",
      "    \"name\": \"Dhiru\",\n",
      "    \"experience\": \"GenAI Learner\",\n",
      "    \"level\": \"Beginner to pro Journey\"\n",
      "}\n",
      "```\n",
      "\n",
      "Here's an explanation of each field:\n",
      "\n",
      "- **name**: 'Dhiru' represents the name of the GenAI learner, which is the person being described.\n",
      "- **experience**: 'GenAI Learner' describes Dhiru's current level of expertise and training in General AI, indicating they are still learning.\n",
      "- **level**: 'Beginner to pro Journey' highlights Dhiru's current position on the learning path, indicating they have started from the basics and are aspiring to advanced expertise in GenAI.\n",
      "\n",
      "\n",
      "\n",
      "===== üü¢ With Strict JSON Template =====\n",
      "{\n",
      "  \"name\": \"Dhiru\",\n",
      "  \"experience\": \"GenAI Learner\",\n",
      "  \"level\": \"Beginner to Pro Journey\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# üìò SECTION 9.5 ‚Äî Structured Output & JSON Mode\n",
    "# ------------------------------------------------------------\n",
    "# Why this matters?\n",
    "#   - LLMs love adding explanations, backticks, and commentary.\n",
    "#   - But production systems require STRICT, machine-readable JSON.\n",
    "#   - APIs, agents, RAG engines, and data pipelines break if output\n",
    "#     is not exactly structured.\n",
    "#\n",
    "# Goal:\n",
    "#   - Compare loose JSON vs strict JSON template + stop sequences.\n",
    "# ============================================================\n",
    "\n",
    "# 1Ô∏è‚É£ Prompt for JSON output (model may add extra text)\n",
    "\n",
    "messages_loose = [\n",
    "{    \"role\":\"system\",\n",
    "    \"content\":\"You are an assistant. Respond to the user request.\"},\n",
    "{    \"role\":\"user\",\n",
    "    \"content\":(\n",
    "        \"Return a json object with fields:\"\n",
    "        \"name= 'Dhiru',experience='GenAI Learner', level='Beginner to pro Journey'.\"\n",
    "        \"After the json, explain each field in one sentence.\")}    \n",
    "]\n",
    "\n",
    "# üü° 2Ô∏è‚É£ Call WITHOUT strict control ‚Äî model may add explanations\n",
    "response_loose = client.chat.completions.create(\n",
    "    model=\"llama-3.1-8b-instant\",\n",
    "    messages=messages_loose,\n",
    "    max_tokens=200\n",
    ")\n",
    "\n",
    "out_loose = response_loose.choices[0].message.content\n",
    "\n",
    "# üü¢ 3Ô∏è‚É£ STRICT JSON ‚Äî provide exact template + stop sequence\n",
    "\n",
    "message_strict = [\n",
    "{    \"role\":\"system\",\n",
    "    \"content\":(\n",
    "        \"You MUST output ONLY valid JSON. No explanation, no commentary,\"\n",
    "        \"no extra text. Follow EXACT format:\\n\\n\"\n",
    "        \"{\\n\"\n",
    "         \"  \\\"name\\\": \\\"...\\\",\\n\"\n",
    "         \"  \\\"experience\\\": \\\"...\\\",\\n\"\n",
    "         \"  \\\"level\\\": \\\"...\\\"\\n\"\n",
    "         \"}\\n\\n\"\n",
    "         \"Do not add anything else beyond this JSON structure.\")},\n",
    "    \n",
    "    {\"role\":\"user\",\n",
    "    \"content\":\"Fill the JSON fields for name='Dhiru', experience='GenAI Learner', level='Beginner to Pro Journey'.\"}\n",
    "]\n",
    "\n",
    "response_strict = client.chat.completions.create(\n",
    "    model=\"llama-3.1-8b-instant\",\n",
    "    messages=message_strict,\n",
    "    max_tokens=200,\n",
    "    stop=[\"\\n\\n\"] # stop before any unwanted explanation begins\n",
    ")\n",
    "\n",
    "out_strict = response_strict.choices[0].message.content\n",
    "\n",
    "\n",
    "# üß™ 4Ô∏è‚É£ Print results\n",
    "print(\"===== üü° Without Strict JSON Mode =====\")\n",
    "print(out_loose)\n",
    "print(\"\\n\\n\")\n",
    "\n",
    "print(\"===== üü¢ With Strict JSON Template =====\")\n",
    "print(out_strict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4ce5dac",
   "metadata": {},
   "source": [
    "#### **Prompt Chaining & Step-by-Step Reasoning**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "c56f2ba1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===== ‚ùå Direct Prompt (No Structured Reasoning) =====\n",
      "**What is Recursion?**\n",
      "--------------------\n",
      "\n",
      "Recursion is a programming concept where a function calls itself as a subroutine. The function will keep calling itself until it reaches a base case that stops the recursion. This allows the function to solve a problem by breaking it down into smaller instances of the same problem.\n",
      "\n",
      "**Example: Factorial Function**\n",
      "-----------------------------\n",
      "\n",
      "Here's an example of the factorial function, which is commonly used to demonstrate recursion:\n",
      "\n",
      "```python\n",
      "def factorial(n):\n",
      "    \"\"\"\n",
      "    Calculate the factorial of a number.\n",
      "\n",
      "    Args:\n",
      "        n (int): The number to calculate the factorial of.\n",
      "\n",
      "    Returns:\n",
      "        int: The factorial of n.\n",
      "    \"\"\"\n",
      "    # Base case: if n is 0 or 1, return 1\n",
      "    if n == 0 or n == 1:\n",
      "        return 1\n",
      "    # Recursive case: n! = n * (n-1)!\n",
      "    else:\n",
      "        return n * factorial(n-1)\n",
      "```\n",
      "\n",
      "Here's how this function works:\n",
      "\n",
      "1. If `n` is 0 or 1, we return 1 because the factorial of 0 and 1 is 1.\n",
      "2. If `n` is greater than 1, we call `factorial(n-1)` and multiply the result by `n`. This is the recursive call.\n",
      "\n",
      "Let's see an example of how this function is called:\n",
      "\n",
      "```python\n",
      "print(factorial(4))  # Output: 24\n",
      "```\n",
      "\n",
      "The calls to `factorial` happen like this:\n",
      "\n",
      "```\n",
      "factorial(4)\n",
      "  => factorial(3) * 4\n",
      "    => factorial(2) * 3\n",
      "      => factorial(1) * 2\n",
      "        => factorial(0) * 1\n",
      "          => 1 * 1\n",
      "         => 1 * 2\n",
      "        => 2\n",
      "      => 2 * 1\n",
      "    => 2 * 3\n",
      "  => 2 * 3 * 4\n",
      "= 24\n",
      "```\n",
      "\n",
      "As you can see, the function calls itself repeatedly until it reaches the base case (`n == 0 or n == 1`).\n",
      "\n",
      "**Benefits of Recursion**\n",
      "-------------------------\n",
      "\n",
      "Recursion has several benefits, including:\n",
      "\n",
      "*   **Readability**: Recursive functions can often be more readable than iterative solutions because they follow the structure of the problem more closely.\n",
      "*   **Elegance**: Recursion can lead to more elegant solutions than iterative solutions.\n",
      "*   **Debugging**: Recursive functions can be easier to debug than iterative solution because the recursive calls are explicit.\n",
      "\n",
      "However, **Recursion also has its costs**:\n",
      "\n",
      "*   **Performance**: Recursive functions can be slower than iterative solutions because they require multiple function calls.\n",
      "*   **Memory**: Recursive functions can use more memory than iterative solutions because each recursive call creates a new stack frame.\n",
      "\n",
      "\n",
      "\n",
      "=========================== ‚úÖ Prompt Chaining (Structured Reasoning) =====\n",
      "**STEP 1: Understand the question.**\n",
      "Recursion is a programming technique where a function calls itself repeatedly with decreasing values of the problem size until a base case is met, at which point the function stops and the values are returned.\n",
      "\n",
      "**STEP 2: Break the concept into simple parts.**\n",
      "\n",
      "- **Function**: The method that solves the problem by calling itself.\n",
      "- **Base Case**: The condition that stops the recursion and returns a value.\n",
      "- **Recursive Case**: The condition that defines what happens when the function calls itself.\n",
      "\n",
      "1. The function calls itself with a smaller or more specific version of the input.\n",
      "2. The function executes until the base case is met.\n",
      "3. When the base case is met, the function returns a value.\n",
      "\n",
      "**STEP 3: Provide a real example.**\n",
      "\n",
      "Example: Calculate the factorial of a number using recursion.\n",
      "\n",
      "```python\n",
      "def factorial(n):\n",
      "    # Base Case: 1! = 1 and 0! = 1\n",
      "    if n == 0 or n == 1:\n",
      "        return 1\n",
      "    \n",
      "    # Recursive Case: n! = n * (n-1)!\n",
      "    else:\n",
      "        return n * factorial(n-1)\n",
      "\n",
      "# Test the function\n",
      "n = 5\n",
      "print(f\"The factorial of {n} is: {factorial(n)}\")\n",
      "```\n",
      "\n",
      "This function works as follows:\n",
      "\n",
      "- When `n` is 0 or 1, the base case is met, and the function returns 1.\n",
      "- When `n` is greater than 1, the function calls itself with `n-1` and multiplies the result by `n`.\n",
      "\n",
      "**STEP 4: Highlight mistakes beginners make.**\n",
      "\n",
      "Some common mistakes beginners make when using recursion are:\n",
      "\n",
      "- Forgetting to include a base case.\n",
      "- Not understanding when to stop the recursion.\n",
      "- Not handling large input values (leading to stack overflow errors).\n",
      "\n",
      "To avoid these mistakes:\n",
      "\n",
      "- Ensure each recursive call decreases the problem size.\n",
      "- Include a clear and efficient base case.\n",
      "- Test the function with edge cases (like large or small input values).\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# üìò SECTION 9.6 ‚Äî Prompt Chaining & Step-by-Step Reasoning\n",
    "# ------------------------------------------------------------\n",
    "# Why this matters?\n",
    "#   - Large questions overwhelm LLMs.\n",
    "#   - Breaking a problem into smaller steps improves:\n",
    "#       * accuracy\n",
    "#       * reasoning\n",
    "#       * reliability\n",
    "#       * factual correctness\n",
    "#\n",
    "# This is EXACTLY how Google/Microsoft build reasoning agents.\n",
    "#\n",
    "# We will demonstrate:\n",
    "#   1) Direct prompting (bad accuracy)\n",
    "#   2) Step-by-step chain (much better)\n",
    "# ============================================================\n",
    "\n",
    "messages_direct = [\n",
    "    {\"role\":\"system\",\"content\":\"You are Python expert.\"},\n",
    "    {\"role\":\"user\",\"content\":\"Explain how recursion works with an example.\"}\n",
    "]\n",
    "\n",
    "response_direct = client.chat.completions.create(\n",
    "    model=\"llama-3.1-8b-instant\",\n",
    "    messages=messages_direct\n",
    ")\n",
    "\n",
    "out_direct =response_direct.choices[0].message.content\n",
    "\n",
    "# üß© 2Ô∏è‚É£ Chained reasoning ‚Äî Force model to think step-by-step\n",
    "\n",
    "messages_chain = [\n",
    "    {\"role\": \"system\",\n",
    "     \"content\": (\n",
    "         \"You are a Python expert. Always think in steps.\\n\"\n",
    "         \"Follow this pattern:\\n\"\n",
    "         \"STEP 1: Understand the question.\\n\"\n",
    "         \"STEP 2: Break the concept into simple parts.\\n\"\n",
    "         \"STEP 3: Provide a real example.\\n\"\n",
    "         \"STEP 4: Highlight mistakes beginners make.\\n\"\n",
    "     )},\n",
    "    {\"role\": \"user\",\n",
    "     \"content\": \"Explain how recursion works with an example.\"}\n",
    "]\n",
    "\n",
    "response_chain = client.chat.completions.create(\n",
    "    model=\"llama-3.1-8b-instant\",\n",
    "    messages=messages_chain\n",
    ")\n",
    "\n",
    "out_chain = response_chain.choices[0].message.content\n",
    "\n",
    "\n",
    "\n",
    "# üß™ 3Ô∏è‚É£ Print both outputs\n",
    "print(\"===== ‚ùå Direct Prompt (No Structured Reasoning) =====\")\n",
    "print(out_direct)\n",
    "print(\"\\n\\n\")\n",
    "\n",
    "print(\"=========================== ‚úÖ Prompt Chaining (Structured Reasoning) =====\")\n",
    "print(out_chain)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "568d4f34",
   "metadata": {},
   "source": [
    "#### **Few-Shot Prompting (Teaching the Model with Examples)**\n",
    "\n",
    "**Few-shot prompting is used in all advanced AI systems:**\n",
    "\n",
    "- SQL generators\n",
    "- Code assistants\n",
    "- Agents\n",
    "- Classification models\n",
    "- Extraction tasks\n",
    "- Multi-turn chatbots\n",
    "- RAG reasoning\n",
    "- Enterprise AI platforms\n",
    "- Prompt tuning models\n",
    "\n",
    "**When you give the model examples, it:**\n",
    "\n",
    "- understands patterns\n",
    "- copies structure\n",
    "- increases accuracy\n",
    "- reduces hallucination\n",
    "- becomes consistent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "1f920c2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===== üéØ FEW-SHOT OUTPUT =====\n",
      "{\"name\": \"Dhiru\",\"age\":36}\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# üìò SECTION 9.7 ‚Äî Few-Shot Prompting (Teach the Model by Example)\n",
    "# ------------------------------------------------------------\n",
    "# Why this matters?\n",
    "#   - LLMs learn patterns extremely well.\n",
    "#   - By giving 1‚Äì2 examples (\"shots\"), we teach the model the\n",
    "#     EXACT format, tone, and structure we want.\n",
    "#\n",
    "# Real-world usage:\n",
    "#   - SQL generation\n",
    "#   - Classification\n",
    "#   - Entity extraction\n",
    "#   - Email drafting\n",
    "#   - Code generation\n",
    "#   - Customer support bots\n",
    "#   - RAG summarization format\n",
    "# ============================================================\n",
    "\n",
    "\n",
    "# üß© 1Ô∏è‚É£ FEW-SHOT EXAMPLES (these teach the pattern)\n",
    "\n",
    "few_shot_examples = [\n",
    "    # Example 1\n",
    "    {\n",
    "        \"role\":\"user\",\n",
    "        \"content\":\"Convert to structured data: The user's name is Arjun and he is 29 years old.\"\n",
    "    },\n",
    "    {\n",
    "        \"role\":\"assistant\",\n",
    "        \"content\":\"{\\\"name\\\":\\\"Arjun\\\",\\\"age\\\":29}\"\n",
    "    },\n",
    "    # Example 2\n",
    "    {\n",
    "        \"role\":\"user\",\n",
    "        \"content\":\"Convert to structured data: The user's name is Meera and she is 24 years old.\"\n",
    "    },\n",
    "    \n",
    "    {\n",
    "        \"role\":\"assistant\",\n",
    "        \"content\":\"{\\\"name\\\": \\\"Meera\\\",\\\"age\\\":24}\"\n",
    "\n",
    "    }\n",
    "]\n",
    "\n",
    "\n",
    "# üß© 2Ô∏è‚É£ NOW THE REAL TASK (model will follow examples above)\n",
    "\n",
    "actual_task = [\n",
    "    {\n",
    "        \"role\":\"user\",\n",
    "        \"content\":\"Convert to structured data: The user's name is Dhiru and he is 36 years old.\"\n",
    "    }\n",
    "]\n",
    "\n",
    "# Combined example + Task\n",
    "messages_few_shot = few_shot_examples + actual_task\n",
    "\n",
    "# üß† 3Ô∏è‚É£ MODEL CALL\n",
    "response_few_shot = client.chat.completions.create(\n",
    "    model=\"llama-3.1-8b-instant\",\n",
    "    messages=messages_few_shot,\n",
    "    max_tokens=50\n",
    ")\n",
    "\n",
    "output_few_shot = response_few_shot.choices[0].message.content\n",
    "\n",
    "\n",
    "# üß™ 4Ô∏è‚É£ PRINT RESULT\n",
    "print(\"===== üéØ FEW-SHOT OUTPUT =====\")\n",
    "print(output_few_shot)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f1f2bd5",
   "metadata": {},
   "source": [
    "#### **üß† Zero-Shot vs One-Shot vs Few-Shot Prompting**\n",
    "\n",
    "In GenAI, the ‚Äúshots‚Äù refer to how many **examples** we show the model before asking it to perform a task.\n",
    "\n",
    "This directly controls:\n",
    "- how accurate the model is\n",
    "- how consistent the output becomes\n",
    "- how much hallucination is reduced\n",
    "- how predictable the format is\n",
    "- how ‚Äúsmart‚Äù the model appears\n",
    "\n",
    "Understanding these 3 is essential for:\n",
    "\n",
    "‚úî RAG  \n",
    "‚úî Agents  \n",
    "‚úî Code generation bots  \n",
    "‚úî SQL assistants  \n",
    "‚úî Email writers  \n",
    "‚úî Summarizers  \n",
    "‚úî Data extractors  \n",
    "‚úî Enterprise AI tools  \n",
    "‚úî Interviews at Big Tech  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "38c960ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===== üü¶ ZERO-SHOT OUTPUT =====\n",
      "To extract the information you've requested:\n",
      "\n",
      "- Name: Neha\n",
      "- Age: 22 \n",
      "\n",
      "===== üüß ONE-SHOT OUTPUT =====\n",
      "{\"name\": \"Neha\", \"age\": 22} \n",
      "\n",
      "===== üü© FEW-SHOT OUTPUT =====\n",
      "{\"name\": \"Neha\", \"age\": 22}\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# üìò SECTION 9.8 ‚Äî Zero-shot vs One-shot vs Few-shot Prompts\n",
    "# ------------------------------------------------------------\n",
    "# Why this matters?\n",
    "#   - Different tasks require different prompting approaches.\n",
    "#   - For structured output, few-shot is best.\n",
    "#   - For simple classification, zero-shot works well.\n",
    "#   - For formatting consistency, one-shot/few-shot is superior.\n",
    "# ============================================================\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 1Ô∏è‚É£ ZERO-SHOT PROMPTING (No examples)\n",
    "# ------------------------------------------------------------\n",
    "# Use case:\n",
    "#   - When task is simple or the model already understands it.\n",
    "#   - Fast, cheap, and works surprisingly well for knowledge queries.\n",
    "\n",
    "zero_shot = client.chat.completions.create(\n",
    "    model=\"llama-3.1-8b-instant\",\n",
    "    messages=[{\n",
    "        \"role\":\"user\",\n",
    "        \"content\":\"Extract name and age: The user's name is Neha and she is 22 years old.\"\n",
    "    }],\n",
    "    max_tokens=50\n",
    ")\n",
    "\n",
    "zero_out = zero_shot.choices[0].message.content\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 2Ô∏è‚É£ ONE-SHOT PROMPTING (Exactly one example)\n",
    "# ------------------------------------------------------------\n",
    "# Use case:\n",
    "#   - When you need consistent formatting.\n",
    "#   - The model follows the pattern of the single example.\n",
    "\n",
    "one_shot_messages = [\n",
    "    # ONE example\n",
    "    {\"role\":\"user\",\"content\":\"Extract: The user's name is Arjun and he is 29.\"},\n",
    "    {\"role\":\"assistant\",\"content\":\"{\\\"name\\\": \\\"Arjun\\\", \\\"age\\\": 29}\"},\n",
    "    \n",
    "    # NOW the real task\n",
    "    {\"role\":\"user\",\n",
    "    \"content\":\"Extract: The user's name is Neha and she is 22 years old.\"}\n",
    "]\n",
    "\n",
    "one_shot = client.chat.completions.create(\n",
    "    model=\"llama-3.1-8b-instant\",\n",
    "    messages=one_shot_messages,\n",
    "    max_tokens=50\n",
    ")\n",
    "\n",
    "one_out = one_shot.choices[0].message.content\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 3Ô∏è‚É£ FEW-SHOT PROMPTING (Multiple examples)\n",
    "# ------------------------------------------------------------\n",
    "# Use case:\n",
    "#   - Best for structured output.\n",
    "#   - Reduces hallucination.\n",
    "#   - Ensures exact format required in production.\n",
    "\n",
    "few_shot_messages = [\n",
    "{    \"role\":\"user\",\n",
    "    \"content\":\"Extract: The user's name is Rohan and he is 31.\"},\n",
    "{    'role':\"assistant\",\n",
    "    \"content\":\"{\\\"name\\\": \\\"Rohan\\\", \\\"age\\\": 31}\"},\n",
    "\n",
    "# REAL TASK\n",
    "\n",
    "{    \"role\":\"user\",\n",
    "    \"content\":\"ExtractExtract: The user's name is Neha and she is 22 years old.\"}\n",
    "]\n",
    "\n",
    "few_shot = client.chat.completions.create(\n",
    "    model=\"llama-3.1-8b-instant\",\n",
    "    messages=few_shot_messages,\n",
    "    max_tokens=50\n",
    ")\n",
    "\n",
    "few_out = few_shot.choices[0].message.content\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 4Ô∏è‚É£ Display Results\n",
    "# ------------------------------------------------------------\n",
    "print(\"===== üü¶ ZERO-SHOT OUTPUT =====\")\n",
    "print(zero_out, \"\\n\")\n",
    "\n",
    "print(\"===== üüß ONE-SHOT OUTPUT =====\")\n",
    "print(one_out, \"\\n\")\n",
    "\n",
    "print(\"===== üü© FEW-SHOT OUTPUT =====\")\n",
    "print(few_out)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fb090f4",
   "metadata": {},
   "source": [
    "# üß† Zero-Shot, One-Shot, Few-Shot Prompting  \n",
    "### (Scenarios ‚Ä¢ When to Use ‚Ä¢ Roles ‚Ä¢ Final Summary)\n",
    "\n",
    "---\n",
    "\n",
    "## üéØ 1. When Should We Use Zero-Shot, One-Shot, and Few-Shot Prompting?\n",
    "\n",
    "These techniques define **how much guidance** we provide an LLM before asking it to perform a task.\n",
    "\n",
    "---\n",
    "\n",
    "# üîµ Zero-Shot Prompting ‚Äî *‚ÄúModel, figure it out yourself.‚Äù*\n",
    "\n",
    "### ‚úÖ When to Use:\n",
    "- The task is **simple**\n",
    "- You don‚Äôt need strict formatting\n",
    "- Want **quick, cheap** inference\n",
    "- The model already understands the concept\n",
    "\n",
    "### üß† Examples:\n",
    "- ‚ÄúExplain recursion.‚Äù\n",
    "- ‚ÄúSummarize this.‚Äù\n",
    "- ‚ÄúTranslate this sentence.‚Äù\n",
    "- ‚ÄúWhat is the capital of Japan?‚Äù\n",
    "\n",
    "### üìå Real-World Usage:\n",
    "- Chatbots  \n",
    "- Knowledge Q&A  \n",
    "- Simple utilities  \n",
    "- Brainstorming  \n",
    "\n",
    "---\n",
    "\n",
    "# üü† One-Shot Prompting ‚Äî *‚ÄúHere is ONE example. Follow this pattern.‚Äù*\n",
    "\n",
    "### ‚úÖ When to Use:\n",
    "- You want the model to follow a **specific style**\n",
    "- Output format is **somewhat important**\n",
    "- You want more consistency than zero-shot\n",
    "- You want to teach tone or structure\n",
    "\n",
    "### üß† Examples:\n",
    "- Customer support reply templates  \n",
    "- Email formats  \n",
    "- JSON structure guidance  \n",
    "- Product description style  \n",
    "\n",
    "### üìå Real-World Usage:\n",
    "- Customer support bots  \n",
    "- Code formatting tasks  \n",
    "- Email writing assistants  \n",
    "\n",
    "---\n",
    "\n",
    "# üü¢ Few-Shot Prompting ‚Äî *‚ÄúHere are MULTIPLE examples. Learn this EXACT pattern.‚Äù*\n",
    "\n",
    "### ‚úÖ When to Use:\n",
    "- You need **consistent and accurate** output  \n",
    "- Structured output (JSON, SQL, XML)  \n",
    "- You must reduce hallucinations  \n",
    "- Model must match your format EXACTLY  \n",
    "- Production-level reliability is required\n",
    "\n",
    "### üß† Examples:\n",
    "- Data extraction (NER ‚Üí JSON)  \n",
    "- SQL generation  \n",
    "- Classification tasks  \n",
    "- Strict document summaries  \n",
    "- Multi-step reasoning  \n",
    "\n",
    "#### üìå Real-World Usage:\n",
    "- ChatGPT internal templates  \n",
    "- Enterprise information extraction  \n",
    "- SQL/text-to-structured pipelines  \n",
    "- RAG post-processing  \n",
    "- Financial report extraction  \n",
    "\n",
    "---\n",
    "\n",
    "### üß© 2. Final Summary Table (A+B)\n",
    "\n",
    "| Prompting Style | Best Time to Use | Strength |\n",
    "|------------------|------------------|----------|\n",
    "| **Zero-Shot** | Simple tasks | Fast, flexible |\n",
    "| **One-Shot** | Semi-structured tasks | Follows 1 example |\n",
    "| **Few-Shot** | Production systems | Accurate, consistent, low hallucination |\n",
    "\n",
    "---\n",
    "\n",
    "#### üß† 3. Role Explanation: System vs User vs Assistant (C)\n",
    "\n",
    "LLM messages contain roles that control behavior and context.\n",
    "\n",
    "---\n",
    "\n",
    "#### üü£ System Role ‚Äî *‚ÄúThe rulebook + personality.‚Äù*\n",
    "\n",
    "#### Purpose:\n",
    "- Sets rules  \n",
    "- Defines behavior  \n",
    "- Controls tone  \n",
    "- Harder for model to override  \n",
    "- Highest priority instruction  \n",
    "\n",
    "#### Example:\n",
    "```json\n",
    "{\"role\": \"system\", \"content\": \"You are a JSON-only extraction assistant.\"}\n",
    "\n",
    "**üîµ User Role ‚Äî ‚ÄúThe actual input or question**\n",
    "\n",
    "Purpose:\n",
    "\n",
    "- Represents the user's request\n",
    "- The model must respond to this\n",
    "\n",
    "Example: {\"role\": \"user\", \"content\": \"Extract name and age.\"}\n",
    "\n",
    "**üü¢ Assistant Role ‚Äî ‚ÄúModel‚Äôs previous replies.‚Äù**\n",
    "\n",
    "**Purpose:**\n",
    "- Shows examples (one-shot/few-shot)\n",
    "- Helps maintain continuity\n",
    "- Teaches formatting patterns\n",
    "\n",
    "Example: {\"role\": \"assistant\", \"content\": \"{\\\"name\\\": \\\"Arjun\\\", \\\"age\\\": 29}\"}\n",
    "\n",
    "**ü§î Why Didn't We Use System Role in This Exercise?**\n",
    "\n",
    "Because Step 9.8 focused on teaching through examples, not enforcing global rules.\n",
    "\n",
    "Few-shot examples already taught:\n",
    "\n",
    "- Structure\n",
    "- Format\n",
    "- Output pattern\n",
    "\n",
    "But in real production systems, you ALWAYS use the system role."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c382fbe1",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "caffef97",
   "metadata": {},
   "source": [
    "#### **Temperature, Top-p, Max Tokens, and Controlling Model Behavior**\n",
    "\n",
    "**üìå Before I give the next code cell, we follow our rule:**\n",
    "\n",
    "We will do ONE sub-step at a time.\n",
    "\n",
    "So Step 9.9 is large ‚Äî\n",
    "We will break it into sub-steps like:\n",
    "\n",
    "- 9.9A ‚Äî Understanding Temperature\n",
    "- 9.9B ‚Äî Understanding Top-p\n",
    "- 9.9C ‚Äî Max Tokens (output control)\n",
    "- 9.9D ‚Äî Frequency & Presence Penalties\n",
    "- 9.9E ‚Äî Comparing outputs with examples\n",
    "- 9.9F ‚Äî When to use which settings\n",
    "- 9.9G ‚Äî Final Summary (as per your new rule)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66518959",
   "metadata": {},
   "source": [
    "### **9.9A ‚Äî Understanding Temperature (Concept Only)**\n",
    "\n",
    "#### üî• Temperature ‚Äî Controls Creativity vs Factual Accuracy\n",
    "\n",
    "Temperature is a value between **0 and 2**.\n",
    "\n",
    "It decides how ‚Äúrandom‚Äù or ‚Äúcreative‚Äù the model will be.\n",
    "\n",
    "#### Low Temperature (0.0 ‚Äì 0.3)\n",
    "- Very deterministic  \n",
    "- Factual  \n",
    "- Reproducible  \n",
    "- Good for:\n",
    "  - SQL  \n",
    "  - Coding  \n",
    "  - Math  \n",
    "  - JSON extraction  \n",
    "  - RAG answers  \n",
    "\n",
    "#### Medium Temperature (0.4 ‚Äì 0.7)\n",
    "- Balanced  \n",
    "- Useful for:\n",
    "  - Explanations  \n",
    "  - Friendly chatbots  \n",
    "  - Educational tutors  \n",
    "\n",
    "#### High Temperature (0.8 ‚Äì 1.3)\n",
    "- Creative, unpredictable  \n",
    "- Good for:\n",
    "  - Stories  \n",
    "  - Brainstorming  \n",
    "  - Marketing  \n",
    "\n",
    "#### Very High (1.4 ‚Äì 2.0)\n",
    "- Chaotic  \n",
    "- Not recommended for production  \n",
    "\n",
    "#### Simple Analogy:\n",
    "Temperature = How \"imaginative\" the model becomes.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab24528c",
   "metadata": {},
   "source": [
    "#### **9.9B ‚Äî Understanding Top-p (Nucleus Sampling)**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7731777b",
   "metadata": {},
   "source": [
    "#### üü£ Step 9.9B ‚Äî Top-p (Nucleus Sampling)\n",
    "\n",
    "#### üéØ What is Top-p?\n",
    "\n",
    "Top-p controls **how many possible words** the model is allowed to choose from when generating the next token.\n",
    "\n",
    "Think of it like this:\n",
    "\n",
    "- Temperature = *How creative should the model be?*  \n",
    "- Top-p = *How wide should the model‚Äôs choice options be?*\n",
    "\n",
    "Both seem similar but work differently.\n",
    "\n",
    "---\n",
    "\n",
    "#### üß† How Top-p Works\n",
    "\n",
    "The model sorts all possible next tokens by probability and includes **only the smallest set of tokens whose probabilities sum to p**.\n",
    "\n",
    "Example:  \n",
    "If p = 0.9 ‚Üí include tokens until their total probability = 90%  \n",
    "If p = 0.5 ‚Üí include fewer possibilities (more restrictive)\n",
    "\n",
    "---\n",
    "\n",
    "#### üìå Typical Values and Their Meaning\n",
    "\n",
    "#### üîµ **Top-p = 1.0 (default)**\n",
    "- No restriction  \n",
    "- Model can pick from all possible words  \n",
    "- Most natural, balanced output  \n",
    "\n",
    "#### üü° **Top-p = 0.9**\n",
    "- Removes unlikely/rare words  \n",
    "- Makes writing cleaner, more stable  \n",
    "- Good for:\n",
    "  - Chatbots\n",
    "  - Explanations  \n",
    "  - RAG  \n",
    "\n",
    "#### üü† **Top-p = 0.5**\n",
    "- Very limited choice  \n",
    "- Makes output:\n",
    "  - Simple  \n",
    "  - Safe  \n",
    "  - Predictable  \n",
    "\n",
    "#### üî¥ **Top-p < 0.3**\n",
    "- Very restrictive  \n",
    "- Often too robotic  \n",
    "\n",
    "---\n",
    "\n",
    "#### üß™ Temperature vs Top-p ‚Äî Key Difference\n",
    "\n",
    "| Setting | Controls | Example |\n",
    "|--------|----------|---------|\n",
    "| **Temperature** | Randomness / Creativity | How wild or boring ideas are |\n",
    "| **Top-p** | Token selection range | How many options the model can choose from |\n",
    "\n",
    "#### ‚úî Temperature = intensity  \n",
    "#### ‚úî Top-p = choice range  \n",
    "\n",
    "---\n",
    "\n",
    "#### üéØ Best Practices (Real-World)\n",
    "\n",
    "| Task Type | Temperature | Top-p | Why |\n",
    "|-----------|-------------|--------|------|\n",
    "| SQL/Code | 0‚Äì0.2 | 0.9 | Accurate, deterministic |\n",
    "| RAG QA | 0.1‚Äì0.3 | 0.9 | Stable factual answers |\n",
    "| Formal writing | 0.2‚Äì0.5 | 0.9 | Polished output |\n",
    "| Creative writing | 0.7‚Äì1.1 | 1.0 | More ideas allowed |\n",
    "| Poetry/story | 0.9‚Äì1.3 | 1.0 | Maximum creativity |\n",
    "\n",
    "---\n",
    "\n",
    "#### üî• Simple Analogy  \n",
    "If Temperature = *How crazy the chef can be*,  \n",
    "then Top-p = *How many ingredients the chef is allowed to choose from.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "697377b7",
   "metadata": {},
   "source": [
    "#### **Step 9.9C ‚Äî Max Tokens (Output Length Control)**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a34a6cb3",
   "metadata": {},
   "source": [
    "#### üü© Step 9.9C ‚Äî Max Tokens (Output Length Control)\n",
    "\n",
    "#### üéØ What is max_tokens?\n",
    "\n",
    "`max_tokens` specifies **how many tokens the model is allowed to generate in the output**.\n",
    "\n",
    "Tokens ‚â† words.  \n",
    "A token is roughly:\n",
    "- 1 word (short word), or  \n",
    "- Part of a word (longer word)\n",
    "\n",
    "Example:\n",
    "- \"fantastic\" = 2 tokens  \n",
    "- \"I am fine\" = 4 tokens  \n",
    "\n",
    "---\n",
    "\n",
    "### üéØ Why is max_tokens important?\n",
    "\n",
    "Max tokens prevents:\n",
    "\n",
    "- runaway responses  \n",
    "- infinite loops  \n",
    "- extra text the model may add  \n",
    "- over-long answers  \n",
    "- too much verbosity  \n",
    "\n",
    "Especially in RAG, SQL, code generation, chatbots ‚Äî  \n",
    "**you MUST control output size**.\n",
    "\n",
    "---\n",
    "\n",
    "### üìå How It Works\n",
    "\n",
    "### Example:\n",
    "`max_tokens = 20`\n",
    "\n",
    "Model stops generating after ~20 tokens, even if:\n",
    "\n",
    "- The answer is incomplete  \n",
    "- The model had more to say  \n",
    "- The model was in the middle of a sentence  \n",
    "\n",
    "---\n",
    "\n",
    "#### üß© Common Mistake New Learners Make  \n",
    "They think `max_tokens` limits *input length* ‚Äî  \n",
    "but actually, it limits **output length** only.\n",
    "\n",
    "---\n",
    "\n",
    "#### üî• Real-World Usage\n",
    "\n",
    "| Use Case | max_tokens | Reason |\n",
    "|----------|------------|--------|\n",
    "| JSON extraction | 50 | Output small, predictable |\n",
    "| SQL generation | 100 | SQL not very long |\n",
    "| RAG QA | 150‚Äì300 | Moderate answers |\n",
    "| Email drafting | 200‚Äì400 | Longish content |\n",
    "| Essay/story | 500‚Äì800 | More space needed |\n",
    "| Code generation | 300‚Äì600 | Medium length required |\n",
    "\n",
    "---\n",
    "\n",
    "### üß† Why This Matters for Production Systems\n",
    "\n",
    "If you don't control max tokens:\n",
    "\n",
    "- Chatbot may write pages of text  \n",
    "- SQL generator may hallucinate full explanations  \n",
    "- JSON extractor may add unwanted commentary  \n",
    "- API cost increases  \n",
    "- Response time increases  \n",
    "- Users get confused  \n",
    "\n",
    "So **max_tokens = part of prompt control**.\n",
    "\n",
    "---\n",
    "\n",
    "#### üìå Recommended Defaults\n",
    "\n",
    "#### Facts / JSON / SQL:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a24791b",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "acab623d",
   "metadata": {},
   "source": [
    "#### **STEP 9.9D ‚Äî Frequency Penalty & Presence Penalty**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "736729b9",
   "metadata": {},
   "source": [
    "These two parameters control **how much the model should avoid repeating words or ideas.**\n",
    "\n",
    "They are extremely useful in:\n",
    "- Chatbots  \n",
    "- Story generation  \n",
    "- RAG summaries  \n",
    "- Email writing  \n",
    "- Answers where repetition looks bad  \n",
    "- Avoiding loops (very important for agents)  \n",
    "\n",
    "---\n",
    "\n",
    "**üü¶ 1. Frequency Penalty ‚Äî ‚ÄúDon‚Äôt repeat the same word too much.‚Äù**\n",
    "\n",
    "**üéØ What it does:**\n",
    "- If the model repeats a word many times, frequency_penalty pushes it to **reduce repetition**.\n",
    "\n",
    "**Example of unwanted repetition:**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7210dca",
   "metadata": {},
   "source": [
    "\n",
    "Presence penalty makes the model **explore more ideas**.\n",
    "\n",
    "---\n",
    "\n",
    "**üìå Summary Table ‚Äî Difference Between Both**\n",
    "\n",
    "| Penalty Type | Controls What | Helps With |\n",
    "|--------------|---------------|------------|\n",
    "| **Frequency Penalty** | Repeated words | Avoiding repetition, more natural sentences |\n",
    "| **Presence Penalty** | Repeated topics/ideas | Exploring new ideas, preventing narrow responses |\n",
    "\n",
    "---\n",
    "\n",
    "**‚≠ê Recommended Values (Industry Standard)**\n",
    "\n",
    "| Use Case | frequency_penalty | presence_penalty |\n",
    "|----------|--------------------|------------------|\n",
    "| JSON / SQL | 0 | 0 |\n",
    "| Chatbots | 0.2 | 0.2 |\n",
    "| Conversational agents | 0.3‚Äì0.7 | 0.3‚Äì0.7 |\n",
    "| Creative writing | 0.5‚Äì1.0 | 0.5‚Äì1.0 |\n",
    "| Story generation | 1.0+ | 1.0+ |\n",
    "\n",
    "---\n",
    "\n",
    "**üß† Real-World Examples**\n",
    "\n",
    "**Chatbots (avoid repeating user's sentence)**\n",
    "frequency_penalty = 0.3  \n",
    "presence_penalty = 0.2  \n",
    "\n",
    "**üîπ Long-form content (avoid loops)**\n",
    "frequency_penalty = 0.7  \n",
    "presence_penalty = 0.7  \n",
    "\n",
    "**üîπ Creative writing (encourage new ideas)**\n",
    "frequency_penalty = 1.0  \n",
    "presence_penalty = 1.2  \n",
    "\n",
    "---\n",
    "\n",
    "**üéØ Simple Analogy**\n",
    "\n",
    "- **Frequency Penalty** = ‚ÄúStop repeating the same words.‚Äù\n",
    "- **Presence Penalty** = ‚ÄúTalk about new things too.‚Äù\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d64bbe4",
   "metadata": {},
   "source": [
    "#### **STEP 9.9E ‚Äî Side-by-Side Comparison of Model Parameters (SEE the Difference)**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b9064de",
   "metadata": {},
   "source": [
    "**üî¨ Step 9.9E ‚Äî Parameter Comparison (Practical Intuition)**\n",
    "\n",
    "#### **Objective**\n",
    "Understand how changing model parameters affects:\n",
    "- Creativity\n",
    "- Repetition\n",
    "- Output length\n",
    "- Topic diversity\n",
    "\n",
    "We will compare multiple responses to the SAME question\n",
    "by changing only the model parameters.\n",
    "\n",
    "This helps in:\n",
    "- Chatbot tuning\n",
    "- RAG answer quality\n",
    "- Agent stability\n",
    "- Interview explanations\n",
    "- Production reliability"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecfa7310",
   "metadata": {},
   "source": [
    "- **Low temperature (0.1) is more accurate and deterministic.**\n",
    "- **Medium temperature (0.6) is a balance of accuracy and creativity.**\n",
    "- **High temperature (1.1) is more creative and can produce varied responses.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "777938ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===== üü¶ LOW TEMPERATURE (0.1) =====\n",
      "**What is a Python List?**\n",
      "\n",
      "In Python, a list is a collection of items that can be of any data type, including strings, integers, floats, and other lists. It's similar to an array in other programming languages, but more flexible and powerful.\n",
      "\n",
      "**Basic Concepts:**\n",
      "\n",
      "1. **Indexing**: Each item in a list has a unique index, which is like a label that helps you access it. Indexing starts from 0, so the first item is at index 0, the second item is at index 1, and so on.\n",
      "2. **S \n",
      "\n",
      "================üüßMEDIUM TEMPERATURE (0.6) =====\n",
      "**Python Lists: A Simple Explanation**\n",
      "\n",
      "In Python, a list is a collection of items that can be of any data type, including strings, integers, floats, and other lists. Think of it like a shopping list where you can store multiple items.\n",
      "\n",
      "**Basic Syntax**\n",
      "\n",
      "A list is defined using square brackets `[]` and elements are separated by commas `,`. For example:\n",
      "```python\n",
      "fruits = ['apple', 'banana', 'cherry']\n",
      "```\n",
      "In this example, `fruits` is a list containing three strings: `'apple'`, `'banana'`, and `'ch \n",
      "\n",
      "=======================üü• HIGH TEMPERATURE (1.1) =====\n",
      "**What are Python Lists?**\n",
      "\n",
      "In Python, a list is a type of data structure that stores multiple values in a single variable. Think of it like a shopping list where you can add, remove, and modify items as needed.\n",
      "\n",
      "**Basic Concepts:**\n",
      "\n",
      "- A list is defined by using square brackets `[]` and values are separated by commas.\n",
      "- Lists can store different data types, including numbers, strings, and even other lists.\n",
      "- Lists are mutable, which means they can be modified after they are created.\n",
      "\n",
      "**Example of a Python List:**\n",
      "\n",
      "```python\n",
      "my_list = [\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# üìò SECTION 9.9E ‚Äî Side-by-Side Output Comparison\n",
    "# ------------------------------------------------------------\n",
    "# Goal:\n",
    "#   - Ask the SAME question\n",
    "#   - Change only model parameters\n",
    "#   - Observe how output changes\n",
    "# ============================================================\n",
    "\n",
    "question = \"Explain Python lists in simple terms :\"\n",
    "\n",
    "response_low_temp = client.chat.completions.create(\n",
    "    model=\"llama-3.1-8b-instant\",\n",
    "    messages=[{\"role\":\"user\",\"content\":question}],\n",
    "    temperature=0.1,\n",
    "    top_p=0.9,\n",
    "    max_tokens=120\n",
    ")\n",
    "\n",
    "low_temp_output = response_low_temp.choices[0].message.content\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 2Ô∏è‚É£ MEDIUM TEMPERATURE (Balanced)\n",
    "# ------------------------------------------------------------\n",
    "\n",
    "response_mid_temp = client.chat.completions.create(\n",
    "    model=\"llama-3.1-8b-instant\",\n",
    "    messages=[{\"role\":\"user\",\"content\":question}],\n",
    "    temperature=0.6,\n",
    "    top_p=0.9,\n",
    "    max_tokens=120\n",
    ")\n",
    "\n",
    "mid_temp_output = response_mid_temp.choices[0].message.content\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 3Ô∏è‚É£ HIGH TEMPERATURE (Creative)\n",
    "# ------------------------------------------------------------\n",
    "\n",
    "response_high_temp = client.chat.completions.create(\n",
    "    model=\"llama-3.1-8b-instant\",\n",
    "    messages=[{\"role\":\"user\",\"content\":question}],\n",
    "    temperature=1.1,\n",
    "    top_p=1.0,\n",
    "    max_tokens=120,\n",
    "    presence_penalty=0.6\n",
    ")\n",
    "\n",
    "high_temp_output = response_high_temp.choices[0].message.content\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 4Ô∏è‚É£ DISPLAY RESULTS\n",
    "# ------------------------------------------------------------\n",
    "print(\"===== üü¶ LOW TEMPERATURE (0.1) =====\")\n",
    "print(low_temp_output, \"\\n\")\n",
    "\n",
    "print(\"================üüßMEDIUM TEMPERATURE (0.6) =====\")\n",
    "print(mid_temp_output, \"\\n\")\n",
    "\n",
    "print(\"=======================üü• HIGH TEMPERATURE (1.1) =====\")\n",
    "print(high_temp_output)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d90f7a1a",
   "metadata": {},
   "source": [
    "**# Observations from Step 9.9E**\n",
    "\n",
    "- Low temperature produced factual and concise output.\n",
    "- Medium temperature gave a balanced explanation.\n",
    "- High temperature introduced creativity and expressive language.\n",
    "\n",
    "Conclusion:\n",
    "- Parameter tuning is task-dependent.\n",
    "- There is NO single best setting.\n",
    "- Real-world GenAI systems dynamically adjust parameters.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f47d1d46",
   "metadata": {},
   "source": [
    "## üß† Parameter Selection by Use-Case\n",
    "\n",
    "### ü§ñ 1. Chatbots (Learning / Support / Assistant)\n",
    "- temperature: 0.5 ‚Äì 0.7\n",
    "- top_p: 0.9\n",
    "- max_tokens: 200‚Äì400\n",
    "- frequency_penalty: 0.2\n",
    "- presence_penalty: 0.2\n",
    "\n",
    "Why:\n",
    "- Friendly tone\n",
    "- Avoid repetition\n",
    "- Balanced creativity\n",
    "\n",
    "---\n",
    "\n",
    "### üìö 2. RAG (Retrieval-Augmented Generation)\n",
    "- temperature: 0.1 ‚Äì 0.3\n",
    "- top_p: 0.9\n",
    "- max_tokens: 150‚Äì300\n",
    "- frequency_penalty: 0\n",
    "- presence_penalty: 0\n",
    "\n",
    "Why:\n",
    "- Accuracy over creativity\n",
    "- Reduce hallucinations\n",
    "- Faithful to retrieved documents\n",
    "\n",
    "---\n",
    "\n",
    "### üßÆ 3. SQL / Code Generation\n",
    "- temperature: 0.0 ‚Äì 0.2\n",
    "- top_p: 0.9\n",
    "- max_tokens: 100‚Äì300\n",
    "- frequency_penalty: 0\n",
    "- presence_penalty: 0\n",
    "\n",
    "Why:\n",
    "- Deterministic output\n",
    "- Syntax correctness\n",
    "- No creativity needed\n",
    "\n",
    "---\n",
    "\n",
    "### üìÑ 4. JSON / Structured Extraction\n",
    "- temperature: 0.0\n",
    "- top_p: 0.9\n",
    "- max_tokens: 50‚Äì100\n",
    "- stop sequences: YES\n",
    "- penalties: 0\n",
    "\n",
    "Why:\n",
    "- Strict formatting\n",
    "- Machine-readable output\n",
    "- API safe\n",
    "\n",
    "---\n",
    "\n",
    "### üß† 5. Agents / Multi-Step Reasoning\n",
    "- temperature: 0.3 ‚Äì 0.5\n",
    "- top_p: 0.9\n",
    "- max_tokens: 300‚Äì600\n",
    "- frequency_penalty: 0.3\n",
    "- presence_penalty: 0.3\n",
    "\n",
    "Why:\n",
    "- Encourage reasoning\n",
    "- Avoid loops\n",
    "- Maintain stability\n",
    "\n",
    "---\n",
    "\n",
    "### ‚úçÔ∏è 6. Creative Writing / Brainstorming\n",
    "- temperature: 0.9 ‚Äì 1.2\n",
    "- top_p: 1.0\n",
    "- max_tokens: 500+\n",
    "- frequency_penalty: 0.7\n",
    "- presence_penalty: 0.7\n",
    "\n",
    "Why:\n",
    "- High creativity\n",
    "- Diverse ideas\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63689913",
   "metadata": {},
   "source": [
    "**FINAL SUMMARY ‚Äî Step 9.9E**\n",
    "- Learned how model parameters affect responses.\n",
    "- Saw real output differences with same input.\n",
    "- Understood why tuning is critical in production.\n",
    "- Built intuition required for GenAI interviews.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "828daefb",
   "metadata": {},
   "source": [
    "####  **‚≠ê STEP 9.9G ‚ÄîFinal Parameter Cheat-Sheet + Interview & Production Notes**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e61ceb88",
   "metadata": {},
   "source": [
    "**# üß† Step 9.9G ‚Äî Final LLM Parameter Cheat-Sheet & Interview Notes**\n",
    "\n",
    "This section summarizes all LLM control parameters learned so far.\n",
    "It is designed for:\n",
    "- Quick revision\n",
    "- Interview preparation\n",
    "- Production reference\n",
    "- Architecture decision-making\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "562c6175",
   "metadata": {},
   "source": [
    "**üìò FINAL PARAMETER CHEAT-SHEET (Core Knowledge)**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4958bea2",
   "metadata": {},
   "source": [
    "#### **üîß LLM Control Parameters ‚Äî Quick Reference**\n",
    "\n",
    "### üî• Temperature\n",
    "Controls creativity and randomness.\n",
    "\n",
    "- 0.0‚Äì0.2 ‚Üí factual, deterministic (SQL, JSON, RAG)\n",
    "- 0.3‚Äì0.6 ‚Üí balanced (chatbots, tutoring)\n",
    "- 0.7‚Äì1.2 ‚Üí creative (ideas, stories)\n",
    "\n",
    "---\n",
    "\n",
    "### üü£ Top-p (Nucleus Sampling)\n",
    "Controls how wide the model‚Äôs choice set is.\n",
    "\n",
    "- 1.0 ‚Üí allow all tokens (default)\n",
    "- 0.9 ‚Üí remove rare/unlikely tokens (recommended)\n",
    "- <0.5 ‚Üí very restrictive, robotic\n",
    "\n",
    "---\n",
    "\n",
    "### üü© Max Tokens\n",
    "Controls output length (NOT input length).\n",
    "\n",
    "- 50‚Äì100 ‚Üí JSON / extraction\n",
    "- 150‚Äì300 ‚Üí RAG answers\n",
    "- 300‚Äì600 ‚Üí agents / reasoning\n",
    "- 500+ ‚Üí creative writing\n",
    "\n",
    "---\n",
    "\n",
    "### üî∂ Frequency Penalty\n",
    "Reduces repeated words.\n",
    "\n",
    "- 0.0 ‚Üí no restriction\n",
    "- 0.2‚Äì0.7 ‚Üí natural language\n",
    "- 1.0+ ‚Üí strong repetition control\n",
    "\n",
    "---\n",
    "\n",
    "### üî∑ Presence Penalty\n",
    "Encourages new topics.\n",
    "\n",
    "- 0.0 ‚Üí stay focused\n",
    "- 0.2‚Äì0.7 ‚Üí broader responses\n",
    "- 1.0+ ‚Üí idea exploration\n",
    "\n",
    "---\n",
    "\n",
    "### ‚õî Stop Sequences\n",
    "Forces model to stop output.\n",
    "\n",
    "Used for:\n",
    "- JSON-only responses\n",
    "- Tool calling\n",
    "- RAG boundaries\n",
    "- Preventing hallucination"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b555517a",
   "metadata": {},
   "source": [
    "**üß† INTERVIEW-LEVEL INSIGHTS (VERY IMPORTANT)**\n",
    "\n",
    "**üéØ Interview Notes (Google / Microsoft Level)**\n",
    "\n",
    "1. There is NO single best parameter setup.\n",
    "2. Parameters must be chosen based on task.\n",
    "3. RAG prioritizes accuracy over creativity.\n",
    "4. Agents require loop prevention (penalties).\n",
    "5. Structured output requires stop sequences.\n",
    "6. Prompt + parameters together control behavior.\n",
    "7. Determinism is critical for production systems.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45c7922c",
   "metadata": {},
   "source": [
    "**üèóÔ∏è PRODUCTION DECISION TABLE (REAL-WORLD)**\n",
    "\n",
    "**üè≠ Production Parameter Selection**\n",
    "\n",
    "| Use Case | Temp | Top-p | Max Tokens | Penalties |\n",
    "|--------|------|-------|------------|-----------|\n",
    "| Chatbot | 0.5 | 0.9 | 300 | 0.2 / 0.2 |\n",
    "| RAG | 0.2 | 0.9 | 200 | 0 / 0 |\n",
    "| SQL | 0.0 | 0.9 | 150 | 0 / 0 |\n",
    "| JSON | 0.0 | 0.9 | 80 | stop seq |\n",
    "| Agent | 0.4 | 0.9 | 500 | 0.5 / 0.5 |\n",
    "| Creative | 1.0 | 1.0 | 600 | 0.7 / 0.7 |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcd1cd18",
   "metadata": {},
   "source": [
    "## ‚úÖ Step 9 ‚Äî LLM Basics Final Summary\n",
    "\n",
    "- Learned how LLMs generate responses.\n",
    "- Understood prompt roles (system, user, assistant).\n",
    "- Mastered zero/one/few-shot prompting.\n",
    "- Gained control over creativity, length, repetition.\n",
    "- Learned production-grade parameter tuning.\n",
    "- Built interview-ready mental models.\n",
    "\n",
    "Status: LLM BASICS COMPLETED ‚úÖ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89b7de59",
   "metadata": {},
   "source": [
    "### **üíª Mini Practice**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "d6b474d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== Low Temp =====\n",
      "**What are Python Dictionaries?**\n",
      "\n",
      "In Python, a dictionary is a data structure that stores a collection of key-value pairs. It's like a phonebook where you have names (keys) and phone numbers (values).\n",
      "\n",
      "**Key Features:**\n",
      "\n",
      "1. **Key-Value Pairs**: Each item in a dictionary is a pair of a key and a value.\n",
      "2. **Unique Keys**: Each key in a dictionary must be unique, just like a phone number.\n",
      "3. **Flexible Data Types**: Keys and values can be any data type, including strings, integers, floats, lists,\n",
      "\n",
      "===== Medium Temp =====\n",
      "**What are Python Dictionaries?**\n",
      "\n",
      "In Python, a dictionary is a data structure that stores a collection of key-value pairs. It's like a phonebook where you have names (keys) and phone numbers (values). You can easily look up a phone number by its corresponding name.\n",
      "\n",
      "**Key Features:**\n",
      "\n",
      "1. **Keys are unique**: Each key in a dictionary must be unique, just like a phone number.\n",
      "2. **Values can be any type**: The values in a dictionary can be strings, numbers, lists, or even other dictionaries.\n",
      "3. **Access values by key**:\n",
      "\n",
      "===== HIgh Temp =====\n",
      "**What are Python Dictionaries?**\n",
      "\n",
      "In simple terms, a Python dictionary is an unordered collection of key-value pairs. Think of it like a phonebook, where each person's name is the key, and their phone number is the value. You can look up a person's name and quickly find their phone number.\n",
      "\n",
      "**How do Dictionaries Work?**\n",
      "\n",
      "A dictionary is defined using curly brackets `{}` and contains a list of key-value pairs separated by commas. Each key-value pair is separated by a colon `:`.\n",
      "\n",
      "Example:\n",
      "```python\n",
      "person = {\n",
      "    \"John\": \"123-\n",
      "\n",
      "===== Low Top-p =====\n",
      "**What are Python Dictionaries?**\n",
      "\n",
      "In Python, a dictionary is a data structure that stores a collection of key-value pairs. It's like a phonebook where you have names (keys) and phone numbers (values).\n",
      "\n",
      "**Key Features:**\n",
      "\n",
      "1. **Keys**: Unique identifiers for each value. Keys can be strings, integers, or any other immutable type.\n",
      "2. **Values**: The data associated with each key. Values can be of any type, including strings, integers, lists, dictionaries, etc.\n",
      "3. **Unordered**: Dictionaries do not maintain a specific order of key\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# üß™ MINI PRACTICE ‚Äî Parameter Intuition Builder\n",
    "# ------------------------------------------------------------\n",
    "# Goal:\n",
    "#   - Change ONE parameter at a time\n",
    "#   - Observe how output changes\n",
    "# ============================================================\n",
    "\n",
    "question = \"Explain Python dictionaries in simple terms.\"\n",
    "\n",
    "configs = [\n",
    "    {\"label\":\"Low Temp\",\"temperature\":0.1,\"top_p\":0.9},\n",
    "    {\"label\":\"Medium Temp\",\"temperature\":0.6,\"top_p\":0.9},\n",
    "    {\"label\":\"HIgh Temp\",\"temperature\":1.1,\"top_p\":0.9},\n",
    "    {\"label\":\"Low Top-p\",\"temperature\":0.6,\"top_p\":0.4}\n",
    "]\n",
    "\n",
    "for cfg in configs:\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"llama-3.1-8b-instant\",\n",
    "        messages=[{\"role\":\"user\",\"content\":question}],\n",
    "        temperature=cfg[\"temperature\"],\n",
    "        top_p=cfg[\"top_p\"],\n",
    "        max_tokens=120\n",
    "    )\n",
    "\n",
    "    print(f\"\\n===== {cfg['label']} =====\")\n",
    "    print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c1ac50e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
