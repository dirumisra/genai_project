{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "68c15333",
   "metadata": {},
   "source": [
    "#### **Topic 1 Why Prompt Engineering Matters'**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0d47a7a",
   "metadata": {},
   "source": [
    "**üß† What this Topic is About (Plain Language)**\n",
    "\n",
    "Prompt engineering is how you talk to the LLM so it understands:\n",
    "\n",
    "what to do\n",
    "\n",
    "what not to do\n",
    "\n",
    "how to format the answer\n",
    "\n",
    "Even the best model will fail if the prompt is unclear.\n",
    "\n",
    "üß© Key Ideas\n",
    "\n",
    " - LLMs do not think ‚Äî they follow instructions statistically\n",
    " - Vague prompts ‚Üí vague, unsafe, unusable outputs\n",
    " - Clear prompts ‚Üí predictable, production-ready outputs\n",
    " - Prompt ‚â† parameters\n",
    "    - Prompt = WHAT to do\n",
    "    - Parameters = HOW to behave while doing it\n",
    "\n",
    "**üß™ Real-World Impact**\n",
    "\n",
    "- Without good prompts:\n",
    "    - Chatbots hallucinate\n",
    "    - JSON breaks\n",
    "    - APIs return extra text\n",
    "    - Agents behave randomly\n",
    "- With good prompts:\n",
    "    - Outputs are deterministic\n",
    "    - Easy to parse\n",
    "    - Safe for automation\n",
    "    - Interview-ready reasoning\n",
    "\n",
    "**‚úÖ Topic 1 Summary**\n",
    "\n",
    "Topic 1: Why Prompt Engineering Matters\n",
    "\n",
    "Prompts control model intent\n",
    "\n",
    "Parameters cannot fix a bad prompt\n",
    "\n",
    "Prompt clarity reduces hallucinations\n",
    "\n",
    "Every GenAI system starts with prompt design\n",
    "\n",
    "Senior engineers fix prompts before tuning models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "967b298d",
   "metadata": {},
   "source": [
    "#### **Topic 2 Prompt Anatomy (Instruction, Context, Constraints, Output Format)**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4f2d2f2",
   "metadata": {},
   "source": [
    "#### **üß† What ‚ÄúPrompt Anatomy‚Äù Means (Plain Language)**\n",
    "\n",
    "A good prompt is not random text.\n",
    "It is a structured instruction made of four critical parts:\n",
    " - Instruction ‚Äì What the model must do\n",
    " - Context ‚Äì Information the model should use\n",
    " - Constraints ‚Äì What the model must NOT do\n",
    " - Output Format ‚Äì How the answer must look\n",
    " \n",
    " Never write prompts casually."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91a7c266",
   "metadata": {},
   "source": [
    "#### **üîπ Part 1 ‚Äî Instruction (MOST IMPORTANT)**\n",
    "\n",
    "What it is\n",
    "The core task you want the model to perform.\n",
    "\n",
    "**Bad Instruction ‚ùå**\n",
    "\n",
    "- ‚ÄúExplain Python‚Äù\n",
    "\n",
    "Too broad. Too vague.\n",
    "\n",
    "**Good Instruction ‚úÖ**\n",
    "\n",
    "- ‚ÄúExplain Python lists in simple terms for a beginner‚Äù\n",
    "\n",
    "Clear task + scope"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fb4fea4",
   "metadata": {},
   "source": [
    "#### **üîπ Part 2 ‚Äî Context (When Needed)**\n",
    "\n",
    "**What it is**\n",
    "\n",
    "Extra information that grounds the model.\n",
    "\n",
    "**Example**\n",
    "\n",
    "- ‚ÄúYou are teaching a beginner who has never coded before.‚Äù\n",
    "\n",
    "**Context reduces:**\n",
    "\n",
    "- hallucinations\n",
    "- wrong assumptions\n",
    "- over-complex answers\n",
    "\n",
    "‚ö†Ô∏è No context ‚â† wrong\n",
    "But wrong context = bad output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce0bd654",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5d5da789",
   "metadata": {},
   "source": [
    "#### **üîπ Part 3 ‚Äî Constraints (CRITICAL FOR PRODUCTION)**\n",
    "**What it is**\n",
    "\n",
    "Rules that limit model behavior.\n",
    "\n",
    "**Examples:**\n",
    "- ‚ÄúDo not add explanations‚Äù\n",
    "- ‚ÄúAnswer in less than 50 words‚Äù\n",
    "- ‚ÄúUse only the provided context‚Äù\n",
    "- ‚ÄúDo not hallucinate‚Äù\n",
    "\n",
    "**Constraints are how you:**\n",
    "- control safety\n",
    "- control verbosity\n",
    "- control format"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cd7bdf1",
   "metadata": {},
   "source": [
    "**üîπ Part 4 ‚Äî Output Format (SYSTEMS DEPEND ON THIS)**\n",
    "**What it is**\n",
    "\n",
    "Defines how the output must look.\n",
    "\n",
    "**Examples:**\n",
    "- JSON only\n",
    "- Bullet points\n",
    "- SQL query only\n",
    "- Python code only\n",
    "\n",
    "**Without output format:**\n",
    "\n",
    "    ‚ùå APIs break\n",
    "    ‚ùå JSON parsing fails\n",
    "    ‚ùå Automation becomes impossible"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e907f284",
   "metadata": {},
   "source": [
    "**Instruction:**\n",
    "Summarize the following text.\n",
    "\n",
    "**Context:**\n",
    "\n",
    "The text is written for non-technical users.\n",
    "\n",
    "**Constraints:**\n",
    "\n",
    "- Use simple language\n",
    "- Do not exceed 3 bullet points\n",
    "- Do not add extra commentary\n",
    "\n",
    "**Output Format:**\n",
    "\n",
    "Return only bullet points.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ca42370",
   "metadata": {},
   "source": [
    "#### **‚úÖ Topic 2 Summary Prompt Anatomy**\n",
    "\n",
    "- Instruction defines the task\n",
    "- Context grounds the response\n",
    "- Constraints control behavior\n",
    "- Output format enables automation\n",
    "- Missing any part leads to unreliable output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0584b33",
   "metadata": {},
   "source": [
    "#### **Topic 3 Zero-Shot Prompting**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19b7b799",
   "metadata": {},
   "source": [
    "**üß† What Is Zero-Shot Prompting?**\n",
    "\n",
    "Zero-shot prompting means:\n",
    "\n",
    "    You give the model only instructions, no examples,\n",
    "        and expect it to perform the task correctly.\n",
    "\n",
    "**The model relies entirely on:**\n",
    "- its pretrained knowledge\n",
    "- your prompt clarity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88e46d2e",
   "metadata": {},
   "source": [
    "**üîπ When Zero-Shot Works Well**\n",
    "\n",
    "Zero-shot prompting is ideal when:\n",
    "- The task is simple or common\n",
    "- The output format is clear\n",
    "- The domain is general knowledge\n",
    "\n",
    "**Examples:**\n",
    "\n",
    " - Summarization\n",
    " - Classification\n",
    " - Simple Q&A\n",
    " - Explanation tasks\n",
    "\n",
    "#### **üîπ When Zero-Shot Fails**\n",
    "\n",
    "Zero-shot often fails when:\n",
    "- The task is ambiguous\n",
    "- The format is strict (JSON, SQL, code)\n",
    "- The task is domain-specific\n",
    "- Consistency is required\n",
    "\n",
    "In such cases:\n",
    "\n",
    "‚û°Ô∏è One-shot or few-shot is better (next topics)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a19292f",
   "metadata": {},
   "source": [
    "#### **üß™ Zero-Shot Example (Clear vs Unclear)**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc734158",
   "metadata": {},
   "source": [
    "‚ùå Poor Zero-Shot Prompt\n",
    "\n",
    "    Explain LLMs.\n",
    "\n",
    "**Problems:**\n",
    "- No audience defined\n",
    "- No length constraint\n",
    "- No structure"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "822839ca",
   "metadata": {},
   "source": [
    "#### **‚úÖ Good Zero-Shot Prompt**\n",
    "\n",
    "    Explain what a Large Language Model is\n",
    "    for a non-technical beginner.\n",
    "    Use simple language and keep it under 5 lines.\n",
    "    \n",
    "#### **Why this works:**\n",
    "- Clear instruction\n",
    "- Clear audience\n",
    "- Clear constraint\n",
    "\n",
    "**üß† Important Mental Model**\n",
    "\n",
    "Zero-shot success depends more on prompt quality than model power.\n",
    "\n",
    "**Even the best model:**\n",
    "\n",
    "- will guess if instructions are vague\n",
    "- will over-explain if unconstrained\n",
    "\n",
    "**üß™ Real-World Usage**\n",
    "\n",
    "**Zero-shot is commonly used in:**\n",
    "- Chatbots (simple queries)\n",
    "- Support FAQs\n",
    "- Content rewriting\n",
    "- First-pass summarization\n",
    "\n",
    "**But not for:**\n",
    "- Structured APIs\n",
    "- Deterministic pipelines\n",
    "- Critical automation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a4611e9",
   "metadata": {},
   "source": [
    "**‚úÖ Topic 3 Summary Zero-Shot Prompting**\n",
    "\n",
    "- Uses instructions only (no examples)\n",
    "- Works best for simple, general tasks\n",
    "- Fails for strict or complex outputs\n",
    "- Requires very clear instructions\n",
    "- Prompt clarity matters more than model choice"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95039151",
   "metadata": {},
   "source": [
    "#### **Topic 4 Role Prompting (system / user / assistant)**\n",
    "\n",
    "**üß† What Is Role Prompting?**\n",
    "\n",
    "Role prompting means assigning explicit roles to messages so the LLM knows:\n",
    "\n",
    "- who is speaking\n",
    "- how to behave\n",
    "- what rules to follow\n",
    "\n",
    "In chat-based LLMs, roles are:\n",
    "- system\n",
    "- user\n",
    "- assistant\n",
    "\n",
    "Each role has a different purpose and authority.\n",
    "\n",
    "**üîπ Role 1 ‚Äî system (Highest Priority)**\n",
    "**What it does**\n",
    "\n",
    "- Defines global behavior\n",
    "- Sets rules, persona, tone\n",
    "- Overrides user intent if conflicting\n",
    "\n",
    "**Examples of system instructions**\n",
    "- ‚ÄúYou are a strict JSON generator.‚Äù\n",
    "- ‚ÄúYou are a senior Python interviewer.‚Äù\n",
    "- ‚ÄúYou must not hallucinate.‚Äù\n",
    "\n",
    "üìå Think of system as:\n",
    " - The operating system of the LLM session."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e23f9a5a",
   "metadata": {},
   "source": [
    "#### **üîπ Role 2 ‚Äî user (Task Request)**\n",
    "\n",
    "**What it does**\n",
    "- Contains the actual task or question\n",
    "- Follows the rules set by system\n",
    "\n",
    "**Example:**\n",
    "- ‚ÄúExplain Python lists.‚Äù\n",
    "\n",
    "üìå Users can ask anything, but:\n",
    "- System rules still apply\n",
    "- Constraints still apply"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57347632",
   "metadata": {},
   "source": [
    "#### **üîπ Role 3 ‚Äî assistant (Model Output)**\n",
    "\n",
    "**What it does**\n",
    "- Represents the model‚Äôs previous responses\n",
    "- Used mainly for:\n",
    "    - chat history\n",
    "    - multi-turn conversations\n",
    "\n",
    "üìå You rarely write assistant messages manually\n",
    "\n",
    "They are usually stored outputs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6e579af",
   "metadata": {},
   "source": [
    "**System:**\n",
    "\n",
    "You are a strict tutor. Answer briefly.\n",
    "\n",
    "**User:**\n",
    "\n",
    "Explain what a list is in Python.\n",
    "\n",
    "**Assistant:**\n",
    "\n",
    "A list is a collection that stores multiple items in order.\n",
    "\n",
    "---\n",
    "\n",
    "**Why this works:**\n",
    "\n",
    "System sets behavior\n",
    "\n",
    "User gives task\n",
    "\n",
    "Assistant follows both"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d2a3a39",
   "metadata": {},
   "source": [
    "**üß™ Real-World Usage**\n",
    "\n",
    "Role prompting is used in:\n",
    "- Chatbots with personality\n",
    "- Safety-critical systems\n",
    "- Interview bots\n",
    "- Customer support bots\n",
    "- Agent-based workflows"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7bf5d16",
   "metadata": {},
   "source": [
    "‚úÖ Topic 4 Role Prompting Summary\n",
    "\n",
    "- system defines behavior and rules\n",
    "- user requests the task\n",
    "- assistant stores model replies\n",
    "- System role has highest priority\n",
    "- Essential for safety, consistency, and control"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfd8e1e7",
   "metadata": {},
   "source": [
    "#### **Topic 5 - Clear & Deterministic Prompts (Reducing Randomness)**\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "084aefdf",
   "metadata": {},
   "source": [
    "**üß† What Are ‚ÄúClear & Deterministic‚Äù Prompts?**\n",
    "\n",
    "A deterministic prompt is one that:\n",
    "- produces similar output every time\n",
    "- avoids ambiguity\n",
    "- is safe for automation and production\n",
    "\n",
    "Clarity comes from:\n",
    "- explicit instructions\n",
    "- explicit constraints\n",
    "- explicit output format\n",
    "\n",
    "Determinism comes from:\n",
    " - clear prompts + controlled parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85bdb9b1",
   "metadata": {},
   "source": [
    "#### **üîπ Why Determinism Matters (Very Important)**\n",
    "\n",
    "In real systems:\n",
    "- APIs expect consistent responses\n",
    "- JSON parsers break on extra text\n",
    "- Agents fail if outputs vary too much\n",
    "\n",
    "A prompt that works once but fails later is not production-ready."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee5de9cb",
   "metadata": {},
   "source": [
    "#### **üîπ Common Causes of Non-Deterministic Output**\n",
    "\n",
    "‚ùå Vague verbs\n",
    "\n",
    "‚ùå Missing constraints\n",
    "\n",
    "‚ùå No output format\n",
    "\n",
    "‚ùå Overly creative wording"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36c27e35",
   "metadata": {},
   "source": [
    "Give me some details about a user.\n",
    "\n",
    "**This invites:**\n",
    "- guessing\n",
    "- verbosity\n",
    "- hallucination"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37db4190",
   "metadata": {},
   "source": [
    "#### **üîπ How to Make Prompts Deterministic (Checklist)**\n",
    "\n",
    "A deterministic prompt usually contains:\n",
    "1. Specific task\n",
    "2. Clear audience\n",
    "3. Explicit constraints\n",
    "4. Strict output format"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3ed799b",
   "metadata": {},
   "source": [
    "#### **üß™ Example: Non-Deterministic vs Deterministic Prompt**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44e39043",
   "metadata": {},
   "source": [
    "**‚ùå Non-Deterministic**\n",
    "\n",
    "Describe a product.\n",
    "\n",
    "**Deterministic**\n",
    "\n",
    "- Describe the product in exactly 3 bullet points.\n",
    "- Use simple language.\n",
    "- Do not add extra text.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d50bc07",
   "metadata": {},
   "source": [
    "#### **üß™ Real-World Usage**\n",
    "\n",
    "Clear & deterministic prompts are required in:\n",
    "\n",
    "- API responses\n",
    "- JSON outputs\n",
    "- SQL generation\n",
    "- Report generation\n",
    "- Agent tool calls\n",
    "\n",
    "They are less critical for:\n",
    "- casual chat\n",
    "- brainstorming\n",
    "- creative writing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0703ec10",
   "metadata": {},
   "source": [
    "**‚úÖ Topic 5 Summary**\n",
    "\n",
    "Clear & Deterministic Prompts\n",
    "\n",
    "- Deterministic prompts reduce randomness\n",
    "- Clarity comes from explicit instructions\n",
    "- Constraints prevent hallucinations\n",
    "- Output format enables automation\n",
    "- Mandatory for production systems"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2246af84",
   "metadata": {},
   "source": [
    "#### **Mini Mock Test + Day 6 Closure**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5d171f6",
   "metadata": {},
   "source": [
    "**Q1. What is the difference between a prompt and parameters in an LLM system?**\n",
    "\n",
    "**Answer:**\n",
    "- A prompt defines what the model should do (task, rules, format).\n",
    "- Parameters define how the model behaves while doing it (randomness, length, stopping).\n",
    "- A good prompt is mandatory; parameters only fine-tune behavior.\n",
    "\n",
    "---\n",
    "**Q2. Why can‚Äôt parameters (temperature, top_p) fix a badly written prompt?**\n",
    "\n",
    "**Answer:**\n",
    "\n",
    "Because parameters control randomness, not intent.\n",
    "If the prompt is vague or incorrect, the model doesn‚Äôt know the correct goal, \n",
    "so changing parameters only changes how wrong the output is‚Äînot what it should do.\n",
    "\n",
    "---\n",
    "**Q3. What are the four parts of prompt anatomy? Explain each briefly.**\n",
    "\n",
    "**Answer:**\n",
    "\n",
    "1. Instruction ‚Äì Defines the task the model must perform\n",
    "2. Context ‚Äì Provides background or situational information\n",
    "3. Constraints ‚Äì Specifies rules (what not to do, limits, safety)\n",
    "4. Output Format ‚Äì Defines how the response should be structured\n",
    "\n",
    "---\n",
    "**Q4. When does zero-shot prompting fail in real systems?**\n",
    "\n",
    "**Answer:**\n",
    "\n",
    "Zero-shot prompting fails when:\n",
    "\n",
    "- Output must be strictly structured (JSON, SQL)\n",
    "- The task is domain-specific\n",
    "- Consistency across runs is required\n",
    "- The task is complex or ambiguous\n",
    "\n",
    "---\n",
    "**Q5. Why is the system role more powerful than the user role?**\n",
    "\n",
    "**Answer:**\n",
    "\n",
    "The system role defines global behavior and rules.\n",
    "If the system and user instructions conflict, the system role always takes priority, making it critical for safety, tone control, and determinism.\n",
    "\n",
    "---\n",
    "**Q6. You are building an API that returns JSON. What prompt elements are mandatory?**\n",
    "\n",
    "**Answer:**\n",
    "\n",
    "Mandatory elements:\n",
    "\n",
    "- Explicit instruction to return JSON only\n",
    "- Clear output schema (keys, structure)\n",
    "- Constraints like ‚Äúno explanation‚Äù or ‚Äúno extra text‚Äù\n",
    "\n",
    "---\n",
    "**Q7. ‚ÄúHow do you make LLM output deterministic in production?‚Äù**\n",
    "\n",
    "**Answer (Sample):**\n",
    "\n",
    "By writing clear prompts with explicit constraints and output formats, using low temperature and top_p values, setting max_tokens limits, and enforcing stop sequences to prevent extra output."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "758a988d",
   "metadata": {},
   "source": [
    "#### **üìò Day 6 ‚Äî Final Summary (Revised)**\n",
    "\n",
    "\n",
    "Today I learned:\n",
    "- Why prompt engineering is critical for GenAI systems\n",
    "- How prompts differ from parameters\n",
    "- The four-part anatomy of a good prompt\n",
    "- Strengths and limits of zero-shot prompting\n",
    "- How role prompting controls behavior\n",
    "- How clarity and constraints reduce randomness\n",
    "\n",
    "Key insight:\n",
    "Prompt engineering defines intent; parameters refine execution.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68b62b8c",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
