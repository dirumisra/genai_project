{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "47c896c7",
   "metadata": {},
   "source": [
    "#### **üß© üìò Code Cell 1 ‚Äî Configure Grok Client (with Headings + Comments)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4feeffe9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Groq client configured successfully and ready to use.\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# üìò SECTION 1 ‚Äî Import Required Libraries\n",
    "# ------------------------------------------------------------\n",
    "# Why?\n",
    "#   - os: Interact with operating system (read env variables)\n",
    "#   - dotenv: Load API keys from .env files securely\n",
    "#   - OpenAI: Used because Groq follows the OpenAI-compatible API format\n",
    "# ============================================================\n",
    "\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI\n",
    "\n",
    "# ============================================================\n",
    "# üìò SECTION 2 ‚Äî Load Environment Variables (.env.dev)\n",
    "# ------------------------------------------------------------\n",
    "# Why?\n",
    "#   - Keeps API keys OUT of source code\n",
    "#   - Allows switching between environments:\n",
    "#       dev / uat / prod\n",
    "#   - Uses python-dotenv to read envs/.env.dev\n",
    "# ============================================================\n",
    "\n",
    "load_dotenv(\"../../../envs/.env.dev\")\n",
    "\n",
    "# ============================================================\n",
    "# üìò SECTION 3 ‚Äî Read the Groq API Key from Environment\n",
    "# ------------------------------------------------------------\n",
    "# Why?\n",
    "#   - GROQ_API_KEY is stored safely in .env.dev\n",
    "#   - Using os.getenv ensures security + flexibility\n",
    "#   - If the key is missing, we raise an error immediately\n",
    "# ============================================================\n",
    "\n",
    "groq_api_key = os.getenv(\"GROQ_API_KEY\")\n",
    "\n",
    "if not groq_api_key:\n",
    "    raise RuntimeError(\n",
    "        \"‚ùå GROQ_API_KEY is missing. Please verify it exists inside envs/.env.dev\"\n",
    "    )\n",
    "\n",
    "# ============================================================\n",
    "# üìò SECTION 4 ‚Äî Create the Groq Client (OpenAI-compatible)\n",
    "# ------------------------------------------------------------\n",
    "# Why?\n",
    "#   - Groq uses OpenAI-style API endpoints\n",
    "#   - base_url MUST be set to https://api.groq.com/openai/v1\n",
    "#   - After this, we can use:\n",
    "#         client.chat.completions.create(...)\n",
    "#   - This client object will be reused in all other notebook cells\n",
    "# ============================================================\n",
    "\n",
    "client = OpenAI(\n",
    "    api_key=groq_api_key,\n",
    "    base_url=\"https://api.groq.com/openai/v1\"\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Groq client configured successfully and ready to use.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b19d308",
   "metadata": {},
   "source": [
    "#### **First Grok Chat Request (Hello World LLM request)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ee690d6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ü§ñ Groq says:\n",
      "\n",
      "I'm Groq, your friendly Python tutor. I'm here to help you learn and explore the amazing world of Python. I'll try to make coding fun and easy to understand.\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# üìò SECTION 5 ‚Äî First Chat Request to Groq (Hello World)\n",
    "# ------------------------------------------------------------\n",
    "# Goal:\n",
    "#   - Verify that the Groq client works end-to-end\n",
    "#   - Send a simple question to the model\n",
    "#   - Receive and print the assistant's reply\n",
    "# ============================================================\n",
    "\n",
    "# 1Ô∏è‚É£ Build the messages list (conversation context)\n",
    "messages = [\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": \"You are a friendly Python tutor. Explain things in simple language.\"\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"Hello Groq! This is my first request. Please introduce yourself in 2‚Äì3 lines.\"\n",
    "    }\n",
    "]\n",
    "\n",
    "# 2Ô∏è‚É£ Send the chat completion request to Groq\n",
    "#    Using a supported model: llama-3.1-8b-instant\n",
    "response = client.chat.completions.create(\n",
    "    model=\"llama-3.1-8b-instant\",\n",
    "    messages=messages\n",
    ")\n",
    "\n",
    "# 3Ô∏è‚É£ Extract the assistant's reply\n",
    "#    IMPORTANT:\n",
    "#    - response.choices[0].message is an object (ChatCompletionMessage)\n",
    "#    - So we must use `.content` (attribute), NOT [\"content\"] (dict style)\n",
    "assistant_reply = response.choices[0].message.content\n",
    "\n",
    "# 4Ô∏è‚É£ Print reply\n",
    "print(\"ü§ñ Groq says:\\n\")\n",
    "print(assistant_reply)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd0afe25",
   "metadata": {},
   "source": [
    "#### **Inspect the Raw Response Object**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "dd900199",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======= RAW RESPONSE OBJECT =======\n",
      "ChatCompletion(id='chatcmpl-b64537d8-5681-4560-a892-fdfea48fff9c', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content=\"In the context of Large Language Model (LLM) APIs, a response object is a data structure that contains the output of the model's processing. It typically includes:\\n\\n1. The generated text or response to the input query or prompt\\n2. Metadata such as timestamp, model version, and model configuration\\n3. Optional fields such as error messages, warnings, or confidence scores\\n\\nThe response object varies in format depending on the specific LLM API being used, but it generally provides the output of the model in a structured and easily consumable way.\", refusal=None, role='assistant', annotations=None, audio=None, function_call=None, tool_calls=None))], created=1764707117, model='llama-3.1-8b-instant', object='chat.completion', service_tier='on_demand', system_fingerprint='fp_ff2b098aaf', usage=CompletionUsage(completion_tokens=112, prompt_tokens=61, total_tokens=173, completion_tokens_details=None, prompt_tokens_details=None, queue_time=0.049557994, prompt_time=0.003539636, completion_time=0.235021449, total_time=0.238561085), usage_breakdown=None, x_groq={'id': 'req_01kbgbv9sje9t9yze8zm0c8asy', 'seed': 1170721849})\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# üìò SECTION 6 ‚Äî Inspecting the Raw Response Object\n",
    "# ------------------------------------------------------------\n",
    "# Why?\n",
    "#   - To see the full structure returned by the LLM.\n",
    "#   - This helps us understand:\n",
    "#       * where the model's reply lives\n",
    "#       * how choices[] is structured\n",
    "#       * how we might access metadata later (tokens, model, etc.)\n",
    "# ============================================================\n",
    "\n",
    "# üß© 1) Build a simple messages list for testing\n",
    "#    Why?\n",
    "#      - We send a short, clear question so the response object\n",
    "#        is easy to read and understand.\n",
    "\n",
    "messages = [\n",
    "    {\n",
    "        \"role\":\"system\",\n",
    "        \"content\":\"You are a helpful assistant who explains things simply:\" \n",
    "    },\n",
    "    {\n",
    "        \"role\":\"user\",\n",
    "        \"content\":\"What is a response object in the context of LLM APIs? Explain briefly.\"\n",
    "    }\n",
    "]\n",
    "\n",
    "# üß© 2) Send a chat completion request to Groq\n",
    "#    Why?\n",
    "#      - Same pattern as before:\n",
    "#          client.chat.completions.create(...)\n",
    "#      - We use the same model as in Section 5.\n",
    "response = client.chat.completions.create(\n",
    "    model=\"llama-3.1-8b-instant\",\n",
    "    messages=messages\n",
    ")\n",
    "\n",
    "# üß© 3) Print the entire response object\n",
    "#    Why?\n",
    "#      - For learning, we want to see everything Groq returns.\n",
    "#      - In real applications, we wouldn't print this every time.\n",
    "\n",
    "print(\"======= RAW RESPONSE OBJECT =======\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "260689b7",
   "metadata": {},
   "source": [
    "#### **Extract Key Fields from the Response Object**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "54a4ee68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===== üîé RESPONSE SUMMARY =====\n",
      "Model Used    :llama-3.1-8b-instant\n",
      "Assistant role :assistant\n",
      "Finish reason  :stop\n",
      "\n",
      "{'----- üß† Assistant Reply -----'}\n",
      "In the context of Large Language Model (LLM) APIs, a response object is a data structure that contains the output of the model's processing. It typically includes:\n",
      "\n",
      "1. The generated text or response to the input query or prompt\n",
      "2. Metadata such as timestamp, model version, and model configuration\n",
      "3. Optional fields such as error messages, warnings, or confidence scores\n",
      "\n",
      "The response object varies in format depending on the specific LLM API being used, but it generally provides the output of the model in a structured and easily consumable way.\n",
      "\n",
      "----- üìä Token Usage -----\n",
      "Prompt tokens    :61\n",
      "completion_tokens   :112\n",
      "Total Tokens   :173\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# üìò SECTION 7 ‚Äî Extracting Important Fields from Response\n",
    "# ------------------------------------------------------------\n",
    "# Goal:\n",
    "#   - Understand how to read specific parts of the response:\n",
    "#       * Model name\n",
    "#       * Assistant role\n",
    "#       * Assistant message (reply)\n",
    "#       * Finish reason\n",
    "#       * Token usage (if available)\n",
    "#\n",
    "# Note:\n",
    "#   - This assumes 'response' already exists from SECTION 6.\n",
    "#   - If not, re-run SECTION 6 before running this cell.\n",
    "# ============================================================\n",
    "# üß© 1) Extract the model name\n",
    "#    Why?\n",
    "#      - Useful for logging, debugging, and knowing which LLM handled the request.\n",
    "\n",
    "model_name = response.model\n",
    "\n",
    "# üß© 2) Extract the first choice (index 0)\n",
    "#    Why?\n",
    "#      - Most calls only care about the first suggested answer.\n",
    "\n",
    "first_choice = response.choices[0]\n",
    "\n",
    "# üß© 3) Extract the assistant's message role and content\n",
    "#    Why?\n",
    "#      - role  ‚Üí usually \"assistant\"\n",
    "#      - content ‚Üí actual reply text from the model\n",
    "\n",
    "assistant_role = first_choice.message.role\n",
    "assistant_content = first_choice.message.content\n",
    "\n",
    "# üß© 4) Extract the finish reason\n",
    "#    Why?\n",
    "#      - Tells us WHY the model stopped generating:\n",
    "#          * \"stop\"        ‚Üí completed naturally\n",
    "#          * \"length\"      ‚Üí hit max_tokens limit\n",
    "#          * \"content_filter\" ‚Üí blocked by safety filter (in some providers)\n",
    "\n",
    "finish_reason = first_choice.finish_reason\n",
    "\n",
    "# üß© 5) Extract token usage (if available)\n",
    "#    Why?\n",
    "#      - Helps us understand cost and length of prompts/responses.\n",
    "#      - Some providers may not always return usage; we handle that safely.\n",
    "\n",
    "usage_info = getattr(response,\"usage\",None)\n",
    "\n",
    "if usage_info:\n",
    "    prompt_tokens = usage_info.prompt_tokens\n",
    "    completion_tokens = usage_info.completion_tokens\n",
    "    total_tokens = usage_info.total_tokens\n",
    "else:\n",
    "    prompt_tokens = completion_tokens = total_tokens = None\n",
    "\n",
    "# üß© 6) Print everything in a clean, readable way\n",
    "\n",
    "print(\"===== üîé RESPONSE SUMMARY =====\")\n",
    "print(f\"Model Used    :{model_name}\")\n",
    "print(f\"Assistant role :{assistant_role}\")\n",
    "print(f\"Finish reason  :{finish_reason}\")\n",
    "print()\n",
    "print({\"----- üß† Assistant Reply -----\"})\n",
    "print(assistant_content)\n",
    "print()\n",
    "\n",
    "if usage_info:\n",
    "    print(\"----- üìä Token Usage -----\")\n",
    "    print(f\"Prompt tokens    :{prompt_tokens}\")\n",
    "    print(f\"completion_tokens   :{completion_tokens}\")\n",
    "    print(f\"Total Tokens   :{total_tokens}\")\n",
    "else:\n",
    "    print(\"Token usage information not provided by this response.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cf20038",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
