{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "47c896c7",
   "metadata": {},
   "source": [
    "#### **üß© üìò Code Cell 1 ‚Äî Configure Grok Client (with Headings + Comments)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4feeffe9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Groq client configured successfully and ready to use.\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# üìò SECTION 1 ‚Äî Import Required Libraries\n",
    "# ------------------------------------------------------------\n",
    "# Why?\n",
    "#   - os: Interact with operating system (read env variables)\n",
    "#   - dotenv: Load API keys from .env files securely\n",
    "#   - OpenAI: Used because Groq follows the OpenAI-compatible API format\n",
    "# ============================================================\n",
    "\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI\n",
    "\n",
    "# ============================================================\n",
    "# üìò SECTION 2 ‚Äî Load Environment Variables (.env.dev)\n",
    "# ------------------------------------------------------------\n",
    "# Why?\n",
    "#   - Keeps API keys OUT of source code\n",
    "#   - Allows switching between environments:\n",
    "#       dev / uat / prod\n",
    "#   - Uses python-dotenv to read envs/.env.dev\n",
    "# ============================================================\n",
    "\n",
    "load_dotenv(\"../../../envs/.env.dev\")\n",
    "\n",
    "# ============================================================\n",
    "# üìò SECTION 3 ‚Äî Read the Groq API Key from Environment\n",
    "# ------------------------------------------------------------\n",
    "# Why?\n",
    "#   - GROQ_API_KEY is stored safely in .env.dev\n",
    "#   - Using os.getenv ensures security + flexibility\n",
    "#   - If the key is missing, we raise an error immediately\n",
    "# ============================================================\n",
    "\n",
    "groq_api_key = os.getenv(\"GROQ_API_KEY\")\n",
    "\n",
    "if not groq_api_key:\n",
    "    raise RuntimeError(\n",
    "        \"‚ùå GROQ_API_KEY is missing. Please verify it exists inside envs/.env.dev\"\n",
    "    )\n",
    "\n",
    "# ============================================================\n",
    "# üìò SECTION 4 ‚Äî Create the Groq Client (OpenAI-compatible)\n",
    "# ------------------------------------------------------------\n",
    "# Why?\n",
    "#   - Groq uses OpenAI-style API endpoints\n",
    "#   - base_url MUST be set to https://api.groq.com/openai/v1\n",
    "#   - After this, we can use:\n",
    "#         client.chat.completions.create(...)\n",
    "#   - This client object will be reused in all other notebook cells\n",
    "# ============================================================\n",
    "\n",
    "client = OpenAI(\n",
    "    api_key=groq_api_key,\n",
    "    base_url=\"https://api.groq.com/openai/v1\"\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Groq client configured successfully and ready to use.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b19d308",
   "metadata": {},
   "source": [
    "#### **First Grok Chat Request (Hello World LLM request)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ee690d6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ü§ñ Groq says:\n",
      "\n",
      "Hello there! My name is Groq, and I'm a friendly Python tutor. I'm excited to help you learn the basics of Python and explore its many cool features. I'll explain things in simple language, so don't worry if you're new to programming!\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# üìò SECTION 5 ‚Äî First Chat Request to Groq (Hello World)\n",
    "# ------------------------------------------------------------\n",
    "# Goal:\n",
    "#   - Verify that the Groq client works end-to-end\n",
    "#   - Send a simple question to the model\n",
    "#   - Receive and print the assistant's reply\n",
    "# ============================================================\n",
    "\n",
    "# 1Ô∏è‚É£ Build the messages list (conversation context)\n",
    "messages = [\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": \"You are a friendly Python tutor. Explain things in simple language.\"\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"Hello Groq! This is my first request. Please introduce yourself in 2‚Äì3 lines.\"\n",
    "    }\n",
    "]\n",
    "\n",
    "# 2Ô∏è‚É£ Send the chat completion request to Groq\n",
    "#    Using a supported model: llama-3.1-8b-instant\n",
    "response = client.chat.completions.create(\n",
    "    model=\"llama-3.1-8b-instant\",\n",
    "    messages=messages\n",
    ")\n",
    "\n",
    "# 3Ô∏è‚É£ Extract the assistant's reply\n",
    "#    IMPORTANT:\n",
    "#    - response.choices[0].message is an object (ChatCompletionMessage)\n",
    "#    - So we must use `.content` (attribute), NOT [\"content\"] (dict style)\n",
    "assistant_reply = response.choices[0].message.content\n",
    "\n",
    "# 4Ô∏è‚É£ Print reply\n",
    "print(\"ü§ñ Groq says:\\n\")\n",
    "print(assistant_reply)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd0afe25",
   "metadata": {},
   "source": [
    "#### **Inspect the Raw Response Object**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dd900199",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======= RAW RESPONSE OBJECT =======\n",
      "ChatCompletion(id='chatcmpl-bd8410e9-229d-40fe-821b-dc9e3969f217', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content=\"In the context of LLM (Large Language Model) APIs, a response object refers to the output or result generated by the language model in response to an input or prompt. This object typically contains the predicted response or output, as well as other metadata, such as:\\n\\n- The model's confidence score (or probability of the response being correct)\\n- The input that the model received\\n- Any relevant metadata, such as the model's ID or the session ID\\n\\nThe structure and content of a response object can vary depending on the specific LLM API being used, but it generally provides valuable information about the generated response.\", refusal=None, role='assistant', annotations=None, audio=None, function_call=None, tool_calls=None))], created=1766002215, model='llama-3.1-8b-instant', object='chat.completion', service_tier='on_demand', system_fingerprint='fp_1151d4f23c', usage=CompletionUsage(completion_tokens=125, prompt_tokens=61, total_tokens=186, completion_tokens_details=None, prompt_tokens_details=None, queue_time=0.052093615, prompt_time=0.003379825, completion_time=0.224800331, total_time=0.228180156), usage_breakdown=None, x_groq={'id': 'req_01kcpyyhtte0qrz97shftamam4', 'seed': 1628417720})\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# üìò SECTION 6 ‚Äî Inspecting the Raw Response Object\n",
    "# ------------------------------------------------------------\n",
    "# Why?\n",
    "#   - To see the full structure returned by the LLM.\n",
    "#   - This helps us understand:\n",
    "#       * where the model's reply lives\n",
    "#       * how choices[] is structured\n",
    "#       * how we might access metadata later (tokens, model, etc.)\n",
    "# ============================================================\n",
    "\n",
    "# üß© 1) Build a simple messages list for testing\n",
    "#    Why?\n",
    "#      - We send a short, clear question so the response object\n",
    "#        is easy to read and understand.\n",
    "\n",
    "messages = [\n",
    "    {\n",
    "        \"role\":\"system\",\n",
    "        \"content\":\"You are a helpful assistant who explains things simply:\" \n",
    "    },\n",
    "    {\n",
    "        \"role\":\"user\",\n",
    "        \"content\":\"What is a response object in the context of LLM APIs? Explain briefly.\"\n",
    "    }\n",
    "]\n",
    "\n",
    "# üß© 2) Send a chat completion request to Groq\n",
    "#    Why?\n",
    "#      - Same pattern as before:\n",
    "#          client.chat.completions.create(...)\n",
    "#      - We use the same model as in Section 5.\n",
    "response = client.chat.completions.create(\n",
    "    model=\"llama-3.1-8b-instant\",\n",
    "    messages=messages\n",
    ")\n",
    "\n",
    "# üß© 3) Print the entire response object\n",
    "#    Why?\n",
    "#      - For learning, we want to see everything Groq returns.\n",
    "#      - In real applications, we wouldn't print this every time.\n",
    "\n",
    "print(\"======= RAW RESPONSE OBJECT =======\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "260689b7",
   "metadata": {},
   "source": [
    "#### **Extract Key Fields from the Response Object**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "54a4ee68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===== üîé RESPONSE SUMMARY =====\n",
      "Model Used    :llama-3.1-8b-instant\n",
      "Assistant role :assistant\n",
      "Finish reason  :stop\n",
      "\n",
      "{'----- üß† Assistant Reply -----'}\n",
      "In the context of LLM (Large Language Model) APIs, a response object refers to the output or result generated by the language model in response to an input or prompt. This object typically contains the predicted response or output, as well as other metadata, such as:\n",
      "\n",
      "- The model's confidence score (or probability of the response being correct)\n",
      "- The input that the model received\n",
      "- Any relevant metadata, such as the model's ID or the session ID\n",
      "\n",
      "The structure and content of a response object can vary depending on the specific LLM API being used, but it generally provides valuable information about the generated response.\n",
      "\n",
      "----- üìä Token Usage -----\n",
      "Prompt tokens    :61\n",
      "completion_tokens   :125\n",
      "Total Tokens   :186\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# üìò SECTION 7 ‚Äî Extracting Important Fields from Response\n",
    "# ------------------------------------------------------------\n",
    "# Goal:\n",
    "#   - Understand how to read specific parts of the response:\n",
    "#       * Model name\n",
    "#       * Assistant role\n",
    "#       * Assistant message (reply)\n",
    "#       * Finish reason\n",
    "#       * Token usage (if available)\n",
    "#\n",
    "# Note:\n",
    "#   - This assumes 'response' already exists from SECTION 6.\n",
    "#   - If not, re-run SECTION 6 before running this cell.\n",
    "# ============================================================\n",
    "# üß© 1) Extract the model name\n",
    "#    Why?\n",
    "#      - Useful for logging, debugging, and knowing which LLM handled the request.\n",
    "\n",
    "model_name = response.model\n",
    "\n",
    "# üß© 2) Extract the first choice (index 0)\n",
    "#    Why?\n",
    "#      - Most calls only care about the first suggested answer.\n",
    "\n",
    "first_choice = response.choices[0]\n",
    "\n",
    "# üß© 3) Extract the assistant's message role and content\n",
    "#    Why?\n",
    "#      - role  ‚Üí usually \"assistant\"\n",
    "#      - content ‚Üí actual reply text from the model\n",
    "\n",
    "assistant_role = first_choice.message.role\n",
    "assistant_content = first_choice.message.content\n",
    "\n",
    "# üß© 4) Extract the finish reason\n",
    "#    Why?\n",
    "#      - Tells us WHY the model stopped generating:\n",
    "#          * \"stop\"        ‚Üí completed naturally\n",
    "#          * \"length\"      ‚Üí hit max_tokens limit\n",
    "#          * \"content_filter\" ‚Üí blocked by safety filter (in some providers)\n",
    "\n",
    "finish_reason = first_choice.finish_reason\n",
    "\n",
    "# üß© 5) Extract token usage (if available)\n",
    "#    Why?\n",
    "#      - Helps us understand cost and length of prompts/responses.\n",
    "#      - Some providers may not always return usage; we handle that safely.\n",
    "\n",
    "usage_info = getattr(response,\"usage\",None)\n",
    "\n",
    "if usage_info:\n",
    "    prompt_tokens = usage_info.prompt_tokens\n",
    "    completion_tokens = usage_info.completion_tokens\n",
    "    total_tokens = usage_info.total_tokens\n",
    "else:\n",
    "    prompt_tokens = completion_tokens = total_tokens = None\n",
    "\n",
    "# üß© 6) Print everything in a clean, readable way\n",
    "\n",
    "print(\"===== üîé RESPONSE SUMMARY =====\")\n",
    "print(f\"Model Used    :{model_name}\")\n",
    "print(f\"Assistant role :{assistant_role}\")\n",
    "print(f\"Finish reason  :{finish_reason}\")\n",
    "print()\n",
    "print({\"----- üß† Assistant Reply -----\"})\n",
    "print(assistant_content)\n",
    "print()\n",
    "\n",
    "if usage_info:\n",
    "    print(\"----- üìä Token Usage -----\")\n",
    "    print(f\"Prompt tokens    :{prompt_tokens}\")\n",
    "    print(f\"completion_tokens   :{completion_tokens}\")\n",
    "    print(f\"Total Tokens   :{total_tokens}\")\n",
    "else:\n",
    "    print(\"Token usage information not provided by this response.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "391a943f",
   "metadata": {},
   "source": [
    "#### **Why These Fields Matter in REAL GenAI Projects**\n",
    "\n",
    "(One step only. No code yet ‚Äî pure understanding.)\n",
    "\n",
    "Before building:\n",
    "- Chatbots\n",
    "- RAG systems\n",
    "- Agents\n",
    "- Evaluators\n",
    "- Streamlit apps\n",
    "- FastAPI endpoints\n",
    "- Workflow automation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc7dacb5",
   "metadata": {},
   "source": [
    "#### **Understanding Field Importance (Simple & Practical)**\n",
    "\n",
    "**Why Response Fields Matter in Real GenAI Projects**\n",
    "\n",
    "**1Ô∏è‚É£ model ‚Äî Which brain answered your question**\n",
    "\n",
    "- Helps track which model produced what output\n",
    "- Useful in logs & debugging\n",
    "- Important when switching models for performance or cost\n",
    "- In production, you often A/B test multiple models\n",
    "\n",
    "\n",
    "Example:\n",
    "\n",
    "- llama-3.1-8b-instant ‚Üí fast, cheap, good for simple tasks\n",
    "- llama-3.1-70b-versatile ‚Üí slower, expensive, high quality\n",
    "\n",
    "\n",
    "**2Ô∏è‚É£ choices[0].message.role ‚Äî Usually assistant**\n",
    "\n",
    "Why it matters:\n",
    "\n",
    "- Ensures you‚Äôre reading the right message\n",
    "- Maintains consistent chat structure\n",
    "- Needed for chat history formatting\n",
    "\n",
    "**3Ô∏è‚É£ choices[0].message.content ‚Äî The actual answer**\n",
    "\n",
    "This is the core output used in:\n",
    "- Chatbots\n",
    "- RAG responses\n",
    "- SQL generator bots\n",
    "- Code generators\n",
    "- Multimodal apps\n",
    "- Streamlit apps\n",
    "- FastAPI endpoints\n",
    "\n",
    "**4Ô∏è‚É£ finish_reason ‚Äî Why the model stopped writing**\n",
    "\n",
    "| finish_reason      | Meaning              |\n",
    "| ------------------ | -------------------- |\n",
    "| `\"stop\"`           | Completed normally   |\n",
    "| `\"length\"`         | Hit max_tokens limit |\n",
    "| `\"content_filter\"` | Safety block         |\n",
    "| `\"error\"`          | Model failure        |\n",
    "\n",
    "Why it matters:\n",
    "\n",
    "- If \"length\", you must increase max_tokens\n",
    "- If \"content_filter\", your input might be restricted\n",
    "- Used in production monitoring\n",
    "\n",
    "**5Ô∏è‚É£ usage ‚Äî Token cost + performance indicator**\n",
    "\n",
    "If available, contains:\n",
    "\n",
    "- prompt_tokens\n",
    "- completion_tokens\n",
    "- total_tokens\n",
    "\n",
    "Why it matters:\n",
    "- Cost = based on tokens\n",
    "- Performance tuning\n",
    "- Budget control in production\n",
    "- Monitoring usage per request, per user, per endpoint\n",
    "\n",
    "Even if Groq doesn‚Äôt always return usage, understanding it is essential for:\n",
    "- OpenAI\n",
    "- Anthropic\n",
    "- Gemini\n",
    "- Azure OpenAI\n",
    "\n",
    "\n",
    "6Ô∏è‚É£ Why this entire structure matters\n",
    "\n",
    "You‚Äôll use these fields in:\n",
    "\n",
    "‚úî RAG\n",
    "\n",
    "Monitor reason, track chunks, improve retrieval.\n",
    "\n",
    "‚úî Agents\n",
    "\n",
    "Determine when to stop, retry, or dispatch.\n",
    "\n",
    "‚úî Evaluations\n",
    "\n",
    "Compare output quality across models.\n",
    "\n",
    "‚úî Monitoring dashboards\n",
    "\n",
    "Track per-request cost, latency, and tokens.\n",
    "\n",
    "‚úî Debugging\n",
    "\n",
    "See why a model behaved unexpectedly.\n",
    "\n",
    "‚úî Production logs\n",
    "\n",
    "Every LLM call is logged with:\n",
    "- model\n",
    "- tokens\n",
    "- user prompt\n",
    "- output\n",
    "- finish_reason\n",
    "\n",
    "\n",
    "**üéØ Summary (Remember This!)**\n",
    "\n",
    "This response object is the foundation of everything in GenAI.\n",
    "\n",
    "If you understand this structure deeply:\n",
    "\n",
    "You can build ANY system:\n",
    "\n",
    "- chatbots\n",
    "- RAG\n",
    "- agents\n",
    "- multimodal apps\n",
    "- LLM APIs\n",
    "- batch processing\n",
    "- evaluation frameworks\n",
    "- enterprise AI systems"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ab7688f",
   "metadata": {},
   "source": [
    "#### **LLM API Concepts Explained (Human-Friendly, Deep, Practical)**\n",
    "\n",
    "**‚≠ê 1. client.chat.completions.create(...)**\n",
    "\n",
    "This is the heart of every LLM request.\n",
    "\n",
    "**‚úî What does it do?**\n",
    "\n",
    "- Sends your messages (conversation) to the LLM\n",
    "- Tells the model which brain (model) to use\n",
    "- Returns the model‚Äôs answer\n",
    "\n",
    "**‚úî When do we use it?**\n",
    "\n",
    "Always.\n",
    "Every chatbot, RAG system, agent, app, or API uses this function.\n",
    "\n",
    "**‚úî Why ‚Äúchat‚Äù?**\n",
    "\n",
    "Even if you send one message, the model still works in a chat format with roles."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60910eac",
   "metadata": {},
   "source": [
    "**‚≠ê 2. messages=[...]**\n",
    "\n",
    "**‚úî What does this list represent?**\n",
    "\n",
    "This is the conversation history.\n",
    "\n",
    "**Each item has:**\n",
    "\n",
    "{\"role\": \"system\" / \"user\" / \"assistant\", \"content\": \"...\"}\n",
    "\n",
    "**‚úî Why do we need roles?**\n",
    "\n",
    "- system ‚Üí controls personality & rules\n",
    "- user ‚Üí what you are asking\n",
    "- assistant ‚Üí previous model replies (for multi-turn chat)\n",
    "\n",
    "**‚úî Real scenarios:**\n",
    "\n",
    "- Chatbot\n",
    "- SQL Bot\n",
    "- Business assistant\n",
    "- RAG system with memory\n",
    "- Multi-agent workflows\n",
    "\n",
    "Messages = context.\n",
    "\n",
    "**‚≠ê 3. choices[0]**\n",
    "\n",
    "**‚úî Why ‚Äúchoices‚Äù?**\n",
    "\n",
    "LLMs can generate multiple outputs, like:\n",
    "\n",
    "- Choice 1\n",
    "- Choice 2\n",
    "- Choice 3\n",
    "\n",
    "But we usually want the first one. \n",
    "choices[0]\n",
    "‚ÄúGive me the first answer from the model.\n",
    "\n",
    "**‚úî Real scenario:**\n",
    "\n",
    "99% of industry apps use only choices[0]."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ac0a55b",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b01ecab0",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d3b5e9ea",
   "metadata": {},
   "source": [
    "#### **User Prompt: Clarity, Length & Style Control**\n",
    "\n",
    "\n",
    "How you write the user prompt changes the output quality by 70‚Äì80%.\n",
    "\n",
    "- Why vague prompts fail\n",
    "- Why specific prompts win\n",
    "- How length and detail affect reasoning\n",
    "- How structure affects reliability\n",
    "- How to write prompts like a Google/Microsoft engineer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2cf20038",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===== ‚ùå Version A ‚Äî Vague Prompt =====\n",
      "**Python Variables**\n",
      "\n",
      "### What is a Variable?\n",
      "\n",
      "In Python, a variable is a name given to a location in memory where a value can be stored. Variables are used to store and manipulate data throughout a program.\n",
      "\n",
      "### Simple Example\n",
      "\n",
      "Here's a simple example:\n",
      "```python\n",
      "name = \"John\"\n",
      "age = 30\n",
      "\n",
      "print(name)\n",
      "print(age)\n",
      "```\n",
      "In this example, \"name\" and \"age\" are variables, and \"John\" and \"30\" are their respective values.\n",
      "\n",
      "### Real World Analogy\n",
      "\n",
      "Think of a variable like a labeled box. When you store an object in the box (assign a value to a variable), you can later retrieve it using its label (the variable name). For example:\n",
      "```\n",
      "Box 1: \"Keys\"\n",
      "Box 2: \"Wallet\"\n",
      "\n",
      "# Retrieve the object from the box\n",
      "print(Box 1)  # Keys\n",
      "print(Box 2)  # Wallet\n",
      "```\n",
      "In Python, you wouldn't physically have labeled boxes, but the concept remains the same.\n",
      "\n",
      "### Common Mistake Beginners Make\n",
      "\n",
      "One common mistake beginners make is not understanding the difference between a variable and its value. For example:\n",
      "```python\n",
      "x = 5\n",
      "y = x\n",
      "\n",
      "print(x)  # 5\n",
      "print(y)  # 5\n",
      "```\n",
      "In this example, `x` and `y` are variables that hold the value `5`. However, beginners might be confused and think that `x` is a value assigned to `y`, rather than a separate variable that happens to have the same value.\n",
      "\n",
      "### Interview-Style Point 1\n",
      "\n",
      "**What is the difference between a mutable and immutable variable in Python?**\n",
      "\n",
      "Answer: In Python, variables can hold either mutable or immutable values. Mutable variables are objects that can be modified after they're created, such as lists (`[]`) or dictionaries (`{}`). Immutable variables are objects that cannot be modified, such as integers (`int`) or strings (`str`).\n",
      "\n",
      "Example:\n",
      "```python\n",
      "x = [1, 2, 3]  # mutable\n",
      "y = 5  # immutable\n",
      "\n",
      "x.append(4)  # modify the list\n",
      "y = 6  # reassign the value, can't modify the original integer\n",
      "```\n",
      "### Interview-Style Point 2\n",
      "\n",
      "**How do you use the `del` statement in Python?**\n",
      "\n",
      "Answer: The `del` statement is used to delete a variable or an item from a collection. For example:\n",
      "```python\n",
      "x = 5\n",
      "del x  # delete the variable\n",
      "print(x)  # NameError\n",
      "\n",
      "my_list = [1, 2, 3]\n",
      "del my_list[0]  # delete the first item\n",
      "print(my_list)  # [2, 3]\n",
      "```\n",
      "### Interview-Style Point 3\n",
      "\n",
      "**What is the concept of variable scope in Python?**\n",
      "\n",
      "Answer: Variable scope refers to the region of the code where a variable is defined and accessible. In Python, there are global and local scopes, as well as the concept of nonlocal scope. For example:\n",
      "```python\n",
      "def outer():\n",
      "    x = 5  # global scope\n",
      "\n",
      "    def inner():\n",
      "        x = 10  # local scope\n",
      "        print(\"Inner scope:\", x)\n",
      "\n",
      "    inner()\n",
      "    print(\"Outer scope:\", x)\n",
      "\n",
      "outer()\n",
      "```\n",
      "In this example, the variable `x` has two different values in the global and local scopes.\n",
      "\n",
      "\n",
      "==================================================================== ‚ö†Ô∏è Version B ‚Äî Better Prompt =====\n",
      "**What are Variables in Python?**\n",
      "\n",
      "In Python, a variable is a name given to a piece of data that can be used in a program. Variables are used to store, manipulate and retrieve data. They help to make the code more readable, maintainable and scalable.\n",
      "\n",
      "**Declaring Variables**\n",
      "\n",
      "In Python, you can declare a variable by simply assigning a value to a name. You don't need to declare the type of the variable beforehand.\n",
      "\n",
      "**Example**\n",
      "\n",
      "Let's create a variable `name` and assign it the value `\"John\"`:\n",
      "\n",
      "```python\n",
      "# Assignment statement\n",
      "name = \"John\"\n",
      "```\n",
      "\n",
      "In this example:\n",
      "\n",
      "- `name` is a variable\n",
      "- `\"John\"` is a string value assigned to the variable\n",
      "- The variable `name` can now be used in the program to access the value `\"John\"`\n",
      "\n",
      "**Accessing a Variable's Value**\n",
      "\n",
      "You can access the value of a variable by simply using its name:\n",
      "\n",
      "```python\n",
      "print(name)  # Output: John\n",
      "```\n",
      "\n",
      "This code prints the value of the variable `name`, which is `\"John\"`.\n",
      "\n",
      "\n",
      "===================================================================== ‚úÖ Version C ‚Äî Best Prompt =====\n",
      "**What are Python Variables?**\n",
      "\n",
      "In Python, a variable is a name given to a value. It allows you to store and manipulate data in your program. Think of a variable like a labeled box where you can put a value, and then refer to that value by its label (the variable name).\n",
      "\n",
      "**Simple Example:**\n",
      "```python\n",
      "x = 5  # x is a variable with the value 5\n",
      "print(x)  # outputs: 5\n",
      "```\n",
      "In this example, we created a variable `x` and assigned it the value 5. Now, we can use the variable `x` to refer to the value 5.\n",
      "\n",
      "**Real-World Analogy:**\n",
      "\n",
      "Imagine you're organizing your desk and you have a bunch of papers with different notes. To make it easier to find a specific note, you create labels (like \"To-Do List\", \"Address Book\", etc.) and put the corresponding paper in the labeled box. In Python, the label is like the variable name, and the paper with notes is like the value stored in the variable.\n",
      "\n",
      "**Common Mistake Beginners Make:**\n",
      "\n",
      "One common mistake beginners make is not understanding that variables hold references to values, not the values themselves. This means that if you assign a new value to a variable, it doesn't change the original value; it creates a new reference to the new value. For example:\n",
      "```python\n",
      "my_list = [1, 2, 3]\n",
      "my_list[0] = 10  # changes the first element of the list\n",
      "print(my_list)  # outputs: [10, 2, 3]\n",
      "my_list = [4, 5, 6]  # creates a new reference to a new list\n",
      "print(my_list)  # outputs: [4, 5, 6]\n",
      "print(id(my_list))  # outputs: a new memory address\n",
      "```\n",
      "In this example, `my_list` is a variable that holds a reference to a list. When we modify the list, it's modified in place, but when we assign a new list to `my_list`, it creates a new reference to a new list.\n",
      "\n",
      "**3 Interview-Style Points:**\n",
      "\n",
      "1. **What is the difference between an object and a variable in Python?**\n",
      "\n",
      "A variable is a name given to a value, while an object is an instance of a class that has its own set of attributes (data) and methods (functions). A variable can hold a reference to an object.\n",
      "\n",
      "2. **Can you explain what mutable and immutable objects are in Python?**\n",
      "\n",
      "Mutable objects are objects that can be modified after they're created, like lists, dictionaries, and sets. Immutable objects, on the other hand, cannot be modified after they're created, like integers, floats, and strings.\n",
      "\n",
      "3. **What is the concept of scope in Python, and how does it affect variable accessibility?**\n",
      "\n",
      "In Python, scope refers to the region of the code where a variable is defined and accessible. There are four main scopes: global, local, enclosed, and built-in. A variable's accessibility depends on its scope. If a variable is defined in a function, it's only accessible within that function, unless it's explicitly declared as global.\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# üìò SECTION 9.2 ‚Äî User Prompt Quality: Clarity, Length & Style\n",
    "# ------------------------------------------------------------\n",
    "# Goal:\n",
    "#   - Understand how different user prompt styles affect output.\n",
    "#   - Learn how clarity, detail, and structure change the response.\n",
    "#\n",
    "# Real-world relevance:\n",
    "#   - Client queries\n",
    "#   - Business requirements\n",
    "#   - Analytics agents\n",
    "#   - Chatbots & assistants\n",
    "#   - SQL generators\n",
    "#   - Coding copilots\n",
    "#   - RAG systems\n",
    "# ============================================================\n",
    "\n",
    "\n",
    "# 1Ô∏è‚É£ Three types of user prompts to compare\n",
    "\n",
    "# ‚ùå Version A ‚Äî Vague, unclear\n",
    "user_prompt_vague = \"Explain Python Variable.\"\n",
    "\n",
    "# ‚ö†Ô∏è Version B ‚Äî Better, more clear\n",
    "user_prompt_medium = 'Explain Python variable with one simple example.'\n",
    "\n",
    "# ‚úÖ Version C ‚Äî Best (Google-level), structured, clear\n",
    "user_prompt_best = \"\"\"Explain Python Variable with:\n",
    "- a simple example\n",
    "- a real world analogy\n",
    "- common mistake beginners make\n",
    "- 3 interview-style point\n",
    "\"\"\"\n",
    "\n",
    "# 2Ô∏è‚É£ Build three message sets (system prompt stays the same)\n",
    "messages_vague = [\n",
    "    {\"role\":\"system\",\n",
    "    \"content\":\"You are an expert Python tutor.\"},\n",
    "    {\"role\":\"user\",\"content\": user_prompt_best}\n",
    "]\n",
    "\n",
    "messages_medium = [\n",
    "    {\"role\":\"system\",\"content\":\"You are an expert Python tutor.\"},\n",
    "    {\"role\":\"user\",\"content\":user_prompt_medium}\n",
    "]\n",
    "\n",
    "messages_best = [\n",
    "    {\"role\":\"system\",\"content\":\"You are an expert Python tutor.\"},\n",
    "    {\"role\":\"user\",\"content\":user_prompt_best}\n",
    "\n",
    "]\n",
    "\n",
    "# 3Ô∏è‚É£ Call Groq for each prompt version\n",
    "\n",
    "resp_vague = client.chat.completions.create(\n",
    "    model=\"llama-3.1-8b-instant\",\n",
    "    messages=messages_vague\n",
    ")\n",
    "out_vague = resp_vague.choices[0].message.content\n",
    "\n",
    "resp_medium = client.chat.completions.create(\n",
    "    model=\"llama-3.1-8b-instant\",\n",
    "    messages=messages_medium\n",
    ")\n",
    "out_medium = resp_medium.choices[0].message.content\n",
    "\n",
    "resp_best = client.chat.completions.create(\n",
    "    model=\"llama-3.1-8b-instant\",\n",
    "    messages=messages_best\n",
    ")\n",
    "\n",
    "out_best = resp_best.choices[0].message.content\n",
    "\n",
    "# 4Ô∏è‚É£ Print results for comparison\n",
    "print(\"===== ‚ùå Version A ‚Äî Vague Prompt =====\")\n",
    "print(out_vague)\n",
    "print(\"\\n\")\n",
    "\n",
    "print(\"==================================================================== ‚ö†Ô∏è Version B ‚Äî Better Prompt =====\")\n",
    "print(out_medium)\n",
    "print(\"\\n\")\n",
    "\n",
    "print(\"===================================================================== ‚úÖ Version C ‚Äî Best Prompt =====\")\n",
    "print(out_best)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35940bfa",
   "metadata": {},
   "source": [
    "#### **Temperature, Top_p, Max Tokens (LLM Behavior Controls)**\n",
    "\n",
    "Every LLM engineer at Google, Microsoft, OpenAI must master these 3 parameters because they control:\n",
    "\n",
    "- Creativity\n",
    "- Determinism\n",
    "- Output length\n",
    "- Safety\n",
    "- Reliability\n",
    "- Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "660caa0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================üßä Temperature 0.0 ‚Äî Deterministic =====\n",
      "As the stars whizzed by like diamonds on velvet, Zeta-5, a fearless robot with a heart of circuitry, boldly ventured into the unknown expanse of the Andromeda galaxy.\n",
      "\n",
      "\n",
      "==================================üî• Temperature 1.5 ‚Äî Creative & Random =====\n",
      "As the last rays of Earth's sunset faded from view, the star-gazing robotic explorer, Aurora, pierced the inky blackness of outer space with her shining aluminum heart, charting a courageous course toward the secrets of the Andromeda galaxy\n",
      "\n",
      "\n",
      "===========================================üéØ top_p = 0.3 ‚Äî Restrictive Creativity =====\n",
      "As the stars whizzed by like diamonds on velvet, robot explorer Zeta-5 pierced the unknown expanse of the cosmos, its gleaming metal heart beating with an insatiable thirst for discovery.\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# üìò SECTION 9.3 ‚Äî LLM Behavior Controls:\n",
    "#     temperature, top_p, max_tokens\n",
    "# ------------------------------------------------------------\n",
    "# Why this matters?\n",
    "#   These 3 parameters allow us to control HOW the model behaves.\n",
    "#\n",
    "#   temperature ‚Üí creativity vs stability\n",
    "#   top_p       ‚Üí nucleus sampling (controls randomness range)\n",
    "#   max_tokens  ‚Üí how much the model is allowed to speak\n",
    "#\n",
    "# Real-world impact:\n",
    "#   - chatbots (stable responses)\n",
    "#   - code generation (deterministic answers)\n",
    "#   - story writing (high creativity)\n",
    "#   - RAG systems (must stay factual)\n",
    "#   - SQL bots (must be deterministic, low temperature)\n",
    "# ============================================================\n",
    "\n",
    "# 1Ô∏è‚É£ Let's prepare one simple prompt\n",
    "\n",
    "messages = [\n",
    "    {\"role\":\"system\",\"content\":\"You are a creative storyteller.\"},\n",
    "    {\"role\":\"user\",\"content\":\"Write one line about a brave robot exploring space.\"}\n",
    "]\n",
    "\n",
    "# 2Ô∏è‚É£ Low Temperature (0.0) ‚Üí deterministic / predictable\n",
    "response_low_temp = client.chat.completions.create(\n",
    "    model=\"llama-3.1-8b-instant\",\n",
    "    temperature=0.0,\n",
    "    top_p=1.0,\n",
    "    max_tokens=50,\n",
    "    messages=messages\n",
    ")\n",
    "out_low_temp = response_low_temp.choices[0].message.content\n",
    "\n",
    "# 3Ô∏è‚É£ High Temperature (1.5) ‚Üí creative / random / surprising\n",
    "response_high_temp = client.chat.completions.create(\n",
    "    model=\"llama-3.1-8b-instant\",\n",
    "    temperature=1.5,\n",
    "    top_p=1.0,\n",
    "    max_tokens=50,\n",
    "    messages=messages\n",
    ")\n",
    "out_high_temp = response_high_temp.choices[0].message.content\n",
    "\n",
    "# 4Ô∏è‚É£ Top_p control (restrict randomness window)\n",
    "response_top_p_low = client.chat.completions.create(\n",
    "    model=\"llama-3.1-8b-instant\",\n",
    "    temperature=0.8,\n",
    "    top_p=0.3,\n",
    "    max_tokens=50,\n",
    "   messages=messages\n",
    ")\n",
    "out_top_p_low = response_top_p_low.choices[0].message.content\n",
    "\n",
    "# 5Ô∏è‚É£ Print results side by side\n",
    "print(\"====================üßä Temperature 0.0 ‚Äî Deterministic =====\")\n",
    "print(out_low_temp)\n",
    "print('\\n')\n",
    "\n",
    "print(\"==================================üî• Temperature 1.5 ‚Äî Creative & Random =====\")\n",
    "print(out_high_temp)\n",
    "print(\"\\n\")\n",
    "\n",
    "print(\"===========================================üéØ top_p = 0.3 ‚Äî Restrictive Creativity =====\")\n",
    "print(out_top_p_low)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d702ce99",
   "metadata": {},
   "source": [
    "#### **Stop Sequences (Prevent Unwanted Output)**\n",
    "\n",
    "Stop sequences are EXTREMELY important in:\n",
    "\n",
    "- Chatbots : \n",
    "Stop sequences are critical in chatbot systems because they define where the chatbot's output should stop. Without a stop sequence, the chatbot could continue generating text indefinitely or produce responses that aren't clean or useful. For example, the bot might keep generating irrelevant or redundant responses.\n",
    "\n",
    "- Agents : \n",
    "Similar to chatbots, agents (which might be virtual assistants or automated systems) need stop sequences to prevent runaway or endless output. It ensures that the agent stops once the relevant task or response is completed.\n",
    "\n",
    "- Function calling : \n",
    "In function-based programming or API calls, stop sequences can define where the output or result of a function should be terminated. This helps ensure that the function doesn‚Äôt accidentally return too much or too little data.\n",
    "\n",
    "- Tools : \n",
    "If an AI system interacts with tools (like executing code, querying databases, etc.), stop sequences can be used to limit the response or actions to only what's needed. This can also apply to systems like text editors, where you want to limit the response size or structure.\n",
    "\n",
    "- RAG systems : \n",
    "RAG systems pull in external information to generate responses (often combining a search engine and a generative model). In these systems, a stop sequence is used to cut off the generated text at a logical point, ensuring that the AI doesn't just ramble or include irrelevant information from its knowledge base.\n",
    "\n",
    "- Structured JSON output : \n",
    "When dealing with structured data formats like JSON, stop sequences help ensure the output is clean and properly formatted. Without a stop sequence, the generated JSON could become malformed or continue indefinitely.\n",
    "\n",
    "- Limiting hallucinations : \n",
    "Hallucinations in AI refer to when the model generates incorrect or nonsensical information. Stop sequences can be used as a tool to limit this behavior by halting the output once a coherent answer is generated, preventing the AI from continuing and possibly inventing information\n",
    "\n",
    "- Preventing ‚Äúextra text‚Äù : \n",
    "Sometimes models generate extra text, filler, or tangents that don‚Äôt serve the purpose. Stop sequences are used to halt the model once the response is complete, cutting off unnecessary or irrelevant additions\n",
    "\n",
    "- Controlling formatting : \n",
    "In cases where a specific format is required (e.g., code snippets, structured responses), stop sequences can help ensure the model stops at the correct point to match the desired formatting and avoid messy output\n",
    "\n",
    "- API integration : \n",
    "In API-based systems, stop sequences can be used to control how much data is returned, how the data is formatted, and how the system behaves when interacting with APIs. For instance, you can use stop sequences to ensure the API responses are concise and properly structured, improving performance and readability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d3e3ef2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===== ‚ùå Without Stop Sequence =====\n",
      "{\"name\": \"Dhiru\", \"age\": 36}\n",
      "\n",
      "\n",
      "===== ‚úÖ With Stop Sequence =====\n",
      "{\"name\": \"Dhiru\", \"age\": 36}\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# üìò SECTION 9.4 ‚Äî Stop Sequences (Prevent Unwanted Output)\n",
    "# ------------------------------------------------------------\n",
    "# Why this matters?\n",
    "#   - LLMs sometimes continue speaking beyond what we want.\n",
    "#   - Stop sequences tell the model:\n",
    "#         \"STOP generating when you see this pattern.\"\n",
    "#\n",
    "# Real-world uses:\n",
    "#   - Prevent extra sentences after JSON output\n",
    "#   - Stop the model before adding explanations\n",
    "#   - Control agent/tool responses\n",
    "#   - Enforce strict formatting\n",
    "#   - Avoid hallucinated closing remarks\n",
    "# ============================================================\n",
    "\n",
    "# 1Ô∏è‚É£ Build a prompt where model tends to continue speaking\n",
    "messages = [\n",
    "    {\"role\": \"system\",\n",
    "    \"content\": \"Your Output ONLY the JSON asked for. NOTHING else.\"},\n",
    "    {\"role\": \"user\",\n",
    "    \"content\": \"Give me a JSON with name= 'Dhiru' and age=36\"}\n",
    "]\n",
    "\n",
    "# ‚ùå Without stop sequences ‚Üí model may add:\n",
    "#    - \"Here is the JSON:\"\n",
    "#    - backticks\n",
    "#    - explanations\n",
    "#    - extra comments\n",
    "\n",
    "response_no_stop = client.chat.completions.create(\n",
    "    model=\"llama-3.1-8b-instant\",\n",
    "    messages=messages,\n",
    "    max_tokens=50\n",
    ")\n",
    "\n",
    "out_no_stop = response_no_stop.choices[0].message.content\n",
    "\n",
    "# 2Ô∏è‚É£ Now apply STOP SEQUENCES\n",
    "#    Tell model:\n",
    "#       - stop when you see a newline\n",
    "#       - stop when you see trailing text like \"</end>\"\n",
    "#\n",
    "# Common patterns used in industry:\n",
    "#       stop=[\"```\", \"\\n\\n\", \"</end>\"]\n",
    "\n",
    "response_with_stop = client.chat.completions.create(\n",
    "    model=\"llama-3.1-8b-instant\",\n",
    "    messages=messages,\n",
    "    max_tokens = 50,\n",
    "    stop=[\"\\n\"]  # stop generation at first newline\n",
    ")\n",
    "\n",
    "out_with_stop = response_with_stop.choices[0].message.content\n",
    "\n",
    "# 3Ô∏è‚É£ Print results\n",
    "print(\"===== ‚ùå Without Stop Sequence =====\")\n",
    "print(out_no_stop)\n",
    "print(\"\\n\")\n",
    "\n",
    "print(\"===== ‚úÖ With Stop Sequence =====\")\n",
    "print(out_with_stop)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41b93834",
   "metadata": {},
   "source": [
    "#### **Structured Output & JSON Mode (Production-Grade Output Control)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c90edc6a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1bf87b49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===== üü° Without Strict JSON Mode =====\n",
      "```json\n",
      "{\n",
      "  \"name\": \"Dhiru\",\n",
      "  \"experience\": \"GenAI Learner\",\n",
      "  \"level\": \"Beginner to pro Journey\"\n",
      "}\n",
      "```\n",
      "\n",
      "Here's a brief explanation of each field:\n",
      "\n",
      "1. **Name**: 'Dhiru' is the personal name of the individual, which serves as their identifier.\n",
      "2. **Experience**: 'GenAI Learner' describes the person's level of experience and knowledge in the field of Generative Artificial Intelligence (GenAI).\n",
      "3. **Level**: 'Beginner to pro Journey' signifies that the individual is on a learning path that covers a broad scope, ranging from foundational knowledge to advanced expertise in GenAI.\n",
      "\n",
      "\n",
      "\n",
      "===== üü¢ With Strict JSON Template =====\n",
      "{\n",
      "  \"name\": \"Dhiru\",\n",
      "  \"experience\": \"GenAI Learner\",\n",
      "  \"level\": \"Beginner to Pro Journey\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# üìò SECTION 9.5 ‚Äî Structured Output & JSON Mode\n",
    "# ------------------------------------------------------------\n",
    "# Why this matters?\n",
    "#   - LLMs love adding explanations, backticks, and commentary.\n",
    "#   - But production systems require STRICT, machine-readable JSON.\n",
    "#   - APIs, agents, RAG engines, and data pipelines break if output\n",
    "#     is not exactly structured.\n",
    "#\n",
    "# Goal:\n",
    "#   - Compare loose JSON vs strict JSON template + stop sequences.\n",
    "# ============================================================\n",
    "\n",
    "# 1Ô∏è‚É£ Prompt for JSON output (model may add extra text)\n",
    "\n",
    "messages_loose = [\n",
    "{    \"role\":\"system\",\n",
    "    \"content\":\"You are an assistant. Respond to the user request.\"},\n",
    "{    \"role\":\"user\",\n",
    "    \"content\":(\n",
    "        \"Return a json object with fields:\"\n",
    "        \"name= 'Dhiru',experience='GenAI Learner', level='Beginner to pro Journey'.\"\n",
    "        \"After the json, explain each field in one sentence.\")}    \n",
    "]\n",
    "\n",
    "# üü° 2Ô∏è‚É£ Call WITHOUT strict control ‚Äî model may add explanations\n",
    "response_loose = client.chat.completions.create(\n",
    "    model=\"llama-3.1-8b-instant\",\n",
    "    messages=messages_loose,\n",
    "    max_tokens=200\n",
    ")\n",
    "\n",
    "out_loose = response_loose.choices[0].message.content\n",
    "\n",
    "# üü¢ 3Ô∏è‚É£ STRICT JSON ‚Äî provide exact template + stop sequence\n",
    "\n",
    "message_strict = [\n",
    "{    \"role\":\"system\",\n",
    "    \"content\":(\n",
    "        \"You MUST output ONLY valid JSON. No explanation, no commentary,\"\n",
    "        \"no extra text. Follow EXACT format:\\n\\n\"\n",
    "        \"{\\n\"\n",
    "         \"  \\\"name\\\": \\\"...\\\",\\n\"\n",
    "         \"  \\\"experience\\\": \\\"...\\\",\\n\"\n",
    "         \"  \\\"level\\\": \\\"...\\\"\\n\"\n",
    "         \"}\\n\\n\"\n",
    "         \"Do not add anything else beyond this JSON structure.\")},\n",
    "    \n",
    "    {\"role\":\"user\",\n",
    "    \"content\":\"Fill the JSON fields for name='Dhiru', experience='GenAI Learner', level='Beginner to Pro Journey'.\"}\n",
    "]\n",
    "\n",
    "response_strict = client.chat.completions.create(\n",
    "    model=\"llama-3.1-8b-instant\",\n",
    "    messages=message_strict,\n",
    "    max_tokens=200,\n",
    "    stop=[\"\\n\\n\"] # stop before any unwanted explanation begins\n",
    ")\n",
    "\n",
    "out_strict = response_strict.choices[0].message.content\n",
    "\n",
    "\n",
    "# üß™ 4Ô∏è‚É£ Print results\n",
    "print(\"===== üü° Without Strict JSON Mode =====\")\n",
    "print(out_loose)\n",
    "print(\"\\n\\n\")\n",
    "\n",
    "print(\"===== üü¢ With Strict JSON Template =====\")\n",
    "print(out_strict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4ce5dac",
   "metadata": {},
   "source": [
    "#### **Prompt Chaining & Step-by-Step Reasoning**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c56f2ba1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===== ‚ùå Direct Prompt (No Structured Reasoning) =====\n",
      "**Recursion: A Fundamentals Explanation**\n",
      "\n",
      "Recursion is a fundamental concept in computer science where a function calls itself to solve a problem. This approach is particularly useful for problems that can be broken down into smaller sub-problems, and solving these sub-problems will eventually lead to the solution of the original problem.\n",
      "\n",
      "**How Recursion Works:**\n",
      "\n",
      "1.  **Base Case**: A recursive function must have a base case, which is a condition that stops the recursion. This ensures that the recursion will eventually terminate.\n",
      "2.  **Recursive Case**: If the base case is not met, the function calls itself with a modified input or argument. This creates a new instance of the function, which will eventually lead to the base case being met.\n",
      "3.  **Return Statement**: The final result of the recursion is returned by the function.\n",
      "\n",
      "**Example: Factorial Calculation Using Recursion**\n",
      "\n",
      "The factorial of a number `n` (denoted as `n!`) is the product of all positive integers less than or equal to `n`. The factorial of 5, for instance, would be:\n",
      "\n",
      "`5! = 5 * 4 * 3 * 2 * 1 = 120`\n",
      "\n",
      "Here's a recursive function in Python to calculate the factorial:\n",
      "\n",
      "```python\n",
      "def factorial(n):\n",
      "    \"\"\"\n",
      "    Calculate the factorial of a number n using recursion.\n",
      "    \n",
      "    Args:\n",
      "        n (int): A positive integer.\n",
      "    \n",
      "    Returns:\n",
      "        int: The factorial of n.\n",
      "    \"\"\"\n",
      "    # Base case: If n is 0 or 1, return 1 (since 0! and 1! are both 1)\n",
      "    if n == 0 or n == 1:\n",
      "        return 1\n",
      "    \n",
      "    # Recursive case: If n is greater than 1, call factorial with n-1 and multiply the result by n\n",
      "    else:\n",
      "        return n * factorial(n-1)\n",
      "\n",
      "# Example usage:\n",
      "print(factorial(5))  # Output: 120\n",
      "```\n",
      "\n",
      "In this example:\n",
      "\n",
      "1.  The base case is met when `n` is 0 or 1, so the function returns 1.\n",
      "2.  In the recursive case, the function calls itself with `n-1` and multiplies the result by `n`.\n",
      "3.  This recursive process eventually leads to the base case being met, resulting in the factorial of `n`.\n",
      "\n",
      "Recursion might seem confusing at first, but with practice, you'll become proficient in applying this technique to solve various problems in computer science.\n",
      "\n",
      "\n",
      "\n",
      "=========================== ‚úÖ Prompt Chaining (Structured Reasoning) =====\n",
      "**Understanding Recursion**\n",
      "==========================\n",
      "\n",
      "**STEP 1: Understand the question.**\n",
      "Recursion is a programming technique where a function calls itself as a subroutine. It's a method of solving problems that are broken down into smaller problems of the same type.\n",
      "\n",
      "**STEP 2: Break the concept into simple parts.**\n",
      "To break down recursion, let's consider the following:\n",
      "\n",
      "* **Base case**: A condition that stops the recursion process. This is usually the simplest case that we can return a result from.\n",
      "* **Recursive call**: The function calls itself with a smaller or more limited version of the original problem.\n",
      "* **Recursion stack**: Each time the function calls itself, a new entry is added to the call stack. This helps the function keep track of the previous instances.\n",
      "\n",
      "**STEP 3: Provide a real example.**\n",
      "Let's consider a simple example with a recursive function to calculate the factorial of a number:\n",
      "\n",
      "```python\n",
      "def factorial(n):\n",
      "    # Base case: if n is 0 or 1, return 1\n",
      "    if n == 0 or n == 1:\n",
      "        return 1\n",
      "    \n",
      "    # Recursive case: n! = n * (n-1)!\n",
      "    else:\n",
      "        return n * factorial(n-1)\n",
      "```\n",
      "\n",
      "Here's an example of how the recursion calls work for `factorial(4)`:\n",
      "\n",
      "1. `factorial(4)` is called\n",
      "2. `factorial(4)` calls `factorial(3)`\n",
      "3. `factorial(3)` calls `factorial(2)`\n",
      "4. `factorial(2)` calls `factorial(1)`\n",
      "5. `factorial(1)` returns 1 (base case)\n",
      "6. `factorial(2)` returns 2 * 1 = 2\n",
      "7. `factorial(3)` returns 3 * 2 = 6\n",
      "8. `factorial(4)` returns 4 * 6 = 24\n",
      "\n",
      "**STEP 4: Highlight mistakes beginners make.**\n",
      "Common mistakes when using recursion include:\n",
      "\n",
      "* **Not identifying a base case**: This can cause the function to call itself indefinitely, leading to a \"RecursionError\".\n",
      "* **Not using the right recursive case**: Make sure the recursive function is solving a smaller or simpler version of the original problem.\n",
      "* **Not handling edge cases**: Recursive functions may not handle cases outside their domain (e.g., a factorial function may not handle negative numbers or non-integer inputs).\n",
      "\n",
      "By following these guidelines, you can write effective recursive functions that solve complex problems by breaking them down into smaller, more manageable parts.\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# üìò SECTION 9.6 ‚Äî Prompt Chaining & Step-by-Step Reasoning\n",
    "# ------------------------------------------------------------\n",
    "# Why this matters?\n",
    "#   - Large questions overwhelm LLMs.\n",
    "#   - Breaking a problem into smaller steps improves:\n",
    "#       * accuracy\n",
    "#       * reasoning\n",
    "#       * reliability\n",
    "#       * factual correctness\n",
    "#\n",
    "# This is EXACTLY how Google/Microsoft build reasoning agents.\n",
    "#\n",
    "# We will demonstrate:\n",
    "#   1) Direct prompting (bad accuracy)\n",
    "#   2) Step-by-step chain (much better)\n",
    "# ============================================================\n",
    "\n",
    "messages_direct = [\n",
    "    {\"role\":\"system\",\"content\":\"You are Python expert.\"},\n",
    "    {\"role\":\"user\",\"content\":\"Explain how recursion works with an example.\"}\n",
    "]\n",
    "\n",
    "response_direct = client.chat.completions.create(\n",
    "    model=\"llama-3.1-8b-instant\",\n",
    "    messages=messages_direct\n",
    ")\n",
    "\n",
    "out_direct =response_direct.choices[0].message.content\n",
    "\n",
    "# üß© 2Ô∏è‚É£ Chained reasoning ‚Äî Force model to think step-by-step\n",
    "\n",
    "messages_chain = [\n",
    "    {\"role\": \"system\",\n",
    "     \"content\": (\n",
    "         \"You are a Python expert. Always think in steps.\\n\"\n",
    "         \"Follow this pattern:\\n\"\n",
    "         \"STEP 1: Understand the question.\\n\"\n",
    "         \"STEP 2: Break the concept into simple parts.\\n\"\n",
    "         \"STEP 3: Provide a real example.\\n\"\n",
    "         \"STEP 4: Highlight mistakes beginners make.\\n\"\n",
    "     )},\n",
    "    {\"role\": \"user\",\n",
    "     \"content\": \"Explain how recursion works with an example.\"}\n",
    "]\n",
    "\n",
    "response_chain = client.chat.completions.create(\n",
    "    model=\"llama-3.1-8b-instant\",\n",
    "    messages=messages_chain\n",
    ")\n",
    "\n",
    "out_chain = response_chain.choices[0].message.content\n",
    "\n",
    "\n",
    "\n",
    "# üß™ 3Ô∏è‚É£ Print both outputs\n",
    "print(\"===== ‚ùå Direct Prompt (No Structured Reasoning) =====\")\n",
    "print(out_direct)\n",
    "print(\"\\n\\n\")\n",
    "\n",
    "print(\"=========================== ‚úÖ Prompt Chaining (Structured Reasoning) =====\")\n",
    "print(out_chain)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "568d4f34",
   "metadata": {},
   "source": [
    "#### **Few-Shot Prompting (Teaching the Model with Examples)**\n",
    "\n",
    "**Few-shot prompting is used in all advanced AI systems:**\n",
    "\n",
    "- SQL generators\n",
    "- Code assistants\n",
    "- Agents\n",
    "- Classification models\n",
    "- Extraction tasks\n",
    "- Multi-turn chatbots\n",
    "- RAG reasoning\n",
    "- Enterprise AI platforms\n",
    "- Prompt tuning models\n",
    "\n",
    "**When you give the model examples, it:**\n",
    "\n",
    "- understands patterns\n",
    "- copies structure\n",
    "- increases accuracy\n",
    "- reduces hallucination\n",
    "- becomes consistent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1f920c2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===== üéØ FEW-SHOT OUTPUT =====\n",
      "{\"name\": \"Dhiru\", \"age\": 36}\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# üìò SECTION 9.7 ‚Äî Few-Shot Prompting (Teach the Model by Example)\n",
    "# ------------------------------------------------------------\n",
    "# Why this matters?\n",
    "#   - LLMs learn patterns extremely well.\n",
    "#   - By giving 1‚Äì2 examples (\"shots\"), we teach the model the\n",
    "#     EXACT format, tone, and structure we want.\n",
    "#\n",
    "# Real-world usage:\n",
    "#   - SQL generation\n",
    "#   - Classification\n",
    "#   - Entity extraction\n",
    "#   - Email drafting\n",
    "#   - Code generation\n",
    "#   - Customer support bots\n",
    "#   - RAG summarization format\n",
    "# ============================================================\n",
    "\n",
    "\n",
    "# üß© 1Ô∏è‚É£ FEW-SHOT EXAMPLES (these teach the pattern)\n",
    "\n",
    "few_shot_examples = [\n",
    "    # Example 1\n",
    "    {\n",
    "        \"role\":\"user\",\n",
    "        \"content\":\"Convert to structured data: The user's name is Arjun and he is 29 years old.\"\n",
    "    },\n",
    "    {\n",
    "        \"role\":\"assistant\",\n",
    "        \"content\":\"{\\\"name\\\":\\\"Arjun\\\",\\\"age\\\":29}\"\n",
    "    },\n",
    "    # Example 2\n",
    "    {\n",
    "        \"role\":\"user\",\n",
    "        \"content\":\"Convert to structured data: The user's name is Meera and she is 24 years old.\"\n",
    "    },\n",
    "    \n",
    "    {\n",
    "        \"role\":\"assistant\",\n",
    "        \"content\":\"{\\\"name\\\": \\\"Meera\\\",\\\"age\\\":24}\"\n",
    "\n",
    "    }\n",
    "]\n",
    "\n",
    "\n",
    "# üß© 2Ô∏è‚É£ NOW THE REAL TASK (model will follow examples above)\n",
    "\n",
    "actual_task = [\n",
    "    {\n",
    "        \"role\":\"user\",\n",
    "        \"content\":\"Convert to structured data: The user's name is Dhiru and he is 36 years old.\"\n",
    "    }\n",
    "]\n",
    "\n",
    "# Combined example + Task\n",
    "messages_few_shot = few_shot_examples + actual_task\n",
    "\n",
    "# üß† 3Ô∏è‚É£ MODEL CALL\n",
    "response_few_shot = client.chat.completions.create(\n",
    "    model=\"llama-3.1-8b-instant\",\n",
    "    messages=messages_few_shot,\n",
    "    max_tokens=50\n",
    ")\n",
    "\n",
    "output_few_shot = response_few_shot.choices[0].message.content\n",
    "\n",
    "\n",
    "# üß™ 4Ô∏è‚É£ PRINT RESULT\n",
    "print(\"===== üéØ FEW-SHOT OUTPUT =====\")\n",
    "print(output_few_shot)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f1f2bd5",
   "metadata": {},
   "source": [
    "#### **üß† Zero-Shot vs One-Shot vs Few-Shot Prompting**\n",
    "\n",
    "In GenAI, the ‚Äúshots‚Äù refer to how many **examples** we show the model before asking it to perform a task.\n",
    "\n",
    "This directly controls:\n",
    "- how accurate the model is\n",
    "- how consistent the output becomes\n",
    "- how much hallucination is reduced\n",
    "- how predictable the format is\n",
    "- how ‚Äúsmart‚Äù the model appears\n",
    "\n",
    "Understanding these 3 is essential for:\n",
    "\n",
    "‚úî RAG  \n",
    "‚úî Agents  \n",
    "‚úî Code generation bots  \n",
    "‚úî SQL assistants  \n",
    "‚úî Email writers  \n",
    "‚úî Summarizers  \n",
    "‚úî Data extractors  \n",
    "‚úî Enterprise AI tools  \n",
    "‚úî Interviews at Big Tech  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "38c960ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===== üü¶ ZERO-SHOT OUTPUT =====\n",
      "Name: Neha\n",
      "Age: 22 \n",
      "\n",
      "===== üüß ONE-SHOT OUTPUT =====\n",
      "{\"name\": \"Neha\", \"age\": 22} \n",
      "\n",
      "===== üü© FEW-SHOT OUTPUT =====\n",
      "{\"name\": \"Neha\", \"age\": 22}\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# üìò SECTION 9.8 ‚Äî Zero-shot vs One-shot vs Few-shot Prompts\n",
    "# ------------------------------------------------------------\n",
    "# Why this matters?\n",
    "#   - Different tasks require different prompting approaches.\n",
    "#   - For structured output, few-shot is best.\n",
    "#   - For simple classification, zero-shot works well.\n",
    "#   - For formatting consistency, one-shot/few-shot is superior.\n",
    "# ============================================================\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 1Ô∏è‚É£ ZERO-SHOT PROMPTING (No examples)\n",
    "# ------------------------------------------------------------\n",
    "# Use case:\n",
    "#   - When task is simple or the model already understands it.\n",
    "#   - Fast, cheap, and works surprisingly well for knowledge queries.\n",
    "\n",
    "zero_shot = client.chat.completions.create(\n",
    "    model=\"llama-3.1-8b-instant\",\n",
    "    messages=[{\n",
    "        \"role\":\"user\",\n",
    "        \"content\":\"Extract name and age: The user's name is Neha and she is 22 years old.\"\n",
    "    }],\n",
    "    max_tokens=50\n",
    ")\n",
    "\n",
    "zero_out = zero_shot.choices[0].message.content\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 2Ô∏è‚É£ ONE-SHOT PROMPTING (Exactly one example)\n",
    "# ------------------------------------------------------------\n",
    "# Use case:\n",
    "#   - When you need consistent formatting.\n",
    "#   - The model follows the pattern of the single example.\n",
    "\n",
    "one_shot_messages = [\n",
    "    # ONE example\n",
    "    {\"role\":\"user\",\"content\":\"Extract: The user's name is Arjun and he is 29.\"},\n",
    "    {\"role\":\"assistant\",\"content\":\"{\\\"name\\\": \\\"Arjun\\\", \\\"age\\\": 29}\"},\n",
    "    \n",
    "    # NOW the real task\n",
    "    {\"role\":\"user\",\n",
    "    \"content\":\"Extract: The user's name is Neha and she is 22 years old.\"}\n",
    "]\n",
    "\n",
    "one_shot = client.chat.completions.create(\n",
    "    model=\"llama-3.1-8b-instant\",\n",
    "    messages=one_shot_messages,\n",
    "    max_tokens=50\n",
    ")\n",
    "\n",
    "one_out = one_shot.choices[0].message.content\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 3Ô∏è‚É£ FEW-SHOT PROMPTING (Multiple examples)\n",
    "# ------------------------------------------------------------\n",
    "# Use case:\n",
    "#   - Best for structured output.\n",
    "#   - Reduces hallucination.\n",
    "#   - Ensures exact format required in production.\n",
    "\n",
    "few_shot_messages = [\n",
    "{    \"role\":\"user\",\n",
    "    \"content\":\"Extract: The user's name is Rohan and he is 31.\"},\n",
    "{    'role':\"assistant\",\n",
    "    \"content\":\"{\\\"name\\\": \\\"Rohan\\\", \\\"age\\\": 31}\"},\n",
    "\n",
    "# REAL TASK\n",
    "\n",
    "{    \"role\":\"user\",\n",
    "    \"content\":\"ExtractExtract: The user's name is Neha and she is 22 years old.\"}\n",
    "]\n",
    "\n",
    "few_shot = client.chat.completions.create(\n",
    "    model=\"llama-3.1-8b-instant\",\n",
    "    messages=few_shot_messages,\n",
    "    max_tokens=50\n",
    ")\n",
    "\n",
    "few_out = few_shot.choices[0].message.content\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 4Ô∏è‚É£ Display Results\n",
    "# ------------------------------------------------------------\n",
    "print(\"===== üü¶ ZERO-SHOT OUTPUT =====\")\n",
    "print(zero_out, \"\\n\")\n",
    "\n",
    "print(\"===== üüß ONE-SHOT OUTPUT =====\")\n",
    "print(one_out, \"\\n\")\n",
    "\n",
    "print(\"===== üü© FEW-SHOT OUTPUT =====\")\n",
    "print(few_out)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fb090f4",
   "metadata": {},
   "source": [
    "# üß† Zero-Shot, One-Shot, Few-Shot Prompting  \n",
    "### (Scenarios ‚Ä¢ When to Use ‚Ä¢ Roles ‚Ä¢ Final Summary)\n",
    "\n",
    "---\n",
    "\n",
    "## üéØ 1. When Should We Use Zero-Shot, One-Shot, and Few-Shot Prompting?\n",
    "\n",
    "These techniques define **how much guidance** we provide an LLM before asking it to perform a task.\n",
    "\n",
    "---\n",
    "\n",
    "# üîµ Zero-Shot Prompting ‚Äî *‚ÄúModel, figure it out yourself.‚Äù*\n",
    "\n",
    "### ‚úÖ When to Use:\n",
    "- The task is **simple**\n",
    "- You don‚Äôt need strict formatting\n",
    "- Want **quick, cheap** inference\n",
    "- The model already understands the concept\n",
    "\n",
    "### üß† Examples:\n",
    "- ‚ÄúExplain recursion.‚Äù\n",
    "- ‚ÄúSummarize this.‚Äù\n",
    "- ‚ÄúTranslate this sentence.‚Äù\n",
    "- ‚ÄúWhat is the capital of Japan?‚Äù\n",
    "\n",
    "### üìå Real-World Usage:\n",
    "- Chatbots  \n",
    "- Knowledge Q&A  \n",
    "- Simple utilities  \n",
    "- Brainstorming  \n",
    "\n",
    "---\n",
    "\n",
    "# üü† One-Shot Prompting ‚Äî *‚ÄúHere is ONE example. Follow this pattern.‚Äù*\n",
    "\n",
    "### ‚úÖ When to Use:\n",
    "- You want the model to follow a **specific style**\n",
    "- Output format is **somewhat important**\n",
    "- You want more consistency than zero-shot\n",
    "- You want to teach tone or structure\n",
    "\n",
    "### üß† Examples:\n",
    "- Customer support reply templates  \n",
    "- Email formats  \n",
    "- JSON structure guidance  \n",
    "- Product description style  \n",
    "\n",
    "### üìå Real-World Usage:\n",
    "- Customer support bots  \n",
    "- Code formatting tasks  \n",
    "- Email writing assistants  \n",
    "\n",
    "---\n",
    "\n",
    "# üü¢ Few-Shot Prompting ‚Äî *‚ÄúHere are MULTIPLE examples. Learn this EXACT pattern.‚Äù*\n",
    "\n",
    "### ‚úÖ When to Use:\n",
    "- You need **consistent and accurate** output  \n",
    "- Structured output (JSON, SQL, XML)  \n",
    "- You must reduce hallucinations  \n",
    "- Model must match your format EXACTLY  \n",
    "- Production-level reliability is required\n",
    "\n",
    "### üß† Examples:\n",
    "- Data extraction (NER ‚Üí JSON)  \n",
    "- SQL generation  \n",
    "- Classification tasks  \n",
    "- Strict document summaries  \n",
    "- Multi-step reasoning  \n",
    "\n",
    "#### üìå Real-World Usage:\n",
    "- ChatGPT internal templates  \n",
    "- Enterprise information extraction  \n",
    "- SQL/text-to-structured pipelines  \n",
    "- RAG post-processing  \n",
    "- Financial report extraction  \n",
    "\n",
    "---\n",
    "\n",
    "### üß© 2. Final Summary Table (A+B)\n",
    "\n",
    "| Prompting Style | Best Time to Use | Strength |\n",
    "|------------------|------------------|----------|\n",
    "| **Zero-Shot** | Simple tasks | Fast, flexible |\n",
    "| **One-Shot** | Semi-structured tasks | Follows 1 example |\n",
    "| **Few-Shot** | Production systems | Accurate, consistent, low hallucination |\n",
    "\n",
    "---\n",
    "\n",
    "#### üß† 3. Role Explanation: System vs User vs Assistant (C)\n",
    "\n",
    "LLM messages contain roles that control behavior and context.\n",
    "\n",
    "---\n",
    "\n",
    "#### üü£ System Role ‚Äî *‚ÄúThe rulebook + personality.‚Äù*\n",
    "\n",
    "#### Purpose:\n",
    "- Sets rules  \n",
    "- Defines behavior  \n",
    "- Controls tone  \n",
    "- Harder for model to override  \n",
    "- Highest priority instruction  \n",
    "\n",
    "#### Example:\n",
    "```json\n",
    "{\"role\": \"system\", \"content\": \"You are a JSON-only extraction assistant.\"}\n",
    "\n",
    "**üîµ User Role ‚Äî ‚ÄúThe actual input or question**\n",
    "\n",
    "Purpose:\n",
    "\n",
    "- Represents the user's request\n",
    "- The model must respond to this\n",
    "\n",
    "Example: {\"role\": \"user\", \"content\": \"Extract name and age.\"}\n",
    "\n",
    "**üü¢ Assistant Role ‚Äî ‚ÄúModel‚Äôs previous replies.‚Äù**\n",
    "\n",
    "**Purpose:**\n",
    "- Shows examples (one-shot/few-shot)\n",
    "- Helps maintain continuity\n",
    "- Teaches formatting patterns\n",
    "\n",
    "Example: {\"role\": \"assistant\", \"content\": \"{\\\"name\\\": \\\"Arjun\\\", \\\"age\\\": 29}\"}\n",
    "\n",
    "**ü§î Why Didn't We Use System Role in This Exercise?**\n",
    "\n",
    "Because Step 9.8 focused on teaching through examples, not enforcing global rules.\n",
    "\n",
    "Few-shot examples already taught:\n",
    "\n",
    "- Structure\n",
    "- Format\n",
    "- Output pattern\n",
    "\n",
    "But in real production systems, you ALWAYS use the system role."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c382fbe1",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "caffef97",
   "metadata": {},
   "source": [
    "#### **Temperature, Top-p, Max Tokens, and Controlling Model Behavior**\n",
    "\n",
    "**üìå Before I give the next code cell, we follow our rule:**\n",
    "\n",
    "We will do ONE sub-step at a time.\n",
    "\n",
    "So Step 9.9 is large ‚Äî\n",
    "We will break it into sub-steps like:\n",
    "\n",
    "- 9.9A ‚Äî Understanding Temperature\n",
    "- 9.9B ‚Äî Understanding Top-p\n",
    "- 9.9C ‚Äî Max Tokens (output control)\n",
    "- 9.9D ‚Äî Frequency & Presence Penalties\n",
    "- 9.9E ‚Äî Comparing outputs with examples\n",
    "- 9.9F ‚Äî When to use which settings\n",
    "- 9.9G ‚Äî Final Summary (as per your new rule)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66518959",
   "metadata": {},
   "source": [
    "### **9.9A ‚Äî Understanding Temperature (Concept Only)**\n",
    "\n",
    "#### üî• Temperature ‚Äî Controls Creativity vs Factual Accuracy\n",
    "\n",
    "Temperature is a value between **0 and 2**.\n",
    "\n",
    "It decides how ‚Äúrandom‚Äù or ‚Äúcreative‚Äù the model will be.\n",
    "\n",
    "#### Low Temperature (0.0 ‚Äì 0.3)\n",
    "- Very deterministic  \n",
    "- Factual  \n",
    "- Reproducible  \n",
    "- Good for:\n",
    "  - SQL  \n",
    "  - Coding  \n",
    "  - Math  \n",
    "  - JSON extraction  \n",
    "  - RAG answers  \n",
    "\n",
    "#### Medium Temperature (0.4 ‚Äì 0.7)\n",
    "- Balanced  \n",
    "- Useful for:\n",
    "  - Explanations  \n",
    "  - Friendly chatbots  \n",
    "  - Educational tutors  \n",
    "\n",
    "#### High Temperature (0.8 ‚Äì 1.3)\n",
    "- Creative, unpredictable  \n",
    "- Good for:\n",
    "  - Stories  \n",
    "  - Brainstorming  \n",
    "  - Marketing  \n",
    "\n",
    "#### Very High (1.4 ‚Äì 2.0)\n",
    "- Chaotic  \n",
    "- Not recommended for production  \n",
    "\n",
    "#### Simple Analogy:\n",
    "Temperature = How \"imaginative\" the model becomes.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab24528c",
   "metadata": {},
   "source": [
    "#### **9.9B ‚Äî Understanding Top-p (Nucleus Sampling)**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7731777b",
   "metadata": {},
   "source": [
    "#### üü£ Step 9.9B ‚Äî Top-p (Nucleus Sampling)\n",
    "\n",
    "#### üéØ What is Top-p?\n",
    "\n",
    "Top-p controls **how many possible words** the model is allowed to choose from when generating the next token.\n",
    "\n",
    "Think of it like this:\n",
    "\n",
    "- Temperature = *How creative should the model be?*  \n",
    "- Top-p = *How wide should the model‚Äôs choice options be?*\n",
    "\n",
    "Both seem similar but work differently.\n",
    "\n",
    "---\n",
    "\n",
    "#### üß† How Top-p Works\n",
    "\n",
    "The model sorts all possible next tokens by probability and includes **only the smallest set of tokens whose probabilities sum to p**.\n",
    "\n",
    "Example:  \n",
    "If p = 0.9 ‚Üí include tokens until their total probability = 90%  \n",
    "If p = 0.5 ‚Üí include fewer possibilities (more restrictive)\n",
    "\n",
    "---\n",
    "\n",
    "#### üìå Typical Values and Their Meaning\n",
    "\n",
    "#### üîµ **Top-p = 1.0 (default)**\n",
    "- No restriction  \n",
    "- Model can pick from all possible words  \n",
    "- Most natural, balanced output  \n",
    "\n",
    "#### üü° **Top-p = 0.9**\n",
    "- Removes unlikely/rare words  \n",
    "- Makes writing cleaner, more stable  \n",
    "- Good for:\n",
    "  - Chatbots\n",
    "  - Explanations  \n",
    "  - RAG  \n",
    "\n",
    "#### üü† **Top-p = 0.5**\n",
    "- Very limited choice  \n",
    "- Makes output:\n",
    "  - Simple  \n",
    "  - Safe  \n",
    "  - Predictable  \n",
    "\n",
    "#### üî¥ **Top-p < 0.3**\n",
    "- Very restrictive  \n",
    "- Often too robotic  \n",
    "\n",
    "---\n",
    "\n",
    "#### üß™ Temperature vs Top-p ‚Äî Key Difference\n",
    "\n",
    "| Setting | Controls | Example |\n",
    "|--------|----------|---------|\n",
    "| **Temperature** | Randomness / Creativity | How wild or boring ideas are |\n",
    "| **Top-p** | Token selection range | How many options the model can choose from |\n",
    "\n",
    "#### ‚úî Temperature = intensity  \n",
    "#### ‚úî Top-p = choice range  \n",
    "\n",
    "---\n",
    "\n",
    "#### üéØ Best Practices (Real-World)\n",
    "\n",
    "| Task Type | Temperature | Top-p | Why |\n",
    "|-----------|-------------|--------|------|\n",
    "| SQL/Code | 0‚Äì0.2 | 0.9 | Accurate, deterministic |\n",
    "| RAG QA | 0.1‚Äì0.3 | 0.9 | Stable factual answers |\n",
    "| Formal writing | 0.2‚Äì0.5 | 0.9 | Polished output |\n",
    "| Creative writing | 0.7‚Äì1.1 | 1.0 | More ideas allowed |\n",
    "| Poetry/story | 0.9‚Äì1.3 | 1.0 | Maximum creativity |\n",
    "\n",
    "---\n",
    "\n",
    "#### üî• Simple Analogy  \n",
    "If Temperature = *How crazy the chef can be*,  \n",
    "then Top-p = *How many ingredients the chef is allowed to choose from.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "697377b7",
   "metadata": {},
   "source": [
    "#### **Step 9.9C ‚Äî Max Tokens (Output Length Control)**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a34a6cb3",
   "metadata": {},
   "source": [
    "#### üü© Step 9.9C ‚Äî Max Tokens (Output Length Control)\n",
    "\n",
    "#### üéØ What is max_tokens?\n",
    "\n",
    "`max_tokens` specifies **how many tokens the model is allowed to generate in the output**.\n",
    "\n",
    "Tokens ‚â† words.  \n",
    "A token is roughly:\n",
    "- 1 word (short word), or  \n",
    "- Part of a word (longer word)\n",
    "\n",
    "Example:\n",
    "- \"fantastic\" = 2 tokens  \n",
    "- \"I am fine\" = 4 tokens  \n",
    "\n",
    "---\n",
    "\n",
    "### üéØ Why is max_tokens important?\n",
    "\n",
    "Max tokens prevents:\n",
    "\n",
    "- runaway responses  \n",
    "- infinite loops  \n",
    "- extra text the model may add  \n",
    "- over-long answers  \n",
    "- too much verbosity  \n",
    "\n",
    "Especially in RAG, SQL, code generation, chatbots ‚Äî  \n",
    "**you MUST control output size**.\n",
    "\n",
    "---\n",
    "\n",
    "### üìå How It Works\n",
    "\n",
    "### Example:\n",
    "`max_tokens = 20`\n",
    "\n",
    "Model stops generating after ~20 tokens, even if:\n",
    "\n",
    "- The answer is incomplete  \n",
    "- The model had more to say  \n",
    "- The model was in the middle of a sentence  \n",
    "\n",
    "---\n",
    "\n",
    "#### üß© Common Mistake New Learners Make  \n",
    "They think `max_tokens` limits *input length* ‚Äî  \n",
    "but actually, it limits **output length** only.\n",
    "\n",
    "---\n",
    "\n",
    "#### üî• Real-World Usage\n",
    "\n",
    "| Use Case | max_tokens | Reason |\n",
    "|----------|------------|--------|\n",
    "| JSON extraction | 50 | Output small, predictable |\n",
    "| SQL generation | 100 | SQL not very long |\n",
    "| RAG QA | 150‚Äì300 | Moderate answers |\n",
    "| Email drafting | 200‚Äì400 | Longish content |\n",
    "| Essay/story | 500‚Äì800 | More space needed |\n",
    "| Code generation | 300‚Äì600 | Medium length required |\n",
    "\n",
    "---\n",
    "\n",
    "### üß† Why This Matters for Production Systems\n",
    "\n",
    "If you don't control max tokens:\n",
    "\n",
    "- Chatbot may write pages of text  \n",
    "- SQL generator may hallucinate full explanations  \n",
    "- JSON extractor may add unwanted commentary  \n",
    "- API cost increases  \n",
    "- Response time increases  \n",
    "- Users get confused  \n",
    "\n",
    "So **max_tokens = part of prompt control**.\n",
    "\n",
    "---\n",
    "\n",
    "#### üìå Recommended Defaults\n",
    "\n",
    "#### Facts / JSON / SQL:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a24791b",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "acab623d",
   "metadata": {},
   "source": [
    "#### **STEP 9.9D ‚Äî Frequency Penalty & Presence Penalty**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "736729b9",
   "metadata": {},
   "source": [
    "These two parameters control **how much the model should avoid repeating words or ideas.**\n",
    "\n",
    "They are extremely useful in:\n",
    "- Chatbots  \n",
    "- Story generation  \n",
    "- RAG summaries  \n",
    "- Email writing  \n",
    "- Answers where repetition looks bad  \n",
    "- Avoiding loops (very important for agents)  \n",
    "\n",
    "---\n",
    "\n",
    "**üü¶ 1. Frequency Penalty ‚Äî ‚ÄúDon‚Äôt repeat the same word too much.‚Äù**\n",
    "\n",
    "**üéØ What it does:**\n",
    "- If the model repeats a word many times, frequency_penalty pushes it to **reduce repetition**.\n",
    "\n",
    "**Example of unwanted repetition:**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7210dca",
   "metadata": {},
   "source": [
    "\n",
    "Presence penalty makes the model **explore more ideas**.\n",
    "\n",
    "---\n",
    "\n",
    "**üìå Summary Table ‚Äî Difference Between Both**\n",
    "\n",
    "| Penalty Type | Controls What | Helps With |\n",
    "|--------------|---------------|------------|\n",
    "| **Frequency Penalty** | Repeated words | Avoiding repetition, more natural sentences |\n",
    "| **Presence Penalty** | Repeated topics/ideas | Exploring new ideas, preventing narrow responses |\n",
    "\n",
    "---\n",
    "\n",
    "**‚≠ê Recommended Values (Industry Standard)**\n",
    "\n",
    "| Use Case | frequency_penalty | presence_penalty |\n",
    "|----------|--------------------|------------------|\n",
    "| JSON / SQL | 0 | 0 |\n",
    "| Chatbots | 0.2 | 0.2 |\n",
    "| Conversational agents | 0.3‚Äì0.7 | 0.3‚Äì0.7 |\n",
    "| Creative writing | 0.5‚Äì1.0 | 0.5‚Äì1.0 |\n",
    "| Story generation | 1.0+ | 1.0+ |\n",
    "\n",
    "---\n",
    "\n",
    "**üß† Real-World Examples**\n",
    "\n",
    "**Chatbots (avoid repeating user's sentence)**\n",
    "frequency_penalty = 0.3  \n",
    "presence_penalty = 0.2  \n",
    "\n",
    "**üîπ Long-form content (avoid loops)**\n",
    "frequency_penalty = 0.7  \n",
    "presence_penalty = 0.7  \n",
    "\n",
    "**üîπ Creative writing (encourage new ideas)**\n",
    "frequency_penalty = 1.0  \n",
    "presence_penalty = 1.2  \n",
    "\n",
    "---\n",
    "\n",
    "**üéØ Simple Analogy**\n",
    "\n",
    "- **Frequency Penalty** = ‚ÄúStop repeating the same words.‚Äù\n",
    "- **Presence Penalty** = ‚ÄúTalk about new things too.‚Äù\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d64bbe4",
   "metadata": {},
   "source": [
    "#### **STEP 9.9E ‚Äî Side-by-Side Comparison of Model Parameters (SEE the Difference)**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b9064de",
   "metadata": {},
   "source": [
    "**üî¨ Step 9.9E ‚Äî Parameter Comparison (Practical Intuition)**\n",
    "\n",
    "#### **Objective**\n",
    "Understand how changing model parameters affects:\n",
    "- Creativity\n",
    "- Repetition\n",
    "- Output length\n",
    "- Topic diversity\n",
    "\n",
    "We will compare multiple responses to the SAME question\n",
    "by changing only the model parameters.\n",
    "\n",
    "This helps in:\n",
    "- Chatbot tuning\n",
    "- RAG answer quality\n",
    "- Agent stability\n",
    "- Interview explanations\n",
    "- Production reliability"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecfa7310",
   "metadata": {},
   "source": [
    "- **Low temperature (0.1) is more accurate and deterministic.**\n",
    "- **Medium temperature (0.6) is a balance of accuracy and creativity.**\n",
    "- **High temperature (1.1) is more creative and can produce varied responses.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "777938ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===== üü¶ LOW TEMPERATURE (0.1) =====\n",
      "**What is a Python List?**\n",
      "\n",
      "In Python, a list is a collection of items that can be of any data type, including strings, integers, floats, and other lists. It's similar to an array in other programming languages, but more flexible and powerful.\n",
      "\n",
      "**Basic Concepts:**\n",
      "\n",
      "* A list is denoted by square brackets `[]`.\n",
      "* Items in a list are separated by commas `,`.\n",
      "* Each item in a list is called an **element** or **value**.\n",
      "* Lists are **mutable**, meaning you can change them after they're created.\n",
      "\n",
      "**Example:**\n",
      "\n",
      "``` \n",
      "\n",
      "================üüßMEDIUM TEMPERATURE (0.6) =====\n",
      "**What is a Python List?**\n",
      "\n",
      "In Python, a list is a collection of items that can be of any data type, including strings, integers, floats, and other lists. It's like a container that can hold multiple values.\n",
      "\n",
      "**Basic Syntax**\n",
      "\n",
      "A Python list is defined using square brackets `[]`. You can create a list by separating items with commas.\n",
      "\n",
      "```python\n",
      "# Example of a list with different data types\n",
      "my_list = [1, \"hello\", 3.14, True, [1, 2, 3]]\n",
      "```\n",
      "\n",
      "**Key Features of Python Lists \n",
      "\n",
      "=======================üü• HIGH TEMPERATURE (1.1) =====\n",
      "**What are Python Lists?**\n",
      "\n",
      "In Python, a list is a collection of items stored in a single variable. Think of a list as a box where you can store multiple items, and each item can be a number, text, or even another list.\n",
      "\n",
      "**Basic Characteristics:**\n",
      "\n",
      "- **Ordered**: Lists store items in a specific order.\n",
      "- **Indexed**: Each item in a list has a unique index (like a postal code) that identifies its position in the list.\n",
      "- **Editable**: You can add, remove, or change items in a list after it's created.\n",
      "\n",
      "**How to\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# üìò SECTION 9.9E ‚Äî Side-by-Side Output Comparison\n",
    "# ------------------------------------------------------------\n",
    "# Goal:\n",
    "#   - Ask the SAME question\n",
    "#   - Change only model parameters\n",
    "#   - Observe how output changes\n",
    "# ============================================================\n",
    "\n",
    "question = \"Explain Python lists in simple terms :\"\n",
    "\n",
    "response_low_temp = client.chat.completions.create(\n",
    "    model=\"llama-3.1-8b-instant\",\n",
    "    messages=[{\"role\":\"user\",\"content\":question}],\n",
    "    temperature=0.1,\n",
    "    top_p=0.9,\n",
    "    max_tokens=120\n",
    ")\n",
    "\n",
    "low_temp_output = response_low_temp.choices[0].message.content\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 2Ô∏è‚É£ MEDIUM TEMPERATURE (Balanced)\n",
    "# ------------------------------------------------------------\n",
    "\n",
    "response_mid_temp = client.chat.completions.create(\n",
    "    model=\"llama-3.1-8b-instant\",\n",
    "    messages=[{\"role\":\"user\",\"content\":question}],\n",
    "    temperature=0.6,\n",
    "    top_p=0.9,\n",
    "    max_tokens=120\n",
    ")\n",
    "\n",
    "mid_temp_output = response_mid_temp.choices[0].message.content\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 3Ô∏è‚É£ HIGH TEMPERATURE (Creative)\n",
    "# ------------------------------------------------------------\n",
    "\n",
    "response_high_temp = client.chat.completions.create(\n",
    "    model=\"llama-3.1-8b-instant\",\n",
    "    messages=[{\"role\":\"user\",\"content\":question}],\n",
    "    temperature=1.1,\n",
    "    top_p=1.0,\n",
    "    max_tokens=120,\n",
    "    presence_penalty=0.6\n",
    ")\n",
    "\n",
    "high_temp_output = response_high_temp.choices[0].message.content\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 4Ô∏è‚É£ DISPLAY RESULTS\n",
    "# ------------------------------------------------------------\n",
    "print(\"===== üü¶ LOW TEMPERATURE (0.1) =====\")\n",
    "print(low_temp_output, \"\\n\")\n",
    "\n",
    "print(\"================üüßMEDIUM TEMPERATURE (0.6) =====\")\n",
    "print(mid_temp_output, \"\\n\")\n",
    "\n",
    "print(\"=======================üü• HIGH TEMPERATURE (1.1) =====\")\n",
    "print(high_temp_output)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d90f7a1a",
   "metadata": {},
   "source": [
    "**# Observations from Step 9.9E**\n",
    "\n",
    "- Low temperature produced factual and concise output.\n",
    "- Medium temperature gave a balanced explanation.\n",
    "- High temperature introduced creativity and expressive language.\n",
    "\n",
    "Conclusion:\n",
    "- Parameter tuning is task-dependent.\n",
    "- There is NO single best setting.\n",
    "- Real-world GenAI systems dynamically adjust parameters.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f47d1d46",
   "metadata": {},
   "source": [
    "## üß† Parameter Selection by Use-Case\n",
    "\n",
    "### ü§ñ 1. Chatbots (Learning / Support / Assistant)\n",
    "- temperature: 0.5 ‚Äì 0.7\n",
    "- top_p: 0.9\n",
    "- max_tokens: 200‚Äì400\n",
    "- frequency_penalty: 0.2\n",
    "- presence_penalty: 0.2\n",
    "\n",
    "Why:\n",
    "- Friendly tone\n",
    "- Avoid repetition\n",
    "- Balanced creativity\n",
    "\n",
    "---\n",
    "\n",
    "### üìö 2. RAG (Retrieval-Augmented Generation)\n",
    "- temperature: 0.1 ‚Äì 0.3\n",
    "- top_p: 0.9\n",
    "- max_tokens: 150‚Äì300\n",
    "- frequency_penalty: 0\n",
    "- presence_penalty: 0\n",
    "\n",
    "Why:\n",
    "- Accuracy over creativity\n",
    "- Reduce hallucinations\n",
    "- Faithful to retrieved documents\n",
    "\n",
    "---\n",
    "\n",
    "### üßÆ 3. SQL / Code Generation\n",
    "- temperature: 0.0 ‚Äì 0.2\n",
    "- top_p: 0.9\n",
    "- max_tokens: 100‚Äì300\n",
    "- frequency_penalty: 0\n",
    "- presence_penalty: 0\n",
    "\n",
    "Why:\n",
    "- Deterministic output\n",
    "- Syntax correctness\n",
    "- No creativity needed\n",
    "\n",
    "---\n",
    "\n",
    "### üìÑ 4. JSON / Structured Extraction\n",
    "- temperature: 0.0\n",
    "- top_p: 0.9\n",
    "- max_tokens: 50‚Äì100\n",
    "- stop sequences: YES\n",
    "- penalties: 0\n",
    "\n",
    "Why:\n",
    "- Strict formatting\n",
    "- Machine-readable output\n",
    "- API safe\n",
    "\n",
    "---\n",
    "\n",
    "### üß† 5. Agents / Multi-Step Reasoning\n",
    "- temperature: 0.3 ‚Äì 0.5\n",
    "- top_p: 0.9\n",
    "- max_tokens: 300‚Äì600\n",
    "- frequency_penalty: 0.3\n",
    "- presence_penalty: 0.3\n",
    "\n",
    "Why:\n",
    "- Encourage reasoning\n",
    "- Avoid loops\n",
    "- Maintain stability\n",
    "\n",
    "---\n",
    "\n",
    "### ‚úçÔ∏è 6. Creative Writing / Brainstorming\n",
    "- temperature: 0.9 ‚Äì 1.2\n",
    "- top_p: 1.0\n",
    "- max_tokens: 500+\n",
    "- frequency_penalty: 0.7\n",
    "- presence_penalty: 0.7\n",
    "\n",
    "Why:\n",
    "- High creativity\n",
    "- Diverse ideas\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63689913",
   "metadata": {},
   "source": [
    "**FINAL SUMMARY ‚Äî Step 9.9E**\n",
    "- Learned how model parameters affect responses.\n",
    "- Saw real output differences with same input.\n",
    "- Understood why tuning is critical in production.\n",
    "- Built intuition required for GenAI interviews.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "828daefb",
   "metadata": {},
   "source": [
    "####  **‚≠ê STEP 9.9G ‚ÄîFinal Parameter Cheat-Sheet + Interview & Production Notes**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e61ceb88",
   "metadata": {},
   "source": [
    "**# üß† Step 9.9G ‚Äî Final LLM Parameter Cheat-Sheet & Interview Notes**\n",
    "\n",
    "This section summarizes all LLM control parameters learned so far.\n",
    "It is designed for:\n",
    "- Quick revision\n",
    "- Interview preparation\n",
    "- Production reference\n",
    "- Architecture decision-making\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "562c6175",
   "metadata": {},
   "source": [
    "**üìò FINAL PARAMETER CHEAT-SHEET (Core Knowledge)**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4958bea2",
   "metadata": {},
   "source": [
    "#### **üîß LLM Control Parameters ‚Äî Quick Reference**\n",
    "\n",
    "### üî• Temperature\n",
    "Controls creativity and randomness.\n",
    "\n",
    "- 0.0‚Äì0.2 ‚Üí factual, deterministic (SQL, JSON, RAG)\n",
    "- 0.3‚Äì0.6 ‚Üí balanced (chatbots, tutoring)\n",
    "- 0.7‚Äì1.2 ‚Üí creative (ideas, stories)\n",
    "\n",
    "---\n",
    "\n",
    "### üü£ Top-p (Nucleus Sampling)\n",
    "Controls how wide the model‚Äôs choice set is.\n",
    "\n",
    "- 1.0 ‚Üí allow all tokens (default)\n",
    "- 0.9 ‚Üí remove rare/unlikely tokens (recommended)\n",
    "- <0.5 ‚Üí very restrictive, robotic\n",
    "\n",
    "---\n",
    "\n",
    "### üü© Max Tokens\n",
    "Controls output length (NOT input length).\n",
    "\n",
    "- 50‚Äì100 ‚Üí JSON / extraction\n",
    "- 150‚Äì300 ‚Üí RAG answers\n",
    "- 300‚Äì600 ‚Üí agents / reasoning\n",
    "- 500+ ‚Üí creative writing\n",
    "\n",
    "---\n",
    "\n",
    "### üî∂ Frequency Penalty\n",
    "Reduces repeated words.\n",
    "\n",
    "- 0.0 ‚Üí no restriction\n",
    "- 0.2‚Äì0.7 ‚Üí natural language\n",
    "- 1.0+ ‚Üí strong repetition control\n",
    "\n",
    "---\n",
    "\n",
    "### üî∑ Presence Penalty\n",
    "Encourages new topics.\n",
    "\n",
    "- 0.0 ‚Üí stay focused\n",
    "- 0.2‚Äì0.7 ‚Üí broader responses\n",
    "- 1.0+ ‚Üí idea exploration\n",
    "\n",
    "---\n",
    "\n",
    "### ‚õî Stop Sequences\n",
    "Forces model to stop output.\n",
    "\n",
    "Used for:\n",
    "- JSON-only responses\n",
    "- Tool calling\n",
    "- RAG boundaries\n",
    "- Preventing hallucination"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b555517a",
   "metadata": {},
   "source": [
    "**üß† INTERVIEW-LEVEL INSIGHTS (VERY IMPORTANT)**\n",
    "\n",
    "**üéØ Interview Notes (Google / Microsoft Level)**\n",
    "\n",
    "1. There is NO single best parameter setup.\n",
    "2. Parameters must be chosen based on task.\n",
    "3. RAG prioritizes accuracy over creativity.\n",
    "4. Agents require loop prevention (penalties).\n",
    "5. Structured output requires stop sequences.\n",
    "6. Prompt + parameters together control behavior.\n",
    "7. Determinism is critical for production systems.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45c7922c",
   "metadata": {},
   "source": [
    "**üèóÔ∏è PRODUCTION DECISION TABLE (REAL-WORLD)**\n",
    "\n",
    "**üè≠ Production Parameter Selection**\n",
    "\n",
    "| Use Case | Temp | Top-p | Max Tokens | Penalties |\n",
    "|--------|------|-------|------------|-----------|\n",
    "| Chatbot | 0.5 | 0.9 | 300 | 0.2 / 0.2 |\n",
    "| RAG | 0.2 | 0.9 | 200 | 0 / 0 |\n",
    "| SQL | 0.0 | 0.9 | 150 | 0 / 0 |\n",
    "| JSON | 0.0 | 0.9 | 80 | stop seq |\n",
    "| Agent | 0.4 | 0.9 | 500 | 0.5 / 0.5 |\n",
    "| Creative | 1.0 | 1.0 | 600 | 0.7 / 0.7 |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcd1cd18",
   "metadata": {},
   "source": [
    "## ‚úÖ Step 9 ‚Äî LLM Basics Final Summary\n",
    "\n",
    "- Learned how LLMs generate responses.\n",
    "- Understood prompt roles (system, user, assistant).\n",
    "- Mastered zero/one/few-shot prompting.\n",
    "- Gained control over creativity, length, repetition.\n",
    "- Learned production-grade parameter tuning.\n",
    "- Built interview-ready mental models.\n",
    "\n",
    "Status: LLM BASICS COMPLETED ‚úÖ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89b7de59",
   "metadata": {},
   "source": [
    "### **üíª Mini Practice**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d6b474d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== Low Temp =====\n",
      "**What are Python Dictionaries?**\n",
      "\n",
      "In Python, a dictionary is a data structure that stores a collection of key-value pairs. It's like a phonebook where you have names (keys) and phone numbers (values).\n",
      "\n",
      "**Key Features:**\n",
      "\n",
      "1. **Key-Value Pairs**: Each item in a dictionary is a pair of a key and a value. The key is unique and is used to identify the value.\n",
      "2. **Unordered**: Dictionaries are unordered, meaning that the order of the key-value pairs does not matter.\n",
      "3. **Mutable**: Dictionaries can be modified\n",
      "\n",
      "===== Medium Temp =====\n",
      "**What is a Dictionary in Python?**\n",
      "\n",
      "In Python, a dictionary is a data structure that stores collections of key-value pairs. It's like a container that holds many items, where each item has a unique key (or label) and a value associated with it.\n",
      "\n",
      "**Think of it like a Phonebook**\n",
      "\n",
      "Imagine you have a phonebook with names and phone numbers. Each name is like a key, and the corresponding phone number is like the value. You can look up a name in the phonebook to find the associated phone number.\n",
      "\n",
      "**Key Characteristics of Dictionaries**\n",
      "\n",
      "1. **Keys\n",
      "\n",
      "===== HIgh Temp =====\n",
      "**Python Dictionaries: A Simple Explanation**\n",
      "\n",
      "In Python, a dictionary is a data structure that stores a collection of key-value pairs. Think of it like a phonebook where you store names (keys) along with their phone numbers (values).\n",
      "\n",
      "**Key Features:**\n",
      "\n",
      "1. **Keys**: Unique identifiers for each value. Keys can be strings, integers, or any other immutable data type.\n",
      "2. **Values**: The data associated with each key. Values can be any data type, including strings, integers, lists, dictionaries, etc.\n",
      "3. **Associative**: Each key is associated with\n",
      "\n",
      "===== Low Top-p =====\n",
      "**What are Python Dictionaries?**\n",
      "\n",
      "In Python, a dictionary is a data structure that stores a collection of key-value pairs. It's like a phonebook where you have names (keys) and phone numbers (values).\n",
      "\n",
      "**Key Features:**\n",
      "\n",
      "1. **Keys**: Unique identifiers for each value. Keys can be strings, integers, or any other immutable type.\n",
      "2. **Values**: The data associated with each key. Values can be of any type, including strings, integers, lists, dictionaries, etc.\n",
      "3. **Unordered**: Dictionaries are not ordered, meaning the order of\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# üß™ MINI PRACTICE ‚Äî Parameter Intuition Builder\n",
    "# ------------------------------------------------------------\n",
    "# Goal:\n",
    "#   - Change ONE parameter at a time\n",
    "#   - Observe how output changes\n",
    "# ============================================================\n",
    "\n",
    "question = \"Explain Python dictionaries in simple terms.\"\n",
    "\n",
    "configs = [\n",
    "    {\"label\":\"Low Temp\",\"temperature\":0.1,\"top_p\":0.9},\n",
    "    {\"label\":\"Medium Temp\",\"temperature\":0.6,\"top_p\":0.9},\n",
    "    {\"label\":\"HIgh Temp\",\"temperature\":1.1,\"top_p\":0.9},\n",
    "    {\"label\":\"Low Top-p\",\"temperature\":0.6,\"top_p\":0.4}\n",
    "]\n",
    "\n",
    "for cfg in configs:\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"llama-3.1-8b-instant\",\n",
    "        messages=[{\"role\":\"user\",\"content\":question}],\n",
    "        temperature=cfg[\"temperature\"],\n",
    "        top_p=cfg[\"top_p\"],\n",
    "        max_tokens=120\n",
    "    )\n",
    "\n",
    "    print(f\"\\n===== {cfg['label']} =====\")\n",
    "    print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b4baf00",
   "metadata": {},
   "source": [
    "## ‚úÖ LLM Basics ‚Äî Completed\n",
    "\n",
    "Covered:\n",
    "- LLM API usage\n",
    "- Prompt roles\n",
    "- Parameter tuning\n",
    "- Prompting strategies\n",
    "- Practical intuition through experiments\n",
    "\n",
    "Next:\n",
    "‚û° 02_streaming_responses.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c1ac50e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
