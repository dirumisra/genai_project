{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "30df250e",
   "metadata": {},
   "source": [
    "#### **Client Configuration**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "214907ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Groq client configured successfully and ready to use.\n"
     ]
    }
   ],
   "source": [
    "# Importing the 'nbimporter' package which allows you to import Jupyter notebooks as Python modules\n",
    "import nbimporter\n",
    "\n",
    "# Commented out line (doesn't execute): The '%run' magic command would run the '01_grokai_chat_intro.ipynb' notebook in Jupyter.\n",
    "## %run 01_grokai_chat_intro.ipynb\n",
    "\n",
    "# Importing 'sys' to interact with the Python runtime environment (for modifying the system path)\n",
    "import sys\n",
    "\n",
    "# Importing 'os' to work with the operating system, like handling file paths\n",
    "import os\n",
    "\n",
    "# Add the absolute path to the project directory ('genai_project') to the Python path\n",
    "# This allows Python to find and import modules from this directory\n",
    "sys.path.append(os.path.abspath(\"C:/Users/dhira/Desktop/genai_project\"))\n",
    "\n",
    "# Import the 'client' object from the 'grokai_client_setup.py' file located in the project directory\n",
    "# This client is likely responsible for setting up communication with the GrokAI system\n",
    "from grokai_client_setup import client"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fef07b1",
   "metadata": {},
   "source": [
    "#### **üéØ Topic 1 ‚Äî Why LLM Parameters Matter**\n",
    "\n",
    "Large Language Models (LLMs) are inherently **probabilistic**:\n",
    "- This means the output varies based on input parameters\n",
    "- If you use default settings (temperature=1, max_tokens=2048), the model‚Äôs behavior will be **random**, **unpredictable**, and **costly** in production.\n",
    "\n",
    "Understanding **LLM parameters** is **critical** for:\n",
    "- Reducing hallucinations\n",
    "- Controlling cost\n",
    "- Making your models more deterministic and predictable\n",
    "\n",
    "**In this topic, we will:**\n",
    "- Understand what parameters control\n",
    "- Discuss how to fine-tune them for specific use cases\n",
    "- Explore common mistakes and best practices"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1885f66f",
   "metadata": {},
   "source": [
    "#### **üîç Why Do LLMs Have Parameters?**\n",
    "\n",
    "LLMs are designed to **generate text** based on probability.\n",
    "- Without parameters, the model would give random responses.\n",
    "- Parameters allow us to **shape** this randomness for desired behaviors.\n",
    "\n",
    "For instance:\n",
    "- **temperature** controls how creative or deterministic the model is.\n",
    "- **top_p** limits token selection to the top N most probable tokens.\n",
    "- **max_tokens** restricts how long the response can be.\n",
    "- **stop** sequences ensure responses are cut off cleanly.\n",
    "\n",
    "These controls let you **fine-tune** the model‚Äôs outputs for **specific tasks**.\n",
    "\n",
    "### Why we must **not rely on default settings**:\n",
    "- Default values are **good for demos** but not for real-world applications.\n",
    "- In real-world applications, **controlled behavior** is paramount for **reliability**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79068cb3",
   "metadata": {},
   "source": [
    "#### **üß† Senior-Level Insight**\n",
    "\n",
    "> \"Without parameter control, you are at the mercy of the model's randomness.\"\n",
    "\n",
    "In production:\n",
    "- **LLM behavior\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c7632da",
   "metadata": {},
   "source": [
    "#### **üéØ Topic 2 ‚Äî Temperature (Creativity vs Determinism)**\n",
    "\n",
    "Temperature is one of the most important **LLM parameters** that controls the **creativity** vs **determinism** of the model's output.\n",
    "\n",
    "### What is Temperature?\n",
    "- **Temperature** ranges from 0 to 1 (or higher in some cases).\n",
    "- At **high temperatures (e.g., 0.8)**, the model‚Äôs output is **more creative** and **less deterministic**.\n",
    "- At **low temperatures (e.g., 0.2)**, the model's output is **more predictable**, **conservative**, and **consistent**.\n",
    "\n",
    "### Why It Matters:\n",
    "- **High temperature**: Increases randomness. Useful for tasks that need creativity, like writing poetry or brainstorming.\n",
    "- **Low temperature**: Reduces randomness. Ideal for factual answers, like generating documentation, code completion, or providing precise information.\n",
    "\n",
    "For example:\n",
    "- **Temperature = 0.0** ‚Üí Extremely deterministic, always similar responses.\n",
    "- **Temperature = 1.0** ‚Üí Fully creative, with high variability."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3e1cdf6",
   "metadata": {},
   "source": [
    "#### **Use Temperature to Control Output Creativity**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "04733fa8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Low Temperature Response (Predictable):\n",
      "The capital of France is Paris.\n",
      "\n",
      "High Temperature Response (Creative):\n",
      "The capital of France is Paris.\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# üìò SECTION 1 ‚Äî Set Low Temperature (Deterministic)\n",
    "# ------------------------------------------------------------\n",
    "# Why?\n",
    "#   - This example demonstrates a low temperature, forcing the model to give more predictable, deterministic responses.\n",
    "#   - Ideal for structured or fact-based outputs.\n",
    "# ============================================================\n",
    "\n",
    "prompt = \"What is capital of france?\"\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "    model=\"llama-3.1-8b-instant\",  # Select model\n",
    "    messages=[{\"role\":\"user\",\"content\":prompt}],\n",
    "    temperature=0.2, # Low temperature for deterministic output\n",
    "    max_tokens=100\n",
    ")\n",
    "\n",
    "# Extract and display the response\n",
    "assistant_reply = response.choices[0].message.content\n",
    "print(\"Low Temperature Response (Predictable):\")\n",
    "print(assistant_reply)\n",
    "\n",
    "# ============================================================\n",
    "# üìò SECTION 2 ‚Äî Set High Temperature (Creative)\n",
    "# ------------------------------------------------------------\n",
    "# Why?\n",
    "#   - This example demonstrates a high temperature, allowing for creative and varied responses.\n",
    "#   - Ideal for unstructured, creative tasks like storytelling or generating ideas.\n",
    "# ============================================================\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "    model=\"llama-3.1-8b-instant\",\n",
    "    messages=[{\"role\":\"user\",\"content\":prompt}],\n",
    "    temperature=0.8, # High temperature for creative output\n",
    "    max_tokens=100\n",
    ")\n",
    "\n",
    "# Extract and display the response\n",
    "assistant_reply = response.choices[0].message.content\n",
    "\n",
    "print(\"\\nHigh Temperature Response (Creative):\")\n",
    "print(assistant_reply)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d07d18d",
   "metadata": {},
   "source": [
    "#### **Explanation:**\n",
    "\n",
    "- In **Section 1**, we're setting **temperature=0.2**. This ensures that the model gives **consistent and predictable responses**. \n",
    "   - Example: **What is the capital of France?** ‚Üí Answer: \"Paris.\"\n",
    "   - Very low variability, always the same or similar answer.\n",
    "  \n",
    "- In **Section 2**, we increase **temperature to 0.8**. This introduces more **creativity** in the model‚Äôs responses.\n",
    "   - Example: The same question about France could get more imaginative answers with varied phrasing and unexpected details.\n",
    "\n",
    "The **temperature parameter** essentially determines how **creative** or **controlled** the model will be when answering.\n",
    "\n",
    "In real-world scenarios:\n",
    "- **Low temperature** is preferred when you need **precise answers**.\n",
    "- **High temperature** is used when you need the model to **generate ideas** or be **creative**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f531fb2",
   "metadata": {},
   "source": [
    "#### **Temperature Tuning in Production**\n",
    "\n",
    "When working with GenAI in production, **temperature tuning** is essential for controlling the system's output.\n",
    "\n",
    "- **High temperature** could lead to **hallucinations** or **irrelevant answers**, which might cause **loss of user trust**.\n",
    "- **Low temperature** ensures that answers are **predictable**, but if set too low, it could lack **creativity** and **engagement**.\n",
    "\n",
    "Best practice:\n",
    "- **Use low temperatures** for factual, structured tasks (e.g., data extraction).\n",
    "- **Use higher temperatures** for exploratory tasks (e.g., content generation, idea brainstorming).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7c45e45",
   "metadata": {},
   "source": [
    "**‚úÖ Topic 2 Final Summary (Rule Applied)**\n",
    "\n",
    "Temperature controls the randomness of LLM responses\n",
    "\n",
    "Low temperature = more deterministic answers\n",
    "\n",
    "High temperature = more creative and varied answers\n",
    "\n",
    "Production Impact: The wrong temperature can lead to inefficient or inaccurate responses."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c47b7fd6",
   "metadata": {},
   "source": [
    "#### **üéØ Topic 3 ‚Äî top_p (Nucleus Sampling)**\n",
    "\n",
    "top_p, also known as **Nucleus Sampling**, is a parameter used to control the **quality** and **diversity** of the model's output.\n",
    "\n",
    "#### **What is top_p?**\n",
    "- **top_p** controls the **cumulative probability** of token selection.\n",
    "- Instead of selecting the **highest probability token**, top-p uses **cumulative probability** to choose the top \"p\" most likely tokens, ensuring diversity.\n",
    "\n",
    "#### **Why top_p Matters:**\n",
    "- It‚Äôs useful for controlling **creativity** while still staying within a certain **probability range**.\n",
    "- **top_p=0.9** means: the model will choose from the top 90% of tokens (excluding the least likely ones).\n",
    "\n",
    "#### **When to Use top_p vs Temperature:**\n",
    "- **top_p** limits the number of tokens sampled, which makes it different from **temperature**.\n",
    "- **Temperature** affects randomness; **top_p** affects how much to sample from the \"nucleus\" of likely tokens.\n",
    "\n",
    "It can reduce the chances of the model generating **irrelevant or low-quality** output."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b098e052",
   "metadata": {},
   "source": [
    "#### **Using top_p to Control Output Quality**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ecc22e75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Low top_p Response (Predictable):\n",
      "**Tuples vs Lists in Python**\n",
      "=====================================\n",
      "\n",
      "In Python, `tuples` and `lists` are two fundamental data structures that can store multiple values. While they share some similarities, there are key differences between them.\n",
      "\n",
      "**Lists**\n",
      "---------\n",
      "\n",
      "A `list` is a mutable, ordered collection of items that can be of any data type, including strings, integers, floats, and other lists. Lists are denoted by square brackets `[]` and elements are separated by commas.\n",
      "\n",
      "**Example\n",
      "\n",
      "High top_p Response (Creative):\n",
      "**Tuples vs Lists in Python**\n",
      "================================\n",
      "\n",
      "In Python, `tuples` and `lists` are two types of data structures that can store multiple values. While they share some similarities, they have distinct differences in terms of their usage, characteristics, and behavior.\n",
      "\n",
      "**Lists**\n",
      "---------\n",
      "\n",
      "A `list` is a mutable collection of items that can be of any data type, including strings, integers, floats, and other lists. Lists are denoted by square brackets `[]` and are defined\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# üìò SECTION 1 ‚Äî Set Low top_p (Nucleus Sampling)\n",
    "# ------------------------------------------------------------\n",
    "# Why?\n",
    "#   - This example demonstrates a low top_p (e.g., 0.2)\n",
    "#   - It forces the model to choose from a very **small** set of possible tokens, ensuring **high predictability** in output.\n",
    "# ============================================================\n",
    "\n",
    "prompt = \"Explain the difference between a tuple and a list in Python.\"\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "    model=\"llama-3.1-8b-instant\",\n",
    "    messages=[{\"role\":\"user\",\"content\":prompt}],\n",
    "    temperature=0.7,\n",
    "    top_p=0.2,\n",
    "    max_tokens=100\n",
    ")\n",
    "\n",
    "assistant_reply = response.choices[0].message.content\n",
    "\n",
    "print(\"Low top_p Response (Predictable):\")\n",
    "print(assistant_reply)\n",
    "\n",
    "# ============================================================\n",
    "# üìò SECTION 2 ‚Äî Set High top_p (Nucleus Sampling)\n",
    "# ------------------------------------------------------------\n",
    "# Why?\n",
    "#   - This example demonstrates a high top_p (e.g., 0.9), increasing the range of possible tokens.\n",
    "#   - This increases **creativity** and generates more **diverse** responses.\n",
    "# ============================================================\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "    model=\"llama-3.1-8b-instant\",\n",
    "    messages=[{\"role\":\"user\",\"content\":prompt}],\n",
    "    temperature=0.7,\n",
    "    top_p=0.9, # High top_p for creative output\n",
    "    max_tokens=100\n",
    ")\n",
    "\n",
    "assistant_reply = response.choices[0].message.content\n",
    "print(\"\\nHigh top_p Response (Creative):\")\n",
    "print(assistant_reply)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aec15888",
   "metadata": {},
   "source": [
    "#### **Explanation:**\n",
    "\n",
    "- **In Section 1**, we set **top_p=0.2**, which means the model will choose from a very narrow range of likely tokens, ensuring **high predictability** in responses.\n",
    "  - Example: **‚ÄúExplain the difference between a tuple and a list in Python‚Äù** ‚Üí The response will be **concise and factual**.\n",
    "\n",
    "- **In Section 2**, we increase **top_p to 0.9**, allowing the model to sample from a much wider range of possible tokens. This increases **creativity** and can lead to more **varied responses**.\n",
    "  - Example: The same question could now yield more **creative** answers, with additional elaboration or different phrasing.\n",
    "\n",
    "The **top_p parameter** essentially controls the **breadth of sampling**, helping strike a balance between **creativity** and **accuracy**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ec2d2a2",
   "metadata": {},
   "source": [
    "#### **top_p in Production**\n",
    "\n",
    "When you need **diverse** outputs for tasks like:\n",
    "- brainstorming ideas\n",
    "- creative writing\n",
    "- generating varied responses\n",
    "\n",
    "**High top_p values** (0.8‚Äì1.0) are appropriate.\n",
    "\n",
    "For **factual consistency** and **structure** in tasks like:\n",
    "- documentation\n",
    "- code completion\n",
    "- structured answers\n",
    "\n",
    "**Low top_p values** (0.2‚Äì0.3) are ideal to reduce randomness and ensure more predictable results."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c03208ad",
   "metadata": {},
   "source": [
    "#### **‚úÖ Topic 3 Final Summary (Rule Applied)**\n",
    "\n",
    "top_p determines the diversity of the model‚Äôs output.\n",
    "\n",
    "Low top_p (0.2): Restricts the model to a narrow, deterministic set of tokens.\n",
    "\n",
    "High top_p (0.9): Increases creativity by sampling from a broader range of tokens.\n",
    "\n",
    "Use top_p to fine-tune the balance between randomness and predictability."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9004b448",
   "metadata": {},
   "source": [
    "#### **üéØ Topic 4 ‚Äî max_tokens (Cost & Safety Guardrail)**\n",
    "\n",
    "`max_tokens` controls the **maximum length** of the model‚Äôs output.\n",
    "\n",
    "Why this matters in real systems:\n",
    "- **Cost control:** more tokens = more cost\n",
    "- **Latency control:** longer output = slower response\n",
    "- **Safety control:** prevents the model from producing very long, unnecessary, or risky outputs\n",
    "\n",
    "Key idea:\n",
    "If you don‚Äôt set `max_tokens`, your system can become:\n",
    "- expensive\n",
    "- slow\n",
    "- unpredictable"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cfcb820",
   "metadata": {},
   "source": [
    "#### **max_tokens in Action (Low vs High)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c72f7d09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===== ‚úÖ Low max_tokens (30) =====\n",
      "**Python Tuples**\n",
      "================\n",
      "\n",
      "Tuples in Python are immutable collections of objects that can be of any data type, including strings, integers, floats\n",
      "\n",
      "===== ‚úÖ Higher max_tokens (200) =====\n",
      "**Python Tuples**\n",
      "================\n",
      "\n",
      "A tuple in Python is a collection of objects that can be of any data type, including strings, integers, floats, and other tuples. Tuples are defined by enclosing the values in parentheses `()` and are immutable, meaning they cannot be changed after creation.\n",
      "\n",
      "**Example**\n",
      "-----------\n",
      "\n",
      "Here's an example of creating a tuple in Python:\n",
      "\n",
      "```python\n",
      "# Create a tuple\n",
      "my_tuple = (\"apple\", \"banana\", \"cherry\")\n",
      "\n",
      "# Accessing tuple elements\n",
      "print(my_tuple[0])  # Output: apple\n",
      "print(my_tuple[1])  # Output: banana\n",
      "print(my_tuple[2])  # Output: cherry\n",
      "\n",
      "# Tuple indexing\n",
      "print(my_tuple[0:2])  # Output: ('apple', 'banana')\n",
      "\n",
      "# Tuple unpacking\n",
      "fruit1, fruit2, fruit3 = my_tuple\n",
      "print(fruit1)  # Output: apple\n",
      "print(fruit2)  # Output: banana\n",
      "print(f\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# üìò SECTION 1 ‚Äî Prepare a Single Prompt\n",
    "# ------------------------------------------------------------\n",
    "# Why?\n",
    "#   - Same prompt for both tests so we can compare max_tokens fairly\n",
    "# ============================================================\n",
    "\n",
    "prompt = \"Explain Python tuples with an example.\"\n",
    "\n",
    "# ============================================================\n",
    "# üìò SECTION 2 ‚Äî Low max_tokens (Short + Controlled Output)\n",
    "# ------------------------------------------------------------\n",
    "# Why?\n",
    "#   - Forces a short answer\n",
    "#   - Useful for UI responses, summaries, and cost control\n",
    "# ============================================================\n",
    "\n",
    "response_low = client.chat.completions.create(\n",
    "    model=\"llama-3.1-8b-instant\",                 # Using our course-standard model\n",
    "    messages=[{\"role\":\"user\",\"content\":prompt}],\n",
    "    temperature=0.3,            # Low creativity to keep response stable\n",
    "    max_tokens=30               # ‚úÖ LIMIT output length\n",
    ")\n",
    "\n",
    "reply_low = response_low.choices[0].message.content\n",
    "print(\"===== ‚úÖ Low max_tokens (30) =====\")\n",
    "print(reply_low)\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# üìò SECTION 3 ‚Äî Higher max_tokens (More Detailed Output)\n",
    "# ------------------------------------------------------------\n",
    "# Why?\n",
    "#   - Allows longer explanation\n",
    "#   - Useful for tutorials, deep explanations, documentation-like responses\n",
    "# ============================================================\n",
    "\n",
    "\n",
    "response_high = client.chat.completions.create(\n",
    "    model=\"llama-3.1-8b-instant\",\n",
    "    messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "    temperature=0.3,\n",
    "    max_tokens=200                                # ‚úÖ ALLOW more detail\n",
    ")\n",
    "\n",
    "reply_high = response_high.choices[0].message.content\n",
    "print(\"\\n===== ‚úÖ Higher max_tokens (200) =====\")\n",
    "print(reply_high)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86e1ef71",
   "metadata": {},
   "source": [
    "#### **üîç What to Observe**\n",
    "\n",
    "When max_tokens is LOW:\n",
    "- the answer may feel incomplete\n",
    "- model may stop mid-explanation\n",
    "- output is cheap + fast\n",
    "\n",
    "When max_tokens is HIGH:\n",
    "- the answer is more complete\n",
    "- higher cost + slightly slower\n",
    "- better for ‚Äúteaching mode‚Äù\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6aec9390",
   "metadata": {},
   "source": [
    "#### **üß† How Senior Engineers Use max_tokens**\n",
    "\n",
    "#### ‚úÖ Chatbots (fast UX)\n",
    "- max_tokens: 100‚Äì300\n",
    "- goal: quick, concise answers\n",
    "\n",
    "#### ‚úÖ RAG answers (grounded + focused)\n",
    "- max_tokens: 200‚Äì500\n",
    "- goal: answer + citations/snippets\n",
    "\n",
    "#### ‚úÖ Extraction / JSON tasks\n",
    "- max_tokens: small and strict (50‚Äì200)\n",
    "- goal: prevent extra text\n",
    "\n",
    "#### ‚úÖ Long-form content generation\n",
    "- max_tokens: 800+\n",
    "- goal: detailed content, but with cost guardrails\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c02af0a",
   "metadata": {},
   "source": [
    "**‚úÖ Topic 4 Final Summary**\n",
    "\n",
    "max_tokens is a hard safety + cost limiter\n",
    "\n",
    "Low values = fast + cheap but may truncate\n",
    "\n",
    "High values = richer output but higher cost/latency\n",
    "\n",
    "Production systems ALWAYS set max_tokens intentionally"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a95a98e0",
   "metadata": {},
   "source": [
    "#### **üéØ Topic 5 ‚Äî Stop Sequences (Output Control)**\n",
    "\n",
    "A **stop sequence** tells the LLM:\n",
    "‚ÄúStop generating text when you reach THIS token or pattern.‚Äù\n",
    "\n",
    "Why this matters:\n",
    "- Prevents extra text\n",
    "- Prevents explanations when you only want data\n",
    "- Critical for JSON, APIs, agents, and tools\n",
    "\n",
    "Stop sequences are a **hard boundary**, unlike temperature or top_p."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1e33e45",
   "metadata": {},
   "source": [
    "#### **‚ùå The Problem Without Stop Sequences**\n",
    "\n",
    "LLMs like to be helpful.\n",
    "That means they often add:\n",
    "- explanations\n",
    "- comments\n",
    "- extra text\n",
    "\n",
    "This breaks:\n",
    "- APIs\n",
    "- JSON parsing\n",
    "- downstream systems\n",
    "\n",
    "We must explicitly tell the model **where to stop**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5bd15ec",
   "metadata": {},
   "source": [
    "#### **Without Stop Sequence (Observe the Problem)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "429cdd37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===== ‚ùå Without Stop Sequence =====\n",
      "```json\n",
      "{\n",
      "  \"name\": \"John Doe\",\n",
      "  \"age\": 30\n",
      "}\n",
      "```\n",
      "\n",
      "Note: I assume the user's name is \"John Doe\" and their age is 30. You can replace these values as per your requirement.\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# üìò SECTION 1 ‚Äî Request WITHOUT Stop Sequence\n",
    "# ------------------------------------------------------------\n",
    "# Why?\n",
    "#   - To observe how the model may add extra text\n",
    "# ============================================================\n",
    "\n",
    "prompt = \"\"\"\n",
    "Return ONLY the user's name and age as JSON.\n",
    "\"\"\"\n",
    "\n",
    "response_no_stop = client.chat.completions.create(\n",
    "    model=\"llama-3.1-8b-instant\",\n",
    "    messages=[\n",
    "        {\"role\":\"system\",\"content\":\"You are a strict JSON generator.\"},\n",
    "        {\"role\":\"user\",\"content\":prompt}],\n",
    "    temperature=0.3,\n",
    "    max_tokens=100\n",
    ")\n",
    "\n",
    "reply_no_stop = response_no_stop.choices[0].message.content\n",
    "\n",
    "print(\"===== ‚ùå Without Stop Sequence =====\")\n",
    "print(reply_no_stop)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39469449",
   "metadata": {},
   "source": [
    "#### **What You May See**\n",
    "\n",
    "Even with strict instructions, the model may return:\n",
    "\n",
    "- Extra explanations\n",
    "- Text before or after JSON\n",
    "- Markdown formatting\n",
    "\n",
    "This is NORMAL LLM behavior ‚Äî not a bug."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e2f7163",
   "metadata": {},
   "source": [
    "#### **WITH Stop Sequence**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "db40fa08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== ‚úÖ With Stop Sequence =====\n",
      "```json\n",
      "{\n",
      "  \"name\": \"John Doe\",\n",
      "  \"age\": 30\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# üìò SECTION 2 ‚Äî Request WITH Stop Sequence\n",
    "# ------------------------------------------------------------\n",
    "# Why?\n",
    "#   - Force the model to stop generation exactly where we want\n",
    "#   - Prevent extra text beyond JSON\n",
    "# ============================================================\n",
    "\n",
    "response_with_stop = client.chat.completions.create(\n",
    "    model=\"llama-3.1-8b-instant\",\n",
    "    messages=[\n",
    "        {\"role\":\"system\",\"content\":\"You are strict JSON generator.\"},\n",
    "        {\"role\":\"user\",\"content\":prompt}],\n",
    "    temperature=0.3,\n",
    "    max_tokens=100,\n",
    "    stop=[\"}\"] # üî¥ Stop generation when JSON closes\n",
    ")\n",
    "\n",
    "reply_with_stop = response_with_stop.choices[0].message.content + \"}\"\n",
    "\n",
    "print(\"\\n===== ‚úÖ With Stop Sequence =====\")\n",
    "print(reply_with_stop)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44c04fef",
   "metadata": {},
   "source": [
    "#### **üß† Senior Insight ‚Äî When Stop Sequences Are Mandatory**\n",
    "\n",
    "Always use stop sequences when:\n",
    "- Returning JSON\n",
    "- Returning SQL\n",
    "- Returning code snippets\n",
    "- Returning tool arguments\n",
    "- Feeding output into another system\n",
    "\n",
    "If you don‚Äôt:\n",
    "- Parsing WILL break\n",
    "- Bugs WILL appear\n",
    "- Systems WILL fail silently"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9ed82ae",
   "metadata": {},
   "source": [
    "#### **‚úÖ Topic 5 Summary**\n",
    "\n",
    "- Stop sequences are **hard output boundaries**\n",
    "- They prevent extra text and hallucinations\n",
    "- Essential for APIs, agents, tools, and RAG\n",
    "- One of the most underrated production controls"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bca961f",
   "metadata": {},
   "source": [
    "#### **üéØ Topic 6 ‚Äî Parameter Combinations & Safe Defaults**\n",
    "\n",
    "In real systems, parameters are **never used in isolation**.\n",
    "\n",
    "A senior GenAI engineer thinks in terms of:\n",
    "- use case\n",
    "- user experience\n",
    "- cost\n",
    "- reliability\n",
    "- safety\n",
    "\n",
    "This topic teaches **how parameters work together** and\n",
    "what safe defaults look like in production."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "001a3c79",
   "metadata": {},
   "source": [
    "#### **üß† Core Idea**\n",
    "\n",
    "Each parameter controls ONE dimension:\n",
    "- temperature ‚Üí randomness\n",
    "- top_p ‚Üí token diversity\n",
    "- max_tokens ‚Üí length & cost\n",
    "- stop ‚Üí hard output boundary\n",
    "\n",
    "Correct behavior emerges from **balanced combinations**,\n",
    "not extreme values.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c72d25a",
   "metadata": {},
   "source": [
    "#### **üìò Chatbot (User-Facing, Conversational)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6df41d70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ü§ñ Chatbot Response:\n",
      "**Python List Overview**\n",
      "==========================\n",
      "\n",
      "In Python, a list is a collection of items that can be of any data type, including strings, integers, floats, and other lists. Lists are denoted by square brackets `[]` and are ordered, meaning that items have a specific position or index.\n",
      "\n",
      "**Basic List Operations**\n",
      "-------------------------\n",
      "\n",
      "### Creating a List\n",
      "\n",
      "```python\n",
      "my_list = [1, 2, 3, 4, 5]\n",
      "```\n",
      "\n",
      "### Accessing List Elements\n",
      "\n",
      "```python\n",
      "print(my_list[0])  # Output: 1\n",
      "print(my_list[-1])  # Output: 5 (accesses the last element)\n",
      "```\n",
      "\n",
      "### Modifying List Elements\n",
      "\n",
      "```python\n",
      "my_list[0] = 10\n",
      "print(my_list)  # Output: [10, 2, 3, 4, 5]\n",
      "```\n",
      "\n",
      "### Adding Elements to a List\n",
      "\n",
      "```python\n",
      "my_list.append(6)\n",
      "print(my\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# üìò PRESET 1 ‚Äî Chatbot (User-Facing, Conversational)\n",
    "# ------------------------------------------------------------\n",
    "# Why these values?\n",
    "#   - temperature: slight creativity for natural tone\n",
    "#   - top_p: controlled diversity\n",
    "#   - max_tokens: prevent long, costly replies\n",
    "#   - stop: not needed for free-form chat\n",
    "# ============================================================\n",
    "\n",
    "response_chatbot = client.chat.completions.create(\n",
    "    model=\"llama-3.1-8b-instant\",\n",
    "    messages=[{\"role\":\"user\",\"content\":\"Explain python list simply.\"}],\n",
    "    temperature=0.6,\n",
    "    top_p=0.9,\n",
    "    max_tokens=200\n",
    ")\n",
    "\n",
    "print(\"ü§ñ Chatbot Response:\")\n",
    "print(response_chatbot.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af7b5a43",
   "metadata": {},
   "source": [
    "#### **üìò API / Structured Output**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "23d6d4cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üì¶ API Output:\n",
      "```python\n",
      "import json\n",
      "\n",
      "def get_user_age():\n",
      "    user_age = 30  # Replace with actual user age\n",
      "    return json.dumps({\"user_age\": user_age}\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# üìò PRESET 2 ‚Äî API / Structured Output\n",
    "# ------------------------------------------------------------\n",
    "# Why these values?\n",
    "#   - temperature: low ‚Üí deterministic\n",
    "#   - top_p: low ‚Üí limit randomness\n",
    "#   - max_tokens: strict ‚Üí cost + safety\n",
    "#   - stop: mandatory ‚Üí prevent extra text\n",
    "# ============================================================\n",
    "\n",
    "response_api = client.chat.completions.create(\n",
    "    model=\"llama-3.1-8b-instant\",\n",
    "    messages=[\n",
    "        {\"role\":\"system\",\"content\":\"Return ONLY JSON.\"},\n",
    "        {\"role\":\"user\",\"content\":\"Return user age as JSON\"}],\n",
    "    temperature=0.2,\n",
    "    top_p=0.3,\n",
    "    max_tokens=50,\n",
    "    stop=[\"}\"]\n",
    ")\n",
    "\n",
    "json_output = response_api.choices[0].message.content + \"}\"\n",
    "print(\"üì¶ API Output:\")\n",
    "print(json_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "220074d3",
   "metadata": {},
   "source": [
    "#### **RAG Answer (Grounded, Focused)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd3c061a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìö RAG-Style Response:\n",
      "In Python, a tuple is an immutable collection of objects. It is similar to a list, but unlike lists, tuples cannot be modified after they are created. Tuples are defined by enclosing a sequence of values in parentheses `()`.\n",
      "\n",
      "Here's an example of a tuple:\n",
      "\n",
      "```python\n",
      "my_tuple = (1, 2, 3, 4, 5)\n",
      "```\n",
      "\n",
      "Tuples are often used when you need to store a collection of values that shouldn't be changed. They are also faster and more memory-efficient than lists because they are immutable.\n",
      "\n",
      "Some key characteristics of tuples include:\n",
      "\n",
      "- They are immutable, meaning their contents cannot be modified after creation.\n",
      "- They are defined using parentheses `()`.\n",
      "- They can contain any type of object, including strings, integers, floats, and other tuples.\n",
      "- They support indexing and slicing, just like lists.\n",
      "- They support methods like `len()`, `index()`, and `count()`, but not `append()`, `insert()`, or `remove()`.\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# üìò PRESET 3 ‚Äî RAG Answer (Grounded + Controlled)\n",
    "# ------------------------------------------------------------\n",
    "# Why these values?\n",
    "#   - temperature: low ‚Üí reduce hallucinations\n",
    "#   - top_p: moderate ‚Üí readable answers\n",
    "#   - max_tokens: enough for explanation, not rambling\n",
    "# ============================================================\n",
    "response_rag = client.chat.completions.create(\n",
    "    model=\"llama-3.1-8b-instant\",\n",
    "    messages=[\n",
    "        {\"role\":\"system\",\"content\":\"Answer ONLY using provided context.\"},\n",
    "        {\"role\":\"user\",\"content\":\"What is a Python tuple?\"}],\n",
    "    temperature=0.3,\n",
    "    top_p=0.6,\n",
    "    max_tokens=300\n",
    ")\n",
    "reply_response_rag = response_rag.choices[0].message.content\n",
    "\n",
    "print(\"üìö RAG-Style Response:\")\n",
    "print(reply_response_rag)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f761e500",
   "metadata": {},
   "source": [
    "#### **‚úÖ Safe Defaults Cheat Sheet**\n",
    "\n",
    "#### Chatbots\n",
    "- temperature: 0.5‚Äì0.7\n",
    "- top_p: 0.8‚Äì0.9\n",
    "- max_tokens: 150‚Äì300\n",
    "\n",
    "#### APIs / JSON / Tools\n",
    "- temperature: 0.0‚Äì0.3\n",
    "- top_p: 0.2‚Äì0.4\n",
    "- max_tokens: 50‚Äì200\n",
    "- stop: REQUIRED\n",
    "\n",
    "#### RAG Systems\n",
    "- temperature: 0.2‚Äì0.4\n",
    "- top_p: 0.5‚Äì0.7\n",
    "- max_tokens: 200‚Äì500\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8997587e",
   "metadata": {},
   "source": [
    "#### **üß† Interview Insight**\n",
    "\n",
    "Question:\n",
    "‚ÄúHow do you reduce hallucinations in production?‚Äù\n",
    "\n",
    "Strong answer:\n",
    "‚ÄúBy combining low temperature, controlled top_p,\n",
    "strict max_tokens, stop sequences, and grounded prompts ‚Äî\n",
    "not by relying on a single parameter.‚Äù\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f517b23",
   "metadata": {},
   "source": [
    "#### **‚úÖ Topic 6 Summary**\n",
    "\n",
    "- Parameters must be tuned **together**\n",
    "- Safe defaults depend on use case\n",
    "- Production systems never rely on defaults\n",
    "- This is where junior engineers usually fail"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "421ee91f",
   "metadata": {},
   "source": [
    "#### **üß™ Topic 7 Micro Practice: Change Parameters & Observe (Hands-on)**\n",
    "\n",
    "#### Goal ‚Äî Build Intuition (Not Memorization)\n",
    "\n",
    "The goal here is NOT to memorize values.\n",
    "\n",
    "It is to:\n",
    "- change ONE parameter at a time\n",
    "- observe the output difference\n",
    "- understand *why* the behavior changed\n",
    "\n",
    "This is how senior engineers learn."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cd25eea",
   "metadata": {},
   "source": [
    "#### **Practice 1: Temperature Only**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "8af739c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===== Low Temperature (0.2) =====\n",
      "**What are Python Dictionaries?**\n",
      "\n",
      "In Python, a dictionary is a data structure that stores a collection of key-value pairs. It's like a phonebook where you have names (keys) and phone numbers (values).\n",
      "\n",
      "**Key Features:**\n",
      "\n",
      "1. **Keys**: These are unique identifiers for each value. They can be strings, numbers, or even other data types.\n",
      "2. **Values**: These are the actual data stored in the dictionary. They can be strings, numbers, lists, dictionaries, or any other data type.\n",
      "3. **Unordered**: Dictionaries don't maintain a specific order of their items.\n",
      "4. **Mutable**: Dictionaries can be modified after they're created.\n",
      "\n",
      "**How to Create a Dictionary:**\n",
      "\n",
      "You\n",
      "\n",
      "===== High Temperature (0.8) =====\n",
      "**What is a Dictionary in Python?**\n",
      "\n",
      "In Python, a dictionary is a collection of key-value pairs. Think of it like a phonebook where you store names (keys) and phone numbers (values).\n",
      "\n",
      "**Key Features:**\n",
      "\n",
      "1. **Keys:** These are the names or identifiers of the values. They are unique, meaning you can't have two keys with the same value.\n",
      "2. **Values:** These are the actual data stored in the dictionary. They can be of any data type, including strings, integers, floats, lists, dictionaries, etc.\n",
      "3. **Key-Value Pairs:** Each key is associated with a value. When you access a key, you get its corresponding value.\n",
      "\n",
      "**Example:**\n",
      "\n",
      "```python\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# üìò PRACTICE 1 ‚Äî Change Temperature Only\n",
    "# ------------------------------------------------------------\n",
    "# Rule:\n",
    "#   - Keep everything SAME\n",
    "#   - Change ONLY temperature\n",
    "# ============================================================\n",
    "\n",
    "prompt = \"Explain Python dictionaries in simple terms.\"\n",
    "\n",
    "# Low temperature (deterministic)\n",
    "response_low_temp = client.chat.completions.create(\n",
    "    model=\"llama-3.1-8b-instant\",\n",
    "    messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "    temperature=0.2,      # üîΩ Low randomness\n",
    "    top_p=0.9,\n",
    "    max_tokens=150\n",
    ")\n",
    "\n",
    "print(\"===== Low Temperature (0.2) =====\")\n",
    "print(response_low_temp.choices[0].message.content)\n",
    "\n",
    "# High temperature (creative)\n",
    "response_high_temp = client.chat.completions.create(\n",
    "    model=\"llama-3.1-8b-instant\",\n",
    "    messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "    temperature=0.8,      # üîº High randomness\n",
    "    top_p=0.9,\n",
    "    max_tokens=150\n",
    ")\n",
    "\n",
    "print(\"\\n===== High Temperature (0.8) =====\")\n",
    "print(response_high_temp.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93b4e806",
   "metadata": {},
   "source": [
    "#### **üìò PRACTICE 2 ‚Äî Change top_p Only**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "a31d3886",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===== Low top_p (0.2) =====\n",
      "**What are Python Dictionaries?**\n",
      "\n",
      "In Python, a dictionary is a data structure that stores a collection of key-value pairs. It's like a phonebook where you have names (keys) and phone numbers (values).\n",
      "\n",
      "**Key Features:**\n",
      "\n",
      "1. **Keys**: These are unique identifiers for each value in the dictionary. They can be strings, numbers, or even other data types.\n",
      "2. **Values**: These are the actual data stored in the dictionary, associated with each key.\n",
      "3. **Unordered**: Dictionaries don't have a specific order, unlike lists or arrays.\n",
      "4. **Mutable**: You can add, remove, or modify key-value pairs in a dictionary.\n",
      "\n",
      "**Example:**\n",
      "\n",
      "```python\n",
      "# Create a dictionary\n",
      "\n",
      "===== High top_p (0.9) =====\n",
      "**What are Python Dictionaries?**\n",
      "\n",
      "In Python, a dictionary is a data structure that stores a collection of key-value pairs. It's like a phonebook where you have names (keys) and phone numbers (values).\n",
      "\n",
      "**Key Features:**\n",
      "\n",
      "1. **Key-Value Pairs:** Dictionaries consist of key-value pairs, where each key is unique and maps to a specific value.\n",
      "2. **Unordered:** Dictionaries are unordered collections, meaning the order of the key-value pairs doesn't matter.\n",
      "3. **Mutable:** Dictionaries can be modified after creation.\n",
      "\n",
      "**Creating a Dictionary:**\n",
      "\n",
      "You can create a dictionary using the `dict()` function or the `{}` syntax:\n",
      "```python\n",
      "# Using the dict() function\n",
      "person\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# üìò PRACTICE 2 ‚Äî Change top_p Only\n",
    "# ------------------------------------------------------------\n",
    "# Rule:\n",
    "#   - Keep temperature SAME\n",
    "#   - Change ONLY top_p\n",
    "# ============================================================\n",
    "\n",
    "# Narrow nucleus\n",
    "response_low_top_p = client.chat.completions.create(\n",
    "    model=\"llama-3.1-8b-instant\",\n",
    "    messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "    temperature=0.5,\n",
    "    top_p=0.2,            # üîΩ Narrow token pool\n",
    "    max_tokens=150\n",
    ")\n",
    "\n",
    "print(\"===== Low top_p (0.2) =====\")\n",
    "print(response_low_top_p.choices[0].message.content)\n",
    "\n",
    "\n",
    "# Wide nucleus\n",
    "response_high_top_p = client.chat.completions.create(\n",
    "    model=\"llama-3.1-8b-instant\",\n",
    "    messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "    temperature=0.5,\n",
    "    top_p=0.9,            # üîº Wider token pool\n",
    "    max_tokens=150\n",
    ")\n",
    "\n",
    "print(\"\\n===== High top_p (0.9) =====\")\n",
    "print(response_high_top_p.choices[0].message.content)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61570e59",
   "metadata": {},
   "source": [
    "#### **üîç What to Observe**\n",
    "\n",
    "Low max_tokens ‚Üí truncated or brief answers\n",
    "\n",
    "High max_tokens ‚Üí fuller explanations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46673de5",
   "metadata": {},
   "source": [
    "#### **üß† My Observations**\n",
    "\n",
    "- Temperature controls **creativity**\n",
    "- top_p controls **token diversity**\n",
    "- max_tokens controls **length & cost**\n",
    "- Changing ONE parameter at a time builds intuition\n",
    "\n",
    "I do NOT need to memorize values.\n",
    "I need to understand behavior.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2a14054",
   "metadata": {},
   "source": [
    "#### **‚úÖ Topic 7 Summary**\n",
    "\n",
    "Today I practiced:\n",
    "- Isolating parameters\n",
    "- Observing behavior changes\n",
    "- Building real intuition\n",
    "\n",
    "This skill directly translates to:\n",
    "- debugging hallucinations\n",
    "- tuning chatbots\n",
    "- controlling production systems"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3b342cd",
   "metadata": {},
   "source": [
    "#### **# üß™ Mini Mock Test ‚Äî LLM Parameters**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2eef9f3",
   "metadata": {},
   "source": [
    "\n",
    "#### **Q1Ô∏è‚É£ What problem does `temperature` solve in LLMs?**\n",
    "\n",
    "#### **A1:** Temperature controls randomness in token selection, balancing creativity vs determinism.\n",
    "---\n",
    "\n",
    "#### **Q2Ô∏è‚É£ Difference between `temperature` and `top_p` in one sentence.**\n",
    "\n",
    "#### **A2:** Temperature controls randomness; top_p controls how many high-probability tokens are considered.\n",
    "---\n",
    "\n",
    "##### **Q3Ô∏è‚É£ Why is not setting `max_tokens` dangerous in production?**\n",
    "\n",
    "#### **A3:** It can lead to runaway costs, long responses, latency spikes, and unsafe outputs.\n",
    "---\n",
    "\n",
    "##### **Q4Ô∏è‚É£ When is `stop` sequence mandatory?**\n",
    "\n",
    "#### **A4:** When returning structured output like JSON, SQL, tool arguments, or API responses.\n",
    "---\n",
    "\n",
    "#### **Q5Ô∏è‚É£ You are building an API that returns JSON.What parameter values would you choose and why?**\n",
    "\n",
    "#### **A5:** Low temperature (0.0‚Äì0.3), low top_p (0.2‚Äì0.4), strict max_tokens, and stop sequences to prevent extra text.\n",
    "---\n",
    "\n",
    "#### **Q6Ô∏è‚É£ True or False:Using a low temperature alone is enough to prevent hallucinations.**\n",
    "\n",
    "#### **A6:** False ‚Äî hallucination control requires multiple parameters + grounding + guardrails.\n",
    "---\n",
    "\n",
    "#### **Q7Ô∏è‚É£ Why do senior engineers think in parameter *combinations* instead of single parameters?**\n",
    "\n",
    "#### **A7:** Because real behavior emerges from how parameters interact, not from isolated values.\n",
    "---\n",
    "\n",
    "#### **Q8Ô∏è‚É£ Interview question:‚ÄúHow do you control LLM behavior in production?‚Äù**\n",
    "\n",
    "#### **A8:** ‚ÄúBy tuning temperature, top_p, max_tokens, and stop sequences together based on the use case, with validation and guardrails.‚Äù"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "037957e3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
